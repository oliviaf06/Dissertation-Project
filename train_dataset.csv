topic,course_name,page_number,module_name,module_no,page_title,page_type,res_content,link,Unnamed: 9
IBM Cloud,Journey to Cloud: Envisioning your Solution,13,Module 2 - Cloud Adoption Journey : Ideation Practices,2,Topic 2: Frame Your Business Opportunity,text,"Unpacking the business problem with Design Thinking 

Introduction

In this topic, we'll conduct a business framing exercise to better understand our business need and create a minimal viable product.

Throughout this topic, we will attempt to answer the following questions:

Why should Design Thinking be an essential part of digital transformation and cloud adoption?
How does Business Framing fit within the context of user-centric design and creating the best possible MVP?
How to put all this into practice through a real-life industry example?
1. Why should Design Thinking be an essential part of digital transformation and cloud adoption? 

Business Framing- an interactive Enterprise Design Thinking session that helps you explore new potential and prioritize strategic initiatives. 

What is Business Framing?   

During business framing, you gather decision-makers who are familiar with your business's strengths and weaknesses in an activity-based session. The goal is to align on the identification, prioritization, selection, and definition of business opportunities. Business framing clarifies and enlists sponsorship around a selected opportunity’s budget, authority, and timeline. 

A table showing IBM garage journey and highlighting Business framing.
The IBM Garage Journey

In a business-framing session, opportunities are identified by answering these questions:

What is your starting point?
Do you already have a roadmap of opportunities?
What are key challenges that you have an opportunity to address?
What is the highest priority opportunity that the team needs to focus on going forward?
Who is the right person to act as the champion and executive sponsor for your selected project?
What is the approximate budget and timeline for the project?
Who has the final authority to approve the project?
Focus on business opportunities, not technical solutions.
2. How does Business Framing fit within the context of user-centric design and creating the best possible MVP? 

A group of people participating in a session
Conduct a Business-Framing Session

You can conduct business-framing activities during a facilitated session either in person or remotely. Select representatives from business and technical roles to participate in the discussion. The business representative provides the business strategy perspective to the discussion. An architect, a designer, and one or more subject matter experts from relevant technical areas can also provide input in your session.As you conduct your business framing session, use whiteboards or MURAL to capture information throughout the conversation. Storyboarding tools such as MURAL are especially useful if you are conducting your discussion with remote participants.

Business-framing sessions are not meant to be rigid. You can customize them based on your needs. For example, you might include education on a technology or practice area if that is necessary to set greater context before you dive into your opportunity spaces.Conduct your business-framing session by facilitating activities that help you align on a common vision; define, refine, and prioritize your opportunities; and align your team to advance a high-priority opportunity.

Align on a common vision
Before you start to identify your opportunities, take time to align on a common vision. Think about where you want your enterprise to be in several years. Determine the internal and external challenges that stand in the way of reaching your goals. Finally, consider all of the resources and advantages that you can use to reach your goal.

Ideate and cluster opportunities 

Image Showing a Box with a Lightbulb emerging from it to show Thinking Outside the Box.
Ideation is the process of generating big ideas. Enterprise Design Thinking explains big ideas by contrasting them with features:

Big idea: Algorithms to predict the future from the past
Feature: Charts with lines that show prediction
Moving to big ideas takes your mind out of the problem space and into the realm of solutions. This realm is where you innovate and create revolutionary, rather than evolutionary, designs.

In the ideation process, have each member of the team put a minimum of five big ideas on the wall. Of each person's five big ideas, at least one must break the laws of physics. The goal here is to think big without inhibition. This last rule forces each member of the team to step out of their current thought process. It's a nod to the famous quotation attributed to Albert Einstein, ""We can’t solve problems by using the same kind of thinking we used when we created them.""

Ideate on your challenges. Identify the people who are affected by each challenge and the business outcome that is achieved if the challenge is addressed. During this activity, each person in the session contributes at least five ideas. Each grouping — people, pain, and business outcome — represents an opportunity with a clearly defined business outcome. After the opportunities are created, the facilitator groups them to eliminate duplicates and to identify overlapping opportunities. 

3. How to put all this into practice through a real-life industry example? 

Case Study 

Episode I
This industry scenario focuses on understanding the business needs of an airline, let's explore the current industry trends in the financial market.

Market Insights


Gaining the Competitive Edge
+

Airlines in a Digitally Connected World  
+

The Impact of Covid-19 on Airlines
+
Business Profile 

Now that we understand the current market landscape, let's dive into the fictitious company featured in our case study – Acme Airlines.

Introduction to Acme Airlines 

Acme Airlines, a flagship national carrier, was founded 50 years ago and now operates on five continents, with numerous daily flights between domestic and international destinations. 

The company employs approximately 25,000 employees around the world with staff in most major airports and serves an estimated 50 million passengers annually.

Acme Airlines prides itself on operational efficiency and outstanding customer service, and it has consistently generated high profits by offering low-cost, high-quality services to passengers, with a primary focus on returning and business class customers. Acme has well-earned reputation for comfort - with more leg room, as premium on-flight meals, and generous discounts on Acme-owned hotels and car rentals. 

Flight attendants helping passengers
The Business Problem 

Now let's identify the problems that the airline is facing so that we can come up with a solution. 

Over the past decade the once thriving airline has seen a steady decline in ticket sales as passengers drifted over to rival carriers. Then along came the Covid-19 pandemic which exposed just how dire the situation really was. Since 2012, the airline has lost 32% of its customer base, translating to a $170 million loss in revenue.   

In a press release, the new CEO stated that:

“For far too long, Acme Airlines has operated under the notion that ‘if its not broken, don’t fix it.’ That philosophy cannot work for a modern company operating in a digital age. Today, Acme Airlines is charting a new course that embraces innovation and transformation …”.

A confident business woman standing with a smile on her face.
Jacqlyn

Acme Airlines - CEO 

The decision is made to partner with the IBM Garage team to assist Acme Airlines in its digital transformation journey. A stakeholder meeting is called that brought together management, Acme Airline team leads, IBM garage consultants, and other stakeholders to discuss the problem and come up with a plan of action. The areas of concern that were discussed have been summarized below: 

Despite offering high quality service and top notch amenities the airline is still losing customers to rival airlines.The company has a public perception of being an &quot;old school&quot; organization that caters primarily to business travelers. 
Despite offering high quality service and top notch amenities the airline is still losing customers to rival airlines.
The company has a public perception of being an ""old school"" organization that caters primarily to business travelers.
Their social media postings reflect this image with strictly business related content that does not allow for much user engagement. Especially among the under 40 target audience. Maintaining a strictly on-prem IT environment is very expensive. The high cost of developers to update and maintain the airline website, storing and tracking customer data, and preventing hacking and other malicious activity has become a concern. 
Their social media postings reflect this image with strictly business related content that does not allow for much user engagement. Especially among the under 40 target audience. 	Maintaining a strictly on-prem IT environment is very expensive. The high cost of developers to update and maintain the airline website, storing and tracking customer data, and preventing hacking and other malicious activity has become a concern.
Acme has not embraced new technologies and ways of attracting customers in a fast-paced, digital age. Although they have both a website and a passenger app, its features are limited to the following: Booking a reservationCheck flight information Track Flyer RewardsGet alerts to flight cancellations &amp; changesCompetitors are using mobile apps with the following features: booking, ticket changes, reservation reminders, travel alerts, GPS airport maps, baggage tracking, hotel and car rental suggestions, passport scanners, flight status tracking, entertainment streaming, airport, food pre-ordering, and ride pickup services. 
Acme has not embraced new technologies and ways of attracting customers in a fast-paced, digital age. Although they have both a website and a passenger app, its features are limited to the following: 

Booking a reservation
Check flight information 
Track Flyer Rewards
Get alerts to flight cancellations & changes
Competitors are using mobile apps with the following features: booking, ticket changes, reservation reminders, travel alerts, GPS airport maps, baggage tracking, hotel and car rental suggestions, passport scanners, flight status tracking, entertainment streaming, airport, food pre-ordering, and ride pickup services.
Conduct a Business Framing Session

Group of people participating in Design thinking session.
To address these problems, the team is invited to evaluate Acme’s current business model through an ideation exercise.

Representatives from business and technical roles participate in the discussion. The business representative provides the business strategy perspective to the discussion. An architect, a designer, and one or more subject matter experts from relevant technical areas can also provide input in your session.

Business Framing Objectives: 

Business-framing sessions are not meant to be rigid. Allow ideas to flow organically. These ideas can be refined after further discussion.
Brainstorm ideas concerning the goals, the outcomes, what the company is doing well, and what is holding it back from achieving its transformation goals.

Business Needs 
After the business framing exercise let's see what we found at Acme Airlines.

Business Problem - The airline wants to modernize its IT Architecture.  Currently it is still relying on a traditional on-premise model. However, that has become too costly to upgrade and maintain.
Business Opportunity - Leverage Hybrid Cloud computing solutions by migrating applications and storage to a third-party SaaS provider.  
Business Outcome - They are looking to reduce the cost of IT development and maintenance while providing customers with a superior travel experience. 
The Business Opportunity Statement

 The Business Opportunity Statement is the key output of the business-framing session. It provides input for a technical discovery session, if needed. It also provides input for the Enterprise Design Thinking workshop where you create a technical roadmap and define the minimum viable product (MVP) that you can build to satisfy your business opportunity. 

Based on the feedback gathered from the Business Framing Session, the team writes a concise Business Opportunity Statement to summarize the outcome and frame the next steps to be taken. 

Acme Airlines: Business Opportunity Statement 
We will improve the experience of the end-to-end process: booking, check-in, in-flight service, and destination, through an integrated cloud-based application for all our airline passengers.

The user struggles today because all interaction can only be done via our website which offers options limited strictly to flight book, check-in, and flight scheduling updates. 

Solving this will be good for our business because passengers will be provided with a seamless end-to-end travel experience where they will be able to book a flight, monitor flight updates, check-in, access airport maps in real time, upgrade their seat, rent a car or book a hotel, and pre-select in-flight entertainment and meals all from the convenience of their mobile device. 

Assembling the Team 

Now that we know our business priorities, Acme Airlines assigned a team of experts to this project including: a Business Liaison, Cloud Architect, Cloud Developer; and, because they were also concerned with Data Privacy in using Public Cloud to store sensitive customer data, they included Adam (Data Privacy Counsel) to join them and ensure everyone in the team is following principles of Trust and Responsibility.





Playback Session 

Group of people in middle of the meeting.
The team has made a lot of progress identifying the pain points and coming up with a proposal for moving Acme forward on its journey to becoming a modern airline offering passengers a superior flight experience driven by cloud solutions. During the Playback session key stakeholders are called into the discussion to hear the insights made during the Business Framing session. The team recommends the following areas to be targeted as the first steps toward modernizing Acme Airlines.

Reorganizing development teams to work in a more agile and independent environment.
Incrementally transition its IT architecture to a Hybrid Cloud model.
Develop a prototype cloud app to improve the user experience in order to attract a more diverse range of passengers.
Translate the Business Needs into Data Needs 

Image of a Data scientist persona
1. Gather further business requirements
Preethi – the Data Scientist at Acme Airlines wants to understand why mobile devices have gained a competitive edge in the airline industry. Her first goal is to articulate the overall business problem: ""What type of features do airline passengers want to see in a mobile app?""

2. Identify matching business analytics patterns
The team then identifies that the goal of the project is to create a predictive model to identify points of customer dissatisfaction in their travel experience. 

Preethi uses both current company data such as a customer survey that was sent out to all passengers who flew Acme, as well as transactional data gathered from Acme's internal databases related to flight schedules and maintenance to analyze trends and make a prediction. 

Preethi starts with structured data by importing all of the databases related to Acme flight cancellations and delays to find how these variables might impact customer satisfaction.

She then builds a model in AutoAI to predict each attribute's impact on wait times where she finds that Acme flight delays and cancellations are similar to that of other carriers. With this information, she can rule out excessive delays as being a leading factor in the migration of customers to other airlines.

Preethi then turns her attention to the results of the Customer Satisfaction survey to ascertain the reasons that might result in passenger angst. She again uses AutoAI, which can analyze unstructured data using Natural Language Understanding capabilities, to extract sentiment from the survey results.

Auto AI Sentiment analysis pulls keywords and tone from the response text which can be analyzed.

The survey results indicate four key factors leading to passenger dissatisfaction.

Anxiety over flight changes and cancellations.
Anxiety due to having to use and monitor multiple services to obtain a ride to and from the airport.
Anxiety due to not knowing the layout of the airport and gate locations.
The majority of responders indicated that they were unimpressed by the current level of services offered by Acme compared to competitors. 
Preethi will bring these findings to the team during the Design Thinking workshop to help the team better understand their end-users' wants and needs when flying. 



Why it matters? 
Business Framing sessions are important in the modernization process. Representatives from various roles throughout the company help to add insight, identify personal painpoints, and offer solutions to how the company can improve. The end result of Business Framing is a set of goals and a framework for how the company will meet those goals.  

Summary

1

1
Frame your business opportunity - the goal of the Business Framing Exercise is to align on the identification, prioritization, selection, and definition of business opportunities. 

2

2
Align to a common goal - think about where you want your enterprise to be in several years. Determine the internal and external challenges that stand in the way of reaching your goals. Finally, consider all of the resources and advantages that you can use to reach your goal. 

3

3
Ideate and cluster opportunities - ideation is the process of generating big ideas. Enterprise Design Thinking explains big ideas by contrasting them with features.

Next 

In Topic 2, Next, the team will learn about Enterprise Design Thinking practices and participate in an EDT exercise called Empathy Mapping. 

Sources:
[1] IBM Cloud. Explore the Garage Methodology. IBM. 
[2] Platenberg, Sarah. Solve Problems Through Ideation. IBM. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/XaBPZBzFmQpG2g76BDh2IdB9Vm5a1WeC,
IBM Security,Getting Started with Threat Intelligence and Huntin,1,Course Overview,0,About this Course,text,"Understand the impact of cybersecurity threats on industries and society.

Why study cybersecurity? 

Cybersecurity is the practice of protecting the integrity of networks, systems, devices, and data from cyber-attacks. Implementing cybersecurity best practices has become a top priority for enterprise businesses, where data sensitivity is at the highest level. Widely available hacking tools and sophisticated digital extortion tactics put client and business data, and even operations, at risk. Maintaining a proactive cybersecurity approach is essential to mitigating both short- and long-term business continuity risks.

The current state of the cybersecurity landscape has left enterprises relying on leadership roles like a Chief Information Security Officer (CISO) or Managed Security Services Providers (MSSP) to implement effective and sustainable cybersecurity strategies. But no matter the level of investment or resources, businesses of all sizes can take steps to secure their data.

Cybersecurity impact in the job market 

Market Insights

Cybersecurity isn’t just about hackers and defenses. It’s about people and the way security impacts their daily lives.

Protecting the world is an ambitious goal, but an important one that unites us. Our mission drives the way we design security solutions. It guides the way we serve customers and help organizations and communities reach their potential.

A new Burning Glass study should serve as a wake-up call for technical training organizations: The demand for cybersecurity professionals is outstripping the supply of skilled workers. The National Center for Education Statistics (NCES) shows the number of new cybersecurity programs has increased 33%, but the demand is growing faster as cybersecurity is now considered mission-critical in most organizations.

How completing this course could benefit you? 
Managing cybersecurity is critical, and employers see it that way, looking for seasoned professionals who can hit the ground running.

The typical cybersecurity job today requires a college degree, creating a major barrier to entry as the school system is not producing enough graduates. In many roles, the need for a college degree is unnecessary and not the best way to prepare for a career where technology is changing rapidly.

Cybersecurity is everywhere, and most jobs today require cybersecurity skills 

Looking for a job? – Gain a new set of skills on threat intelligence and enterprise resilience, and join a new wave of security-aware professionals with access to millions of jobs available in the market. 

Looking for a better job? – If you already have a job and even some experience in this field, use this course to select a specialization and advance your career, by playing different roles within a security team, solving real challenges within the enterprise, leveraging threat intelligence technologies. 

Objectives

This course has the following learning objectives

1

1
Understand the taxonomy of cybersecurity attacks                         

2

2
Analyze top targeted industries and security trends 

3

3
Determine what steps you can take to protect your organization against these threats. 

4

4
Leverage high-end security enterprise solutions in high demand such as IBM Cloud X-Force Exchange. 

Pre-requisites

Basic IT Literacy
Basic IT Literacy - Refers to skills required to operate user-level operating system environment such as Microsoft Windows® or Linux Ubuntu®, performing basic operating commands such, copying and pasting, using menus, windows, through the mouse and keyboard. Additionally, users should be familiar with internet browsers, search engines, page navigation, fill and submit forms.

Recognition

This course grants the following digital credentials upon completion

Learners

Completing this course you will earn the Getting Started with Threat Intelligence and Hunting - Foundational-level badge. 

What are the completion requirements?

Complete all the modules included in this self-paced online course
Pass the quizzes included on every module
More details about this digital certificate below

Badge websitehttps://www.credly.com/org/ibm/badge/getting-started-with-threat-intelligence-and-hunting
Badge website

https://www.credly.com/org/ibm/badge/getting-started-with-threat-intelligence-and-hunting

Skills:

Cybersecurity
Cyber Resilience
Security Practices 
Phishing Attacks
IBM X-Force Exchange
Threat Hunting
Badge Description: 

This badge earner has completed all the learning activities included in this online learning experience, including hands-on experience, concepts, methods and tools related to the threat intelligence and hunting domain. The individual has demonstrated domain knowledge and understanding in adopting practices, methods and tools that relates to the activities performed in cyber threat hunting. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/0udAHKIWgxr6Peb13Kf_MA7oXD8j3YSS,
IBM Automation,IBM Robotic Process Automation - Basic I,24,Advanced Commands,23,Number manipulation,video,,https://learn.ibm.com/mod/video/view.php?id=200990,
IBM Security,Getting Started with Threat Intelligence and Huntin,6,Module 1 - Threat Intelligence,1,Topic 3: Threat Assessment by Industry and Geography,text,"
Let's gain a better understanding of the threat landscape by analyzing the attacks in different industries on a global scale. 

Introduction

In this topic, we will cover in detail the impact of the activities of cybercriminals attacks on our industries and regional societies. 

Throughout this topic, we will attempt to answer the following questions: 

What are the main attack trends by industry? 
What is the impact of cyber attacks per geography? 
1. What are the main attack trends by industry?

Cyber attacks this year targeted most of our industries, with 23% of attacks against the finance and insurance industries, 17.7% against manufacturing, closely followed by energy 11.1%, and retail 10.2%. The rest of the top 10 industries were targeted by under 10% of attacks each. 

Top 10 industries cyber attack breakdown 

For the fifth year in a row, the finance and insurance industry was the most-attacked industry, underscoring the significant interest threat actors have in these organizations.  

The figure below portrays the top attacks on each industry from X-Force incident response data. 

The figure portrays the top attacks on each industry from X-Force incident response data.
Percentage breakdown of industry attacks by type, from X-Force incident response data (Source: IBM Security X-Force)

Key findings 

Attackers also focused on industries connected to operational technology 

Manufacturing moved from the eighth position to the second most attacked. This may be driven by the interest malicious actors have in targeting infrastructure with connections to operational technology (OT). 
Similarly, energy jumped from ninth place to third place, further underscoring attackers’ focus on OT-connected organizations.
COVID-19 drove attackers focus

Healthcare jumped from last place to seventh place probably driven
by COVID-related healthcare attacks and a barrage of ransomware attacks against hospitals.
Transportation targeting continued to drop, falling from third to ninth place, potentially related to less transportation utilization during the pandemic.
Numbered divider 1
1/10 - Financial sector 

Financial institutions experienced 23% of all attacks we analyzed, up from the 17% of attacks the sector experienced in the past.

Malware for mobile banking fraud
Financial malware in the cybercrime arena continues to pose a threat to financial and other organizations, as threat actors continue to innovate and new threats emerge.

Recently IBM Trusteer observed cybercrime gangs use a highly automated process to empty bank accounts via mobile banking fraud, and we are seen remote overlay attacks become even more common, particularly in Europe.

A photograph of a business woman using the ATM machine.
High volume of server attacks
Of all industries, finance and insurance experienced the highest number of server access attacks — primarily related to Citrix vulnerability CVE-2019-19781 — when compared to other industries.
Server access attacks made up 28% of all attacks on finance and insurance, and the industry tied with manufacturing for the highest percentage of attacks that exploited CVE-2019-19781, at 22%.
The highly regulated nature of the finance and insurance sector and finance organizations’ proactive approach to identifying and addressing server access attacks probably contributed to the high percentage of attacks on this sector.

Lower rate of ransomware attacks
In addition, finance and insurance experienced fewer ransomware attacks when compared to other industries, such as manufacturing, professional services and government.
Only 10% of attacks on this industry were ransomware. Ransomware attackers have probably found non-finance organizations to be more profitable for ransomware attacks, potentially because of strong security controls in place at finance and insurance organizations, or because attackers assess that industries such as manufacturing and professional services have a lower tolerance for downtime related to ransomware attacks.

Bringing it all together

In the video below your instructor will guide you through these concepts 


Numbered divider 2
2/10 - Manufacturing

Manufacturing ranked as the second-most attacked industry.

Cars and robots over a conveyor belt inside a car manufacturing plant
21% of attacks were from ransomware
Threat actors find manufacturing to be a profitable sector for ransomware attacks. And, in pure numbers, manufacturing experienced more ransomware attacks than any other sector. This sector’s low tolerance for downtime — often amounting to millions of dollars in losses for each hour of downtime—is probably a contributing factor in its high profitability for threat actors.

4x times more BEC attacks than any other industry
BEC made up 17% of attacks on manufacturing. Manufacturing organizations often need to procure multiple parts from several different suppliers, creating multiple avenues for threat actors to insert themselves into email conversations and redirect funds meant to pay for manufacturing supplies. Many attacks on manufacturing appear to be targeting money through social engineering, rather than targeting operational technology.

Numbered divider 3
3/10 - Energy

Energy ranked as the third- most attacked industry, up from ninth place the year prior. 

Wind turbines surrounded by clouds
Server access attacks
Particularly those exploiting CVE-2019-19781—hit energy organizations hard, and this industry came in fourth place after healthcare for the highest number of such attacks.

Data theft and leak is #1 attack type
Accounting for 35% of all attacks in this sector, and underscoring the threat from information-stealing malware and phishing attacks. Many of these attacks were against oil and gas companies in particular.

BEC attacks, digital currency mining, ransomware, remote access trojans
Affected this industry but not notably more than other sectors. In fact, ransomware attacks against energy accounted for only 6% of all attacks against the industry—considerably lower than many of the other top attacked verticals.

Numbered divider 4
4/10 - Retail

As a hub of credit card payments and other financial transactions, retail has long been a target of choice for malicious threat actors.

The retail industry ranked as fourth-most attacked industry, down from second place last year, and received 10.2% of all attacks on the top 10 industries, down from 16% last year.

A lady walking on the street with many street shops on the side
Retail experienced more credential theft attacks than any other attack type
Credential theft makes up 36% of the attacks and surpasses in pure numbers all other sectors for credential theft attacks.

Ransomware Sodinokibi attacks
The industry also suffered from ransomware attacks — making up 18% of total attacks on retail. Nearly all of these ransomware attacks came from Sodinokibi attacks.

DDoS attacks, fraud, misconfiguration, RATs and server access attacks
To a lesser extent, DDoS attacks, fraud, misconfiguration, RATs and server access attacks also affected the retail industry, indicating that threat actors are using a range of attack types to infiltrate retail organizations for financial gain.

Numbered divider 5
5/10 - Professional services 

Professional services organizations are particularly attractive to attackers because of the avenue they provide to additional victims.

Ranked as the fifth-most attacked industry receiving 8.7% of all attacks—holding its same rank as in previous years, when it received 10% of all attacks.

Two african american women talking to a professional service provider discussing the terms of an agreement laying on the table
Ransomware Sodinokibi attacks
Ransomware made up 35% of incidents at professional services firms—the highest percentage out of all industries—and in terms of raw numbers of ransomware attacks, the professional services sector came in second only to manufacturing.

Sodinokibi went after professional services firms aggressively, including law firms.

The sensitive data these firms hold on their clients, and in some cases celebrity clients, possibly led threat actors to believe these firms would be more likely to pay a ransom to prevent the leak of sensitive data.

One law firm’s data was put up for auction for $40 million dollars, underscoring the high price ransomware attackers perceive they can obtain for professional services firms’ data.

Data theft and server access
In addition to ransomware attacks, data theft and server access attacks hit professional services hard, accounting for 13% of attacks each on the industry. These trends suggest that injection attacks and vulnerability exploitation on professional services firms are common as threat actors seek access to sensitive data.

Numbered divider 6
6/10 - Government 

The public sector—including defense, public administration, and government-provided services—ranked as the sixth most attacked in the ranking, receiving 7.9% of all attacks on the top ten industries.

From IBM Security X-Force incident response data, it appears that ransomware attacks plagued government organizations the most in 2020, followed closely by data theft.

A busy plaza surrounded by several government buildings
Ransomware Sodinokibi attacks
Ransomware attacks accounted for 33% of the total attacks on government organizations—the second highest only after professional services.
X-Force Incident Response observed government judicial systems and government transportation entities in the crosshairs of ransomware. Nearly 50% of ransomware attacks are from Sodinokibi threat actors, following on a trend the group began in September 2019 with a barrage of ransomware attacks against 23 municipalities in Texas, US.

Data theft and leak
The second most common attack type was data theft and leak accounting for 25% of attacks against government in, underscoring the threat of data theft and espionage for government entities.

Foreign governments, cybercriminals, and even hacktivists have all demonstrated an interest in stealing data from government organizations.

Numbered divider 7
7/10 - Healthcare 

Healthcare ranked as the seventh most attacked industry, receiving 6.6% of all attacks on the top ten industries—up from the tenth place and 3% of attacks in previous years. 

This is an appreciable jump, and reflects the heavy targeting that healthcare received during the COVID-19 pandemic, from ransomware attacks to threat actors targeting COVID- related research and treatments.

Real-life image of an ambulance bringing a Covid-19 patient to Hospital
Ransomware Sodinokibi and Ryuk attacks
Nearly 28% of attacks on healthcare were ransomware. Ransomware attacks on healthcare can be particularly devastating—grimly illustrated by the story of a ransomware attack below:

In a German hospital in September 2020 a ransomware attack forced an ambulance to take a patient to a different hospital 20 miles away, after which the patient died. While German authorities determined that the ransomware attack did not play a decisive role in the death, in the future such attacks might directly lead to deaths.

When security researchers became aware of Ryuk cybercriminals’ plans to attack over 400 US hospitals in late October, US law enforcement and several security companies—including IBM Security X-Force—rushed to notify potential victims and identify mitigation measures. Thankfully, only seven of potentially over 400 hospitals were hit by Ryuk within the following week.

Scanning and exploiting vulnerabilities
In addition to ransomware, exploitation of CVE-2019-19781 to gain access to healthcare networks is very common. Healthcare was the third-most exploited industry through this CVE, making up 17% of such attacks on all industries.

In at least one instance involving this CVE on a healthcare network, threat actors combined their activity with PowerShell and Cobalt Strike for lateral movement and executing on objectives. 

Numbered divider 8
8/10 - Media and information communications

This sector includes telecommunications and mobile communications providers, as well as media and social media outlets that can play a critical role in political outcomes, especially during election years.

This industry became the eight most attacked, receiving 5.7% of all attacks on the top ten industries—down from fourth place last year, when it received 10% of attacks.

A person using apps on a smartphone
Misconfigurations
X-Force data identifies misconfiguration as the most common attack type on media, underscoring the importance of correctly configuring cloud instances to prevent unintended data leakage.

Spoofing
Quad9 data indicates that media was the top industry that malicious actors attempted to spoof by creating similar URLs to legitimate media outlets. Nearly 90% of all malicious DNS squatting—where a domain name is misleadingly similar to a legitimate webpage—involved media outlets.

This trend follows from the top brand spoofing trends noted earlier in this course and demonstrates that threat actors are seeking to capitalize on the popularity and trust consumers have in media organizations.

Numbered divider 9
9/10 - Transportation

Transportation experienced 5.1% of all attacks, down from 10% in previous years.

Reasons for this decrease could be attributed to the COVID-19 pandemic and stay-at-home orders may have decreased the profitability of this sector for threat actors—both cybercriminals attempting to capture financial information and nation-states tracking persons of interest.

Bird's eye view of a series of streets connecting in a roundabout
Key attack trends
Malicious insider and misconfiguration incidents had a disproportionately significant impact on transportation, particularly when compared with other industries. Together, these two attack types accounted for nearly 25% of the attacks on transportation last year.

The threat of insider attacks against transportation is significant, particularly given that some of the most damaging cyber attacks—including those that might lead to loss of life — become most feasible when an insider is involved.

Ransomware and server access attacks accounted for another 26% of attacks on transportation in 2020. 

Numbered divider 10
10/10 - Education 

The education sector ranked as tenth-most attacked, receiving 4.0% of all attacks on the top ten industries. This moves education down from the seventh-most attacked position, when it received 8% of all attacks.

Two female students wearing hiyab discussing infront of a computer inside a mixed-gender classroom
Spam and adware
Spam and adware were common attack types against education, together making up 50% of all attacks in the education sector. Approximately half of these originated from spam—a higher percentage than any other industry—highlighting the threat to education organizations from phishing-related threats.

Ransomware
Ransomware accounted for 10% of attacks on Education. Public breach data indicates that several schools and universities were hit with ransomware, with several of these opting to pay the ransom.

Botnets, fraud, and RATs
Botnets, fraud, and RATs also contributed to attacks on the education sector. Common cybercriminal attack techniques, phishing, and commodity malware appeared to be frequent threats to education organizations.

2. What is the impact of cyber attacks per geography? 

Every geography and industry faces a unique attack landscape, as different threat actors, motivations, assets and geopolitical events drive activity in each region and industry.

Geographic impact 

Europe, North America and Asia suffer the bulk of attacks, attracting threat actor activity probably due to the high percentage of the world’s wealth that circulates on these continents—over 89% of the world’s gross domestic product (GDP). Of these three, attacks on European organizations grew the most, driven by ransomware, insider and server access attacks. 

Threat analysis by geography

Explore the interactive graphic below, with information about the impact of cybersecurity attacks on each continent: 

Summary

1

1
The risk surface will continue to grow. With thousands of new vulnerabilities likely to be reported in both old and new applications and devices. 

2

2
Every industry has its share of risks. The year-over-year shift in industry-specific targeting highlights the risk to all industry sectors and a need for meaningful advancements and maturity in cybersecurity programs across the board.

3

3
Get in front of the threat rather than react to it. Leverage threat intelligence to better understand threat actor motivations and tactics to prioritize security resources.

Next

Let's review what are the security domains within the enterprise 

Sources:
[1] IBM Security. IBM X-Force Threat Intelligence Index 2021 
",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/22xhqOnlM8SzgxibOApA-MHFLQYWCxb_,
Data Science,Getting Started with Enterprise Data Science,16,Module 2 - Data Science on the Cloud,2,Topic 5: The role of the data scientist in an integrated Cloud environment,text,"The role of the data scientist in the exploration phase of the data science lifecycle. 
Introduction

In this topic, we'll explore the role of the data scientist in developing an AI model using Watson Studio. 

Throughout this topic, we will attempt to answer the following questions:

What is a data science model?
What tasks must the data scientist perform in order to develop a machine learning model for predicting fraudulent claims? 
1. What is a data science model? 

In practice, several people work on a team to build data products. Your analyses will only be as good as the team that is responsible for collecting, building, and analyzing the underlying data.

Because data scientists are involved in each step of the journey in building data products, they tend to bring a holistic view to solving problems with data. However, they can’t be experts in everything—this is where their team can help. 

Data scientist role in the integrated environment

A woman's profile named maria along with her job and what she does.
Maria
Data Scientist

Her Job: 
Transform data into knowledge to solve business problems.

What she does:

Follow the project end-to-end .
Discover content from multiple data sources.
Use popular statistical libraries.
Run experiments to build custom models that solve business problems.
Use techniques such as Machine Learning or Deep Learning and works with Sally to validate the  success of trained models.
Skilled data scientists, data engineers, developers, and business analysts are transformative figures in modern business. They are the beating heart of the big data economy. It’s not just that they are designing new systems; they are going to bat for new sources of data and new ways to use that data. Of course, IT still must build the system, but the data science professionals are the ones who help departments collaborate to solve problems and speed innovation. 

You defined business goals and spent hours digging through data. You even did a bit of ""data wrangling"" to handle missing values and convert the job experience categorical data field into something numerical. 

Now, what are you going to do with all that data, and how can what you do with it add value to the business? It's time to create a model.

Predictive analytics through data modeling techniques

Maria
Data Scientist

Problem
Now that we have sanitized and curated the data.

Challenge

What predictions can we make? 
Solution 
Data Models !

Data scientist maria talking on her phone
Data modeling is the process of creating a visual representation of either a whole information system or parts of it to communicate connections between data points and structures. The goal is to illustrate the types of data used and stored within the system, the relationships among these data types, the ways the data can be grouped and organized and its formats and attributes.

Data models are built around business needs. 
Rules and requirements are defined upfront through feedback from business stakeholders so they can be incorporated into the design of a new system or adapted in the iteration of an existing one.

Data can be modeled at various levels of abstraction. 
The process begins by collecting information about business requirements from stakeholders and end users. These business rules are then translated into data structures to formulate a concrete database design. A data model can be compared to a roadmap, an architect’s blueprint or any formal diagram that facilitates a deeper understanding of what is being designed.

Data modeling employs standardized schemas and formal techniques.
This provides a common, consistent, and predictable way of defining and managing data resources across an organization, or even beyond. Ideally, data models are living documents that evolve along with changing business needs. They play an important role in supporting business processes and planning IT architecture and strategy. Data models can be shared with vendors, partners, and/or industry peers.

2. What tasks must the data scientist perform in order to develop a machine learning model for predicting fraudulent claims? 

This is the point at which your hard work begins to pay off.  

The data you spent time preparing is brought into the data science toolset, and the results begin to shed some light on the business problem posed during the early stages of the project.  

Exploration phase : AI methodologies and Tools for Data Science

Model Development

Here, the data you've prepared is brought into the data science toolset, and the results begin to shed some light on previously identified business problems.

An image showing AI methodologies and tools for data science phase details.
Test and deploy models using customizable compute environments that scale up and down with your workflow. 
Choose from various capabilities of Anaconda, Apache Spark, and GPU environments.
Model development is usually conducted in multiple iterations. Typically, data scientists run several models using default parameters and then fine-tune the parameters or revert to the data preparation phase for manipulations required by their model of choice. It's rare for an organization's question to be answered satisfactorily with a single algorithm and a single execution. This is what makes data science so interesting. There are many ways to look at a given problem, and today there are a wide variety of tools to help you do that.

Although you may already have some idea about which types of models are most appropriate for your organization's needs, now is the time to make some decisions about which ones to use. Determining the most appropriate model will typically be based on the data types available, your project goals, and specific requirements for data sizes or types. 

An image showing ibm watson dashboard.
Types of Data Models

Data modeling has evolved alongside database management systems, with model types increasing in complexity as businesses' data storage needs have grown. Here are several model types:  


Hierarchical data models
+

Relational data models
+

Entity-relationship (ER) data models
+

Object-oriented data models
+

Dimensional data models
+
Two popular dimensional data models are the star schema, in which data is organized into facts (measurable items) and dimensions (reference information), where each fact is surrounded by its associated dimensions in a star-like pattern. The other is the snowflake schema, which resembles the star schema but includes additional layers of associated dimensions, making the branching pattern more complex. 

Bringing it all together

In the video below your instructor will guide you through these concepts:


AutoAI applies various algorithms to prepare your raw data for machine learning

The AutoAI process follows this sequence to build candidate pipelines:

Data pre-processing
Automated model selection
Automated feature engineering
Hyperparameter optimization
A dashboard showing an example for AutoAI process.
Data pre-processing

Most data sets contain different data formats and missing values, but standard machine learning algorithms work with numbers and no missing values. AutoAI applies various algorithms to analyze, clean, and prepare your raw data for machine learning. It automatically detects and categorizes features based on data type, such as categorical or numerical. Depending on the categorization, it uses hyper-parameter optimization to determine the best combination of strategies for missing value imputation, feature encoding, and feature scaling for your data. 

Build custom ML models using Open-Source libraries
Create ML flows and design Neural Networks visually

A dashboard showing ML models.
Data Analysis Outcomes





In Watson Studio, you can design your neural networks such as the pooling, ReLU, convolution and full circle stages, and you can do so using numerous open-source frameworks such as TensorFlow, Keras, Caffe and PyTorch.

Summary

1

1
A data model identifies the data, the data attributes, and the relationships or associations with other data. It provides a generalized, user-defined view of data that represents the real business scenario and data. 

2

2
Model development is usually conducted in multiple iterations. Typically, data scientists run several models using default parameters and then fine-tune the parameters or revert to the data preparation phase for manipulations required by their model of choice. 

Next 

The role of the data engineer in an integrated Cloud environment 

Sources:
[1] IBM Research. The Data Science Lifecycle: From experimentation to production-level data science. 
[2] IBM Watson Studio. Watson Studio at IBM  ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/24sfhvx1RCHwsFWXTiEyiwNaEx_VJ2j2,
Data Science,Getting Started with Enterprise Data Science,10,module 1 - Data Science Landscape,1,Quiz,text,,https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/c2QfDI4tXDdOM3TjlkHYw80mo32ARcFZ,
Artificial Intelligence,Machine Learning for Dummies,23,Applying Machine Learning,2,Applying Machine Learning,text,"Applying Machine
Learning
With machine learning, you have the opportunity to use
the data generated by your business to anticipate business change and plan for the future. While it is clear
that machine learning is a sophisticated set of technologies, it is
only valuable when you find ways to tie technology to outcomes.
Your business is not static; therefore, as you learn more and more
from your data, you can be prepared for business change.
Getting Started with a Strategy
Before you can define the strategy, you have to understand the
problem that you’re trying to solve. As businesses go through
major strategy transitions, certain challenges present themselves.
What is the status of existing business and existing customer
engagement? What does the future hold for what customers will
buy and expect from you in the future? The obvious answer is to
ask customers if they are happy and what they will purchase in
the future. While this is a sound starting point, it is not enough.
Customers that are happy one minute become unhappy when
something transformational comes along. If you do traditional
Chapter 2
IN THIS CHAPTER
» Getting started with your strategy
» Looking at machine learning techniques
in the business problem
» Tying machine learning to outcomes
» Understanding the business uses of
machine learning
20 Machine Learning For Dummies, IBM Limited Edition
These materials are © 2018 John Wiley & Sons, Inc. Any dissemination, distribution, or unauthorized use is strictly prohibited.
Business Intelligence (BI) analysis, you will have a good sense of
where your business has been in the past but not where it is going
in the future.
Your business isn’t static; much of the nuances and knowledge
about your customers is hidden inside structured, unstructured,
and semi-structured data. The value of machine learning techniques is to be able to uncover the patterns and anomalies in this
massive amount of data. Selecting the right machine learning
algorithms combined with the appropriate data sources helps you
to determine what’s next.
Using machine learning to remove
biases from strategy
Typically, strategic planning and strategy exercises begin by gaining insights into customer satisfaction and future requirements.
Where is the market headed? What are the competitive threats
that could impact the company? But this is not enough. Even the
best strategy consultants can’t anticipate the sudden emergence
of new discoveries or new trends.
One of the traps that company leadership falls into is its assumptions and biases. Too often company management looks at the
data presented and interprets the results through its own lens.
Is the business sustainable in light of emerging competitors with
unforeseen business models? While it is easy to be caught unaware
of change, the seeds of change exist. However, those leading indicators are often buried inside huge amounts of unstructured or
semi-structured data.
To gain benefit from a massive amount of unstructured data, it
is important to truly understand these data sources. What is the
source of the data? Who has manipulated that data? Are the data
sources reliable? Early experiences in advanced analytics often
resulted in disappointing results because analysts grabbed data
sources without vetting them first. Before taking action, the data
has to be verified as clean and accurate. After you are confident
that you’re using accurate data to address your business problem,
machine learning approaches can provide significant insights. At
the same time, you have to make sure that you have enough data
to discover the patterns and anomalies within that data.
CHAPTER 2 Applying Machine Learning 21
These materials are © 2018 John Wiley & Sons, Inc. Any dissemination, distribution, or unauthorized use is strictly prohibited.
After the data quality is good, it is important to understand the
context of the data being applied to the problem. For example, if
a tree is losing its leaves in the middle of the summer, it is a sign
that the tree is unhealthy. The same tree that has lost leaves in
the middle of a cold winter day is a normal occurrence. Therefore,
without understanding the context of data, you will likely misinterpret results. At the same time, there is considerable attention
paid to correlation between data elements. What are the relationships between conditions? In the example of the health of trees,
there is a direct correlation between the seasons and the color
and amount of leaves on the trees. But you also have to be careful about correlations. You might find a correlation that makes
no sense because the context is wrong. There may seem to be a
correlation between leaves falling off the trees and the number of
coats being purchased online. While both events are happening
because the weather is colder, there is no relationship between
trees and coats.
For the business to effectively use machine learning to support
business strategy, you need these statistical methods to find
patterns and anomalies in these data sets. With the best data
available and in the right volume and the best level of cleanliness, it is possible to create a model by using the most appropriate machine learning algorithm based on the business problem
being addressed. This model is only the beginning of the machine
learning workflow.
By leveraging massive amounts of data, it is possible to model
data, train the data, and then begin to learn from that data in
order to improve the ability to make decisions. The value of learning from data means that the machine learning system is able to
look at underlying patterns and anomalies that aren’t necessarily obvious. Are there relationships between what customers buy
with the time to repair? Are there impacts of weather on sales
during a period of time? Are there indications in social media data
that indicate subtle changes in customer perceptions or buying
patterns? Being able to model massive amounts of data from different data sources can add insights that no single human could
have understood by simply relying on data available in isolation.
There has been much discussion about correlation of data as an
analytic method. While data correlation is incredibly important, it
can sometimes be misleading. There may seem to be a correlation
between the consumption of orange juice in June and the rise in 
22 Machine Learning For Dummies, IBM Limited Edition
These materials are © 2018 John Wiley & Sons, Inc. Any dissemination, distribution, or unauthorized use is strictly prohibited.
traffic accidents in the same month, but there is no causal relationship. Therefore, while correlation might be useful in certain
cases, it can also lead to inaccuracies. This is why context is even
more important. If there were a useful context between orange
juice and traffic accidents, then the correlation would be useful.
Therefore, as you move to leverage machine learning as part of
planning and strategy process, you need to make machine learning and advanced analytics indispensable tools.
More data makes planning
more accurate
What difference could machine learning make in business strategy? Take the example of a business that executes a traditional
data analysis of customer satisfaction. In analyzing the data, it
becomes clear that some anomalies in the data exist. Because
of the data set being used, the analyst throws out the data that
doesn’t conform, assuming that this data is not accurate. However, if more data did exist, it may become clear that those anomalies that were assumed to be errors are actually an indication of
a change in customer buying patterns or customer satisfaction.
As more data is added into a model, trained, and analyzed with
the most appropriate machine learning algorithms, it becomes
increasingly clear that there are changes that will directly impact
the future of the business.
For example, data scientists seeing some subtle changes will
begin to add new data sources that will strengthen or debunk a
statistical analysis about business change or growth. Over time
as more data is ingested into the model, the system learns and
gains more insight and more sophistication in order to predict
the future. Therefore, machine learning becomes an invaluable
partner in strategic planning.
Understanding Machine
Learning Techniques
In order to ensure that your data scientists are using the right
machine learning techniques to achieve your business goals, it is
important to understand how your organization can best apply
these advanced techniques to manage your growth and keep
focused on emerging opportunities.
CHAPTER 2 Applying Machine Learning 23
These materials are © 2018 John Wiley & Sons, Inc. Any dissemination, distribution, or unauthorized use is strictly prohibited.
Machine learning is a systematic approach to leveraging advanced
algorithms and models to continually train data and test with
additional data to begin to apply the most appropriate machine
learning algorithms to a problem (we discuss this in more detail
in Chapter 1). The advantage of machine learning is that it is possible to leverage algorithms and models to predict outcomes. The
trick is to ensure that the data scientists doing the work are using
the right algorithms, ingesting the most appropriate data (that
is accurate and clean), and using the best performing models. If
all these elements come together, it’s possible to continuously
train the model and learn from the outcomes by learning from
the data. The automation of this process of modeling, training the
model, and testing leads to accurate predictions to support business change.
Tying Machine Learning
Methods to Outcomes
Machine learning techniques have the potential to reshape entire
markets and business strategies. For example, machine learning
techniques are being used to transform the automobile industry
with self-driving cars. Machine learning algorithms and models
are revolutionizing the way an x-ray image is analyzed. Machine
learning can provide proactive ways of anticipated security vulnerabilities that can be repaired before damage is done. There are
hundreds of different solutions that can be created that rely on
machine learning techniques that can transform whole industries.
Different approaches and algorithms exist for machine learning,
depending on the problem being addressed. You need to understand the problem you’re trying to solve. The model you design
will represent an understanding of the data and your ability to
predict outcomes based on that data.
Applying Machine Learning
to Business Needs
Machine learning offers potential value to companies trying to
leverage big data and helps them better understand the subtle changes in behavior, preferences, or customer satisfaction. 
24 Machine Learning For Dummies, IBM Limited Edition
These materials are © 2018 John Wiley & Sons, Inc. Any dissemination, distribution, or unauthorized use is strictly prohibited.
Business leaders are beginning to appreciate that many things
happen within their organizations and with their industries that
can’t be understood through a query. It isn’t the questions that
you know; it’s the hidden patterns and anomalies buried in the
data that can help or hurt you. In this section, we provide some
examples of how companies are beginning to use machine learning techniques to create business differentiation.
Understanding why customers
are leaving
Have you ever heard, “It costs a lot less to keep an existing customer than to gain a new customer”? Customer churn is a constant problem in certain industries, such as telecommunications,
retail, and financial services.
Understanding how to prevent customers from leaving is more
important than ever. We are in an era where emerging companies are offering new innovative business models. For example,
mobile phone service providers used to demand a two-year contract, which was extended each time the service changed. As the
competitive landscape shifted, companies found that they had to
get rid of the contracts. This change was beneficial to customers
but resulted in a huge spike in customer churn. Without the protection of customer contracts, mobile companies are turning to
new approaches to keep customers.
In order to prevent customer churn, it is critical that you have
enough data about the customer’s history, his preferences, the
services he has purchased in the past, and his complaints. In a
highly stable market, this approach to analytics might have been
a predictor of the future. But in volatile markets, this approach
will not work. You have to be able to anticipate market changes
and changes in customer buying patterns. Using machine learning models can help you predict changes that will impact revenue.
In essence, the mobile provider needs to be able to look at patterns from data as well as anomalies. The mobile provider has
the benefit of having access to huge volumes of data across many
different customers. By using the right algorithm, the vendor can
create a model that maps the types of offerings and promotions
that will retain customers and add new ones. How much will it
cost to retain and add new customers? Will new plans reduce revenue significantly? Will the spending justify the efforts? These 
CHAPTER 2 Applying Machine Learning 25
These materials are © 2018 John Wiley & Sons, Inc. Any dissemination, distribution, or unauthorized use is strictly prohibited.
are the types of predictions that a machine learning technique
can provide.
What is the difference between a traditional BI approach and a
machine learning approach to customer churn? With traditional
BI, the organization is able to understand what has happened in
the past and can evaluate trends of customer loyalty. In contrast,
the machine learning algorithm creates a model that brings in
massive amounts of both internal and external data. After the data
is trained and tested, analysts can begin to anticipate changes in
customer preferences. The model may be able to anticipate how
customers’ buying patterns will change in the future.
Machine learning uses statistical algorithms as the foundation to
creating a model that can learn and predict. The most common
models used for predictive models for churn analysis are classification statistical algorithms, such as logistic regression and
neural networks.
Recognizing who has
committed a crime
Police departments have a difficult task when tracking criminals.
Increasingly, there are more and more cameras in neighborhoods
that help identify unlawful activity. But who has committed the
act? While a picture may be worth a thousand words, without
someone to identify the bad actor, it isn’t easy to solve crimes.
One of the ways law enforcement is trying to leverage image data
is through the use of machine learning.
Specifically, deep learning algorithms and neural network–
based algorithms are best suited to deal with facial recognition.
In essence, neural networks are intended to emulate the human
brain. By using a neural network algorithm, people can identify
clusters and patterns in images. Image analytics can index and
search video events by classifying objects into different categories, such as people, cars, roads, or streetlights. Further, facial
recognition algorithms can be used to digitize sections of a photograph of a person in a way that eliminates extraneous data that
isn’t useful. The most important elements needed to identify a
person include the eyes, nose, mouth, and things like scars. By
collecting massive amounts of data of facial images, the algorithm can identify patterns in faces. Testing becomes a core technique that helps the model discriminate between two different 
26 Machine Learning For Dummies, IBM Limited Edition
These materials are © 2018 John Wiley & Sons, Inc. Any dissemination, distribution, or unauthorized use is strictly prohibited.
faces. Some of the emerging neural network techniques enable
this type of training to be done with sparse data, which makes
these systems more practical for a police force.
How would a police force take advantage of this type of neural
network? The solution incorporates image data of known criminals. It includes data collected by surveillance cameras as well as
images of suspicious individuals who might be involved in crimes
locally. When a crime happens, such as a robbery at a local store,
the images from the cameras can identify the faces of the individuals involved. These images can be matched against the quantity of data. Basically, the model is looking to match the pattern
of a specific face against the collection of images to see if there is
a match. If police can find the match, they will be able to quickly
make an arrest without first taking the time to interview witnesses and spending hours reviewing store videos.
Preventing accidents from happening
Many industries rely on sophisticated preventive maintenance
approaches to ensure that processes and systems are safe and
operate as expected. Industries such as manufacturing, oil and
gas, and utilities succeed or fail based on their ability to prevent
accidents. While it is common to have a maintenance schedule,
that is often not enough. For example, there may be environmental conditions that impact the operations of a machine or system.
For example, there may be a failure of a heating or air conditioning system. There could be a dramatic shift in weather conditions
that could impact machinery.
Machine learning algorithms can be applied to preventive maintenance in a number of ways. For example, a regression algorithm
can be used as the foundation for a model that can predict time
to failure of a machine. Various classification algorithms can be
used to model the patterns associated with machine failures. Data
generated by sensors provides a huge volume of semi-structured
data that can model and compare patterns of performance so that
an anomaly from normal performance can be detected.",https://www.ibm.com/downloads/cas/GB8ZMQZ3,
IBM Cloud,Overview,0,IBM Cloud,0,What is IBM Cloud?,text,"What is IBM Cloud?
IBM Cloud is a full-stack cloud platform that spans public, private and hybrid environments. Build with a robust suite of advanced data and AI tools, and draw on deep industry expertise to help you on your journey to the cloud.

What are the benefits of Cloud Computing?
Flexibility - Users can scale services to fit their needs, customize applications and access cloud services from anywhere with an internet connection. 

Efficiency - Enterprise users can get applications to market quickly, without worrying about underlying infrastructure costs or maintenance. 

Strategic value - Cloud services give enterprises a competitive advantage by providing the most innovative technology available.",https://www.ibm.com/academic/topic/cloud,
IBM Security,Getting Started with Threat Intelligence and Huntin,12,Module 2 - Threat Hunting,2,Topic 2: Why Threat Hunting?,text,"
The best way to protect yourself is to know your enemy

Introduction

Explore the rise of threat intelligence and hunting practices to deal with advanced threats that cause the most damage.

Gain an understanding of the concepts covered in this lecture, by going through the videos, reading material and, slides included in the sections below:

Why do we need Cyber Threat Hunting?
What is the Human-vs-Human approach?
What is the security structure within an organization?
What is the purpose of adversary cyber-attack models?
What is the IBM X-Force IRIS Cyber Attack Framework?
1. Why do we need Cyber Threat Hunting?

In order to attack the full cyber threat spectrum, an organization must embrace both information security and the natural evolution of cyber analysis which includes the use of cognitive tools and is commonly called “Cyber Threat Hunting.”

Information security creates a foundation of security with a framework and builds upon that with some specialization and technology. Eventually, the security process evolves into cyber analysis with long-term research and ecosystem visibility concerning malicious actors.

Why there is a need for Cyber Threat Hunting?

The sophistication of attackers is outstripping the organization's defenses

Advanced threats in cyberspace are highly resourced, highly sophisticated bad guys that can evade detection from the rule and policy-based defenses. 

Dwell time is the time an attacker enters the network to the time of detection. The average dwell time is 191 days. This is a lot of time for the bad guys to achieve their objectives.

They dwell in the network and although advanced threats might be a smaller percentage of threats, they can cause the most damage. The longer an attacker spends in your network the more damage they can do and the higher the cost of an attack.

Slide9_NOPROCESS_.png
Slide11.png
They use sophisticated attack kits that compare with the best tools our national security agencies have available today are constantly developed and sold on the dark web.

Sophisticated developers who spent years honing their hacking techniques can now profit from their experience by selling hacking toolkits as a software package. Exploit kits attack known vulnerabilities to deliver malicious payloads of the attacker’s choice. New exploit kits are continuously being developed with different attack vectors and infection techniques. At any given time there are dozens of exploit kits available — including Zeus variants, FlokiBot, NukeBot, and GM Bot — and the widespread use of these tools has increased the sophistication of tactics, techniques, and procedures among a full spectrum of attackers.

Slide12.png
20% of threats are unknown, undetected, and can cause the most damage

Typically when experts discuss the breakdown of cyber threats, the 80/20 principle is brought up — meaning 80 percent of cyber actors are generally less sophisticated and the top 20 percent are so advanced that given enough time and resources they will break onto any network. Historically, the top 20 percent of actors were mainly the concern of the defense and intelligence community. 

Slide13.png
That's the reason we need Cyber Threat Hunting

Human-led analysis capabilities help us quickly find hidden connections and critical patterns buried in internal, external, and open-source data.

2. What is the Human-vs-Human approach?

Adversarial goals and tactics, techniques, and procedures (TTPs) can be very different for each incident, but all attacks share some core concepts that defenders can work with to expose malevolent activity before it causes damage.

Adversarial goals and tactics, techniques, and procedures (TTPs) can be very different for each incident, but all attacks share some core concepts that defenders can work with to expose malevolent activity before it causes damage.

Approach to Threat Hunting

Why the rise in threat hunting and how does threat hunting help you deal with the undetected threats that cause the most damage? It helps you detect threats faster by proactively looking for threats instead of waiting for an alarm to go off.

Slide15.png
Human-versus-human approach to understanding the enemy

Policy-based security solutions such as endpoint protection, network security, and identity and access management are necessary forms of threat protection. But these solutions are even more effective when combined with the power of human insight into human behavior that can be developed through multidimensional analysis. To stay ahead of threats, security and intelligence analysts need to think like the “bad guys” they’re trying to defeat, whether that’s in the world of cybersecurity or mission intelligence.

Slide16.png
Watch the Video

Listen as our expert covers this topic


3. What is the security structure within an organization?

By connecting, analyzing, and visualizing disparate data sources time insight that can help provide unique intelligence a Security Operation Center team can detect, disrupt and defeat threat actors.

Cyber security solutions today provide a human-versus-human approach to understanding the enemy. Powerful analytics can help analysts develop insights to understand and anticipate where threat actors might strike next.

The role of Security Operation Center teams

Security Intelligence Analyst teams in an organization are located generally within a Security Operations Center, their organizational structure and capabilities are essential to the capacity of the organization to respond and prevent a security incident.

The leverage data from different sources, including open-source and third-party data stores, whether from a human resources system, a crime database, or social media.

Watch the Video

Listen as our expert covers this topic


4. What is the purpose of adversary cyber-attack models?

To build an effective cyberattack security strategy, organizations need to thoroughly understand exactly how cyberattacks occur from the attacker’s perspective.

See firsthand the approach cyber attackers use to get into your system, from the moment of its inception within the Dark Web of hackers, to when it is sent around the world to infect as many systems as possible.

The Cyber Attacker Adversary Framework

Some [exploits] could be malicious or hacked websites to steal information, but there are other approaches. Criminals can send highly credible-looking phishing emails to trick employees into downloading malware that gives hackers access to secure systems. These attacks continue to spread because companies don't share information with each other on how these attacks are perpetrated.

When the file is open, Malware installs on a user's device, where it can hide for months. Without the right security in place, the user suspects nothing and security tools detect nothing. The attackers are now inside the organization. They use this position to infect other systems on the network and search for sensitive information. They copy the stolen information to a remotely accessible server and wait until the opportune moment, download it, and destroy all records of the attack. 

MITRE ATT&CK™ FRAMEWORK

ATT&CK™ for Enterprise is an adversary model and framework for describing the actions an adversary may take to compromise and operate within an enterprise network. The model can be used to better characterize and describe post-compromise adversary behavior. It both expands the knowledge of network defenders and assists in prioritizing network defense by detailing the tactics, techniques, and procedures (TTPs) cyber threats use to gain access and execute their objectives while operating inside a network.

Slide18.png
ATT&CK for Enterprise incorporates information on cyber adversaries gathered through MITRE research, as well as from other disciplines such as penetration testing and red teaming to establish a collection of knowledge characterizing the activities adversaries use against enterprise networks. While there is significant research on initial exploitation and use of perimeter defenses, there is a gap in central knowledge of adversary processes after initial access has been gained. 

ATT&CK for Enterprise focuses on TTPs adversaries use to make decisions, expand access, and execute their objectives. It aims to describe an adversary's steps at a high enough level to be applied widely across platforms, but still maintain enough details to be technically useful.

Watch the Video

Listen as our expert covers this topic


5. What is the IBM X-Force IRIS Cyber Attack Framework?

Experts within the IBM® X-Force® Incident Response and Intelligence Services (X-Force IRIS) team has developed a comprehensive framework to address all actions an attacker takes

Empowering security analysts and threat hunters with the insight they need to narrow risk exposure and thwart ever-increasing cyberattacks.

IBM X-Force IRIS Cyber Attack Framework

The framework outlines each phase of a cyberattack so security analysts can examine them in a repeatable and comprehensive way. The phases in a cyberattack don’t necessarily happen sequentially, or at all. Depending on how the attack progresses, the phases may occur simultaneously or through multiple iterations—or may be skipped entirely.

How does it work?

The framework includes a Preparation phase and an Execution phase, learn more about the individual steps in each phase in the section below.

IRIS Cyber Attack Framework

Perform the following activities to learn more:

Interactive Activity #1 - Know your Enemy: Includes information about the framework and details the attacker steps.

Interactive Activity #2 - Provides defense tips to protect from cyber attackers at every step

Numbered divider 1
INTERACTIVE ACTIVITY #1 - ""KNOW YOUR ENEMY""

Click on each one of the attacker's steps to identify his modus operandi or criminal behavior, understanding what are the key approaches the attacker uses to gain momentum.


















































Watch the Video

Explore this concept in more detail through an audio-visual journey


Numbered divider 2
INTERACTIVE ACTIVITY #2 - ""DEFENSE TIPS""

Click on each one of the framework steps below to identify defense strategies that can be used to avoid giving attackers the upper hand and diminish the exposure to security incidents.















































Establish a cyberattack strategy

To effectively prevent bad actors from entering your network, take a holistic view of cyberattack techniques— from the attacker’s perspective.

The X-Force IRIS framework provides a comprehensive and repeatable analysis of each step in the attack lifecycle to empower security analysts and threat hunters to better understand, track and defend against patterns of malicious behavior.

Watch the Video

Listen as our expert covers this topic


Summary

1

1
Threat hunting is needed because even though only 20% of the threats are unknown those are the ones that do the most damage, and unknown threats needed expert threat hunters to take action.

2

2
The sophistication of attackers are outstripping the organization's defenses

3

3
To create a comprehensive security strategy you need to know your enemy, understand their Cyber Attack Framework

Next 

The art and science behind threat hunting

Sources:

1. Smith, S., 2017. Cybercrime to Cost Global Business Over $8 Trillion in the Next 5 Years. [online] Juniperresearch.com.  
2. Security Intelligence. n.d. Prepared for the GDPR? Top 10 Findings From Hurwitz & Associates Survey. [online] 
3. IBM.COM n.d. Intelligence Analysis Solutions for National Security and Defense | IBM. [online]
4. Security Intelligence. n.d. IBM X-Force IRIS Cyberattack Preparation and Execution Frameworks - Security Intelligence. [online]
5. Ibm.com. n.d. Detect, disrupt and defeat advanced physical and cyber threats. [online]
6. Berninger, A., 2018. How a Cyberattack Framework Can Help Reduce Risk at All Levels, Part 2. [online] Ibm.com.
7. Wrolstad, J. and Berninger, A., 2021. Cyberattack Preparation and Execution Frameworks. [online] Ibm.com.
8. Attack.mitre.org. n.d. MITRE ATT&CK for Enterprise. [online] 
",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/ourf4ShB5klP80KqFSAEcvgSFAMwQVH7,
IBM Cloud,Journey to Cloud: Envisioning your Solution,7,Module 1 - Digital Transformation with Cloud Computing,1,Topic 4: Cloud Delivery Models,text,"Weighing the benefits and challenges of choosing a cloud delivery model.

Introduction

In this topic, we'll identify the main cloud delivery models and explore some key considerations when choosing a CDM. 

Throughout this topic, we will attempt to answer the following questions: 

What defines Traditional IT Architecture and what are the pros and cons of continuing to keep operations on-premise?
What differentiates Public and Private delivery models?
How does Hybrid Cloud solve many of the problems posed by Public and Private cloud deployment models?
What considerations need to be made (in terms of security, storage, and pricing) before choosing a cloud service model?
1. What defines Traditional IT Architecture and what are the pros and cons of continuing to keep operations on-premise?

Within the next three years, 75 percent of existing non-cloud apps will move to the cloud. Today’s computing landscape shows companies not only adopting cloud but using more than one cloud environment. Even then, the cloud journey for many has only just begun, moving beyond low-end infrastructure as a service to establish higher business value. 

Choosing a Cloud Deployment Model   

Cloud computing represents a massive paradigm shift in the way IT resources are created, delivered, and consumed. Cloud computing technology also serves as a core foundational driver of growth and innovation across the global IT landscape. However, the cloud computing solution space can indeed be confusing with a lot of new terms and technologies. Even organizations that have been using the Internet and cloud computing for basic tasks like mobile email, can go a little deeper and take a broader view of cloud computing. 

Image showing a person thinking before making a choice.
Enterprises that are eager to undergo digital transformations and modernize their applications are quick to see the value of adopting a cloud computing platform. They are increasingly finding business agility or cost savings by renting software. Each cloud computing service and deployment model type provides you with different levels of control, flexibility, and management. Therefore, it’s important to understand the differences between them.

The right model depends on your workload and budget. You should understand the advantages and disadvantages of each cloud deployment model and take a methodical approach to determine which workloads to move to which type of cloud for the maximum benefit. 

Traditional infrastructure 

Before we dive into cloud, it's important to understand the foundations upon which cloud arose. Prior to cloud computing, companies stored their data, software, and operations on-premise, using their own IT servers. This increasingly outdated model is called Traditional Infrastructure, and sometimes referred to as 'legacy'  or 'heritage' IT architecture.

Two people in Data Center while walking next to server Racks
A traditional IT infrastructure is made up of the usual hardware and software components: facilities, data centers, servers, networking hardware desktop computers, and enterprise application software solutions. Typically, this infrastructure setup requires more power, physical space, and money than other infrastructure types. Traditional infrastructure is typically installed on-premises for company-only, or private, use.

Organizations can have compelling reasons for keeping a legacy system, such as the system works satisfactorily and the owner sees no reason to change it, or certain data is seen as being too sensitive to store on a public cloud environment. Often the cost is one of the biggest concerns for companies unsure of whether to migrate some or all of their operations to the cloud. The costs of redesigning or replacing the system are prohibitive because it is large, monolithic, and/or complex. Retraining on a new system would be costly in lost time and money, compared to the anticipated appreciable benefits of replacing it (which may be zero).

Even if it is no longer used, a legacy system may continue to impact the organization due to its historical role. Historic data may not have been converted into the new system format and may exist within the new system with the use of a customized schema crosswalk, or may exist only in a data warehouse. In either case, the effect on business intelligence and operational reporting can be significant. A legacy system may include procedures or terminology which are no longer relevant in the current context and may hinder or confuse understanding of the methods or technologies used.

2. What differentiates Public and Private, and Hybrid cloud delivery models? 

Comparing Cloud Delivery Models 

Image showing the structure of public cloud showing different resources connected to the common cloud.
PUBLIC CLOUD
PRIVATE CLOUD 
A public cloud is perhaps the simplest of all cloud deployments: A client needing more resources, platforms, or services simply pays a public cloud provider by the hour or byte to have access to what’s needed when it’s needed. Infrastructure, raw processing power, storage, or cloud-based applications are virtualized from hardware owned by the vendor, pooled into data lakes, orchestrated by management and automation software, and transmitted across the internet—or through a dedicated network connection—to the client.


Public cloud architectures are multi-tenant environments—users share a pool of virtual resources that are automatically provisioned for and allocated to individual tenants through a self-service interface. This means that multiple tenants’ workloads might be running CPU instances running on a shared physical server at the same time. Each cloud tenant’s data is logically isolated from that of other tenants, however.


Think about it like this. Cloud computing is the result of a meticulously developed infrastructure, kind of like today’s electric, water, and gas utilities are the result of years of infrastructural development. Cloud computing is made available through network connections in the same way that utilities have been made available through networks of underground pipes.


Homeowners and tenants don’t necessarily own the water the comes from their pipes; don’t oversee operations at the plant generating the electricity that powers their appliances, and don’t determine how the gas that heats their home is acquired. These homeowners and tenants simply make an agreement, use the resources, and pay for what’s used within a certain amount of time.


Public cloud computing is very similar. The clients don’t own the gigabytes of storage of the data they use; don’t manage operations at the server farm where the hardware lives; and don’t determine how their cloud-based platforms, applications, or services are secured or maintained. Public cloud users simply make an agreement, use the resources, and pay for what’s used.

Public Cloud or Private Cloud? 

In general, public cloud is a better choice if the following are true:

Scalability and elasticity—the ability to add capacity instantly or automatically in response to unexpected surges in traffic—are important to you. 
You’d like to avoid upfront capital expenses and prefer more predictable ongoing operating expenses.
You want unlimited access to particular resources that are available through a public cloud provider.
However, if you have highly specialized security, regulatory, or infrastructure needs, want maximum control over your cloud environment, and find that your workloads have predictable usage patterns, a private cloud or private cloud-like service could better a good fit. 

3. How does Hybrid Cloud solve many of the problems posed by Public and Private cloud deployment models? 

Hybrid cloud integrates private and public clouds, using technologies and management tools that allow workloads to move seamlessly between both as needed for optimal performance, security, compliance, and cost-effectiveness.

For example, hybrid cloud enables a company to keep sensitive data and mission-critical legacy applications (which can’t easily be migrated to the cloud) on-premises, while leveraging public cloud for SaaS applications, PaaS for rapid deployment of new applications, and IaaS for additional storage or compute capacity on demand.

Hybrid Cloud 

Image showing structure of hybrid cloud which allows to combine cloud applications and data.
Today’s hybrid clouds are architected differently. Instead of connecting the environments themselves, modern IT teams build hybrid clouds by focusing on the portability of the apps that run in the environments.

Think about it like this: Instead of building a local two-lane road (fixed middleware instances) to connect two interstate highways (a public cloud and a private cloud), you could instead focus on creating an all-purpose vehicle that can drive, fly, and float. Either strategy still gets you from one place to another, but there's a lot less permitting, construction, permanency, and ecological impact if you focus on a universally capable vehicle.

Modern IT teams build hybrid clouds by focusing on the car—the app. They develop and deploy apps as collections of small, independent, and loosely coupled services. By running the same operating system in every IT environment and managing everything through a unified platform, the app's universality is extended to the environments below it. 

Hybrid Cloud Explained 


4. What considerations need to be made (in terms of security, storage, and pricing) before choosing a cloud delivery model? 

Things to Consider When Choosing a CDM 

Image showing people working on the things done with cloud technology

Cloud Computing - Storage
+

Cloud Computing - Pricing
+

Security Issues in Cloud Computing
+
10 Tips for Choosing the Right Cloud Delivery Model 

Image showing people thinking before making a choice.
Do your homework. Don’t assume that the provider that’s currently in the news with price decreases will be the best-priced provider for your workload.
Understand all workload requirements that will impact cloud workload costs and operations (not just compute and storage). Consider costs associated with licensing software for each core, data transfer to the internet or private network, and persistent storage.
Understand how the provider will support geographically-dispersed workloads. If your application must move data throughout the globe, ensure that the provider not only has data centers in the regions where you do business, but also a high-performance, private global network. Also consider whether the provider charges data transfer fees between and among cloud centers — any such fees can considerably add to costs if your company expands globally.
Consider your business requirements, including the “agility” tradeoff. Can you afford to lock into a specific provider, unit type, volume or time frame, even if it means a discount?
Consider the “net present value of money” when you evaluate long-term pricing options. Seek input from your finance department, especially if you are considering an upfront payment option. This will ensure your comparisons are valid and adhere to your company’s accounting rules for net present value.
Factor in the non-workload-specific costs your business will need to run the workload optimally, including technical support, engineering, and even professional services.
Allow for changing workload needs. You should be able to move workloads as needed from, for example, bare metal to virtualized servers without a major effort.
Consider the big picture. Each cloud workload should fit into a holistic cloud strategy, one that will likely comprise multiple deployment models, geographies, and vendors.
Even as you consider price performance based on the individual cloud workload, consider the provider’s ability to support your broader hybrid IT strategy via OpenStack-compatible platforms, integrated solutions, and seamless migration across models.
Finally, don’t just look at the price for the workload. Instead, consider price-performance to be your base unit of comparison as you consider cloud options or any type of IT option.
Why it matters? 
One of the most important features of cloud computing is that it gives you the ability to customize a cloud solution to your business needs. Choosing a cloud delivery model, however, involves careful consideration of factors such as cost, speed, security, and maintaining compliance with government regulations. 

Summary

1

1
A legacy system is an old method, technology, computer system, or application program, ""of, relating to, or being a previous or outdated computer system,"" yet still in use. 

2

2
Public cloud is a type of cloud computing in which a third-party service provider makes computing resources—which can include anything from ready-to-use software applications to individual virtual machines (VMs), to complete enterprise-grade infrastructures and development platforms—available to users over the public Internet. 

3

3
Private cloud (also known as an internal cloud or corporate cloud) is a cloud computing environment in which all hardware and software resources are dedicated exclusively to, and accessible only by, a single customer.

4

4
Hybrid cloud uses technology that allows you to combine or “stitch together” cloud applications and data that spans traditional on-premise IT systems, private cloud services, and data and public cloud services.

Next 

We compare the three main cloud service types: IaaS, PaaS, and SaaS and evaluate the criteria for selecting an appropriate cloud service. 

Sources:
[1] Barillaud, Franck. Cloud technologies: How they all fit together. June 19, 2017. 
[2] IBM Cloud Education. Private Cloud. April 10, 2020. 
[3] IBM Cloud Education. Public Cloud. March 3, 2020. 
[4] Legacy System. Wikipedia. Wikimedia Foundation. October 7, 2021. 
[5] Red Hat. Types of Cloud Computing. March 15, 2018. 
[6] U.S. General Services Administration. Cloud Information Center. Technical Implementation.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/mp5Z1tda8zhiR3XDfT5NeldPAKx7TXQv,
IBM Automation,IBM Robotic Process Automation - Basic I,13,Advanced Commands,12,File Manipulation I,video,,https://learn.ibm.com/mod/video/view.php?id=152647,
IBM Automation,IBM Robotic Process Automation - Basic I,22,Advanced Commands,21,Text Manipulation I,video,,https://learn.ibm.com/mod/video/view.php?id=152665,
IBM Engineering,Quick Start Sessions,9,Quick Starts: IBM Engineering Requirements Management DOORS and ELM,9,What is IBM Engineering Requirements Management DOORS and ELM?,text,"IBM Engineering Requirements Management DOORS is a leading requirements management tool that makes it easy to capture, trace, analyze, and manage changes to information. IBM Engineering Lifecycle Management (ELM) is an integrated set of capabilities for systems and software engineering, product development and managing complexity. Integrating DOORS with the Engineering Lifecycle Management solution allows requirements engineers to enjoy all of the benefits of the wider ELM solution. In this lab you will explore some of the aspects of that integration.",https://learn.ibm.com/mod/page/view.php?id=170880,
Data Science,Overview,0,Data Science,0,What is Data Science?,text,"What is Data Science?
Today’s businesses are awash in data with more streaming in every day.  Simply having the data, however, doesn’t guaranty that anything useful can be extracted from it.  That’s where data science comes in.  Data science is the practice of leveraging a unique set of skills and tools to help businesses derive value from their data.

Why is Data Science important?
Answers to the most important questions facing a business often lie within its own data.  Data science enables businesses to mine that data for drivers of past and future performance, opportunities for new revenue, methods to increase efficiency, ways to improve customer experience and a host of other critical business insights. In short, data science provides an unbiased examination of the past while pointing the way to the future.

Courseware
Software
Resources
OpenDS4All
OpenDS4All is a project created to accelerate the creation of data science curriculum at academic institutions.

Getting Started with Enterprise Data Science
This course provides a basic understanding of the foundations of Data Science including: Data Science Team Roles, Data Analysis Tools, and real-world use cases.

Constraint Programming with ILOG CP Optimizer
Fundamentals of constraint programming (CP). CP is demonstrated through the use of ILOG CP Optimizer via ILOG OPL-CPLEX/ILOG Analyst Studio, a complete integrated [...]

Overview of IBM Cognos Analytics
This course provides participants with a high level overview of the IBM Cognos Analytics suite of products and their underlying architecture. They will examine each component as it relates to an Analytics solution.

IBM Planning Analytics - Design & Develop Models in Workspace
This course is designed to teach modelers how to build a complete model in IBM Planning Analytics. Through a series of lectures and hands-on exercises, students will learn how to set up dimensions [...]

View All
Important skills when working with Data Science
Curiosity, creativity and critical thinking are all hallmarks of a successful data scientist. And until recently, getting the most out of data required a working knowledge of SQL or statistical programming languages such as R or Python. However, modern data science tools have opened the door to the business user or citizen data scientist, as drag-and-drop, visual data modeling has made no-code data science a reality. ",https://www.ibm.com/academic/topic/data-science,
IBM Automation,IBM Robotic Process Automation - Basic I,9,Basic Commands and Variables,8,Variables - Use case II,video,,https://learn.ibm.com/mod/video/view.php?id=152637,
Data Science,Getting Started with Enterprise Data Science,28,Final Remarks,4,Final Remarks,text,"Key Takeaways

If you went through all the activities included in this course, you are now on your path to becoming a Data Science Practitioner. 

As a Data Science practitioner most times than not, you will be the driving force for Data Science adoption within your organization, so below are a few key items to remember from this course that may be relevant to future projects and field practice: 

1

1
Data science is a cross-disciplinary set of skills found at the intersection of statistics, computer programming, and domain expertise.

2

2
A successful Data Science practice knows how to fail, adapt and progress your business.

3

3
AI trust is paramount in your AI strategy, and to reach that, you must build an appropriate culture for your data science team and for their relationship with the broader organization 

4

4
Your data is your unique value add to your AI use cases. Break down the data silos, collaborate across teams, and get the appropriate tools and processes in place to unlock the value of your data. 

5

5
Provide the right tools to your data science team to enrich the team’s culture and skills, both for immediate purposes and as their curiosity and insights and organization learning grows.

6

6
Lastly – focus on business outcomes, iterate quickly, fail fast, and use an Agile AI approach throughout this entire process. Utilize feedback loops that you can recognize and nurture to allow your team to improve continuously.

7

7
Now that you had an overview of the overall tools, roles, and domains, identify which ones are the ones you feel more aligned with your current or future role, and look for additional certifications and learning opportunities in that area of study.

Remember to promote achievement in social media

Once you complete this course you will be eligible to receive an IBM official digital badge - recognizing the level of knowledge you have achieved.

Follow the instructions below to make the best of your digital badge:

Accepting your digital credential


Click to flip
Most digital credentials are issued automatically by IBM upon completion of the course requirements.
Once issued, you will receive a notification email from admin@youracclaim.com with instructions for claiming the badge.
Upon clicking the 'Accept' link in the email sent by Acclaim (admin@youracclaim.com), you will be taken to the Acclaim web site. youracclaim.com
There you will create an account in a matter of minutes that will allow you to claim the badge IBM has issued to you. 
Once you have accepted the badge, you will have the ability to easily share with social sites and send emails announcing your achievement.

Click to flip
Make yourself known, by promoting your badge in social media


You can share your IBM digital credential on popular social and professional networking sites, emails, or personal websites. 
The visual representation of your IBM digital credential will also allow your contacts to verify the credentials that you have achieved. 
It is a quick and easy way to share your achievements with your contacts. 
You've worked hard to earn your credentials, so why not share this accomplishment with your colleagues? 
Every credential and profile on Acclaim has a unique URL that can be embedded on a resume or website. Acclaim also offers seamless integration with several popular social and professional networking platforms for the display of credentials as badges.

Sharing your digital credential in LinkedIn


       
How do I share my IBM digital badge on LinkedIn?
Follow Acclaim's step-by-step instructions to add your badge to your LinkedIn profile.
https://support.youracclaim.com/hc/en-us/articles/360021221491-How-do-I-add-my-badge-to-my-LinkedIn-profile-


Add your digital credential in your e-mail signature


Wondering how to add your IBM digital credential to your email signature? 

Watch the video for instructions specific to Gmail and Outlook (02:26) https://www.youtube.com/watch?v=opQhFOsVpVI


Courses we recommend you take next

Once you have completed this Foundational course, you will have two series of courses ahead of you:

1. Explore other Foundation-level courses - You can explore other technology focus areas such as Cloud, Cybersecurity, or AI, by taking other Foundational courses available within this IBM education program.


Click to flip
Learn more at:

https://www.credly.com/org/ibm/badge/getting-started-with-enterprise-grade-ai




Click to flip

Learn more at:

https://www.credly.com/org/ibm/badge/getting-started-with-threat-intelligence-and-hunting





Learn more at:

https://www.credly.com/org/ibm/badge/getting-started-with-cloud-for-the-enterprise




2. Start your track specialization - You can choose Data Science as your technology focus area and start going deeper, acquiring more skills in this domain by taking the Intermediate and Advanced self-paced courses, and/or completing the Practitioner-level course available within our Data Science roadmap.


Click to flip
Learn more at:

https://www.credly.com/org/ibm/badge/enterprise-data-science-in-practice



Click to flip

Learn more at:

https://www.credly.com/org/ibm/badge/machine-learning-for-data-science-projects





Learn more at:

https://www.credly.com/org/ibm/badge/ibm-artificial-intelligence-practitioner-certificate.1



For more information about the IBM Academic Initiative and our IBM Skills Academy program, please visit: research.ibm.com/universit",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/dss7DrO1lK-9Q-JKn-xwjUEQhyUKdiS2,
IBM Security,Getting Started with Threat Intelligence and Huntin,10,Module 2 - Threat Hunting,2,About this Module,text,"
Module 2 

Threat Hunting

Introduction

In this module, you will understand the need for a cyber threat hunting approach by exploring cyber-attack adversary frameworks and their counterpart enterprise threat protection methods. We will deepen our experience by exploring different relevant industry case studies. Later we evaluate the adoption of cyber resilience frameworks and the cyber resilience lifecycle to strengthen the security posture of our organizations.

This page covers the following lecture details:

Duration
Objectives
Instructor
Topics
Skills
Duration

This lecture can be completed in an average of 120 minutes

Objectives

Upon completing this lecture you should be able to achieve the following objectives:

1

1
Understand the need for a cyber threat hunting approach

2

2
Explore cyber-attack adversary frameworks

3

3
Investigate enterprise threat protection methods

4

4
Explore industry case studies

About your Instructors

Meet the IBM subject-matter experts, who will guide you through this module

Numbered divider 1
Ron CraigProgram Manager, Secure Engineering BiographyRon provides technical leadership in the formulation and execution of secure engineering policies, practices, and implementations involving the development and operations of the entire IBM software portfolio. Ron maintains technical skills in Application Security, Enterprise Java, and WebSphere. 


Ron Craig
Program Manager, Secure Engineering 


Biography



Ron provides technical leadership in the formulation and execution of secure engineering policies, practices, and implementations involving the development and operations of the entire IBM software portfolio. Ron maintains technical skills in Application Security, Enterprise Java, and WebSphere. 

Numbered divider 2
Sidney PearlGlobal SIOC Associate PartnerIBM Security ServicesBiographySidney has been working in Information Security for over 25 years and has extensive experience in leading and providing information security consulting services across a diverse range of international clients. Sidney possesses a holistic understanding of cyber risks through the evaluation and exploitation of all-source intelligence information. He has led and managed complex security consulting engagements and Security Operations Center Analysts, engineers, security architects, and engineers responsible for client security services delivery threat identification, monitoring, and mitigation. 



Sidney Pearl

Global SIOC Associate Partner

IBM Security Services





Biography



Sidney has been working in Information Security for over 25 years and has extensive experience in leading and providing information security consulting services across a diverse range of international clients. Sidney possesses a holistic understanding of cyber risks through the evaluation and exploitation of all-source intelligence information. He has led and managed complex security consulting engagements and Security Operations Center Analysts, engineers, security architects, and engineers responsible for client security services delivery threat identification, monitoring, and mitigation. 



Topics


This module is divided into the following sub-topics: 


Topic 1: Cyber Resilience
+

Topic 2: Threat Hunting
+

Topic 3: Threat Hunting Methodology
+

Summary & Resources
+

Quiz
+
Accessing and completing this module

Access the video guides provided by our IBM experts, acquire a visual understanding of the material through the slides, read the notes in detail, and gain further understanding by clicking on the resource material, and completing the quizzes.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/kWyaX9aSkfboPCwt4o2ITYJn9vF6G3YT,
IBM Automation,IBM Robotic Process Automation - Basic I,12,Basic Commands and Variables,11,Input Box and Show Message Box,video,,https://learn.ibm.com/mod/video/view.php?id=201216,
Data Science,Getting Started with Enterprise Data Science,24,Module 3 - Detecting Pattrns of Fraud wit Data Analytics,3,Milestone 2: Data Preparation,text,"Data preparation is a self-service activity that converts disparate, raw, messy data into a clean and consistent view.

Introduction

In this Milestone, you will refine data. This is a vital process for ensuring accurate results.

Topics:

Case Study - Episode 3
Refine Dataset
Topic 1: Case Study - Episode 3

Sally and Maria meet to discuss which data analysis techniques to use on the data. Browse through the slides below to learn how the story develops

Data Representation

An image of Maria, the Data Scientist
Maria

Data Scientist

After data exploration and preparation

Data Scientists typically use descriptive statistics and data visualization techniques to:

Understand the data content.
Assess data quality.
Discover initial insights about the data.
Additional data collection may be necessary to fill gaps. 

Algorithm alignment

Meeting between Maria,Tom and Sally to analyze fraud scenarios
Data formatting

An image of Sally, the Data Analyst
Sally

Data Analyst

Follow Sally now, as she uses the Data Refinery applying mathematics to aggregate the information, facilitating the application of statistical analysis to the sample data. 

Here we cover the following steps:

Apply mathematical operations
Create column SUSPICIOUS_CLAIMS_TIME
Create aggregator SUSPICIOUS_CLAIM_FLAG
SUSPICIOUS_CLAIM_FLAG is of values 0 or 1
Topic 2: Refine Dataset

Perform the steps captured below:


Summary

1

1
In this Milestone, you learned how to refine a data set. This creates a new data set within the project that we will use to visualize the data. 

Next

Continue to the next Milestone.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/KaP6Wti-ugyP6jqouSTej7ftklOzfBsD,
Data Science,Getting Started with Enterprise Data Science,25,Module 3 - Detecting Pattrns of Fraud wit Data Analytics,3,Milestone 3: Finding Fraud Patterns with Data Representation,text,"Data visualization is the process of translating large datasets and metrics into charts, graphs, and other visuals.

Introduction

In this Milestone, you will use Watson Studio Data Representation to find patterns of fraud within the dataset. 

Topics:

Case Study - Episode 4
Data Visualization
Topic 1: Case Study - Episode 4

Browse through the slides below to refresh your understanding of how Data Exploration works and the different Data Visualization tools

Data Representation : Exploratory Visualization

Graphic showing Exploratory Visulization
Summary statistics can be misleading on their own.

Visualizing the data enables you to prepare or revisit the summary statistics analysis contextualizing them as needed.

Image of map showing Complex data visualization
Making complex data accessible.

One form of powerful visualization is data on a map.

 

Image of effective visualization to understand the data
Effective visualization makes complex data more accessible, understandable, and usable.

Image showing Line and bar plots
If you can visualize diverse data through mapping, you can often do your analysis in half the time. 



Line and bar plots can also be useful in visually detecting trends such as positive or negative correlations.

Sally meets with Maria to discuss how they can use visualization to discover patterns within the dataset



Topic 2: Data Visualization

Perform the steps captured below:


Summary

1

1
In this Milestone, you learned how to use Watson Studio to visualize a dataset and perform basic statistical analysis. 

2

2
Using this method you are able to turn thousands of numbers into a visual representation, that is much easier for someone to analyze. While this is not concrete proof and more analysis will be needed; images are an excellent way to quickly identify possible patterns within the data.

Next

Continue to the Summary and reflect on the tasks performed.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/fewQAO2LvSzoPL1UV1eDY6_a5n_MGSrx,
Data Science,Getting Started with Enterprise Data Science,23,Module 3 - Detecting Pattrns of Fraud wit Data Analytics,3,Milestone 1: Data Collection,text,"
In the Data Collection stage, data scientists will gain a good understanding of what techniques, such as descriptive statistics and visualization, can be applied to the dataset.

Introduction

In this Milestone, you will create a Watson Studio Project and then upload a collected dataset.

Topics:

Case Study - Episode 2
Create Project and Upload Dataset
Topic 1: Case Study - Episode 2

Sally and Mike meet with the IT department team to collect the necessary data. Browse through the slides below to learn how the story develops

Data Requirements

An image about team meeting for Data requirements.
Sally, Ron, and Mike meet to discuss data requirements

Data Collection

An image about Sally, Data analyst who got access for sample dataset.
Sally

Data Analyst

After a week going through a back and forth with Ron’s team to confirm the size and scope of the sample dataset, and getting approvals to handle the personal sensitive information contained in the file.

Sally finally got access to the sample dataset named AutoInsClaims.csv

Privacy Considerations

The dataset contains sensitive personal data, subject to data privacy regulations and will need to be removed.

Sally analyzes the information on the data they collected. Browse through the slides below to learn how the story develops

Lets take a peek at the dataset AutoInsClaims.csv

A table showing the list of dataset AutoInsClaims
At a first glance, the Data Analyst notices that some of the columns do not add any value to downstream activities. Sally cleanses the dataset of these columns in order to work with clean data for the next step.

Data Understanding

Sally sifts through the actual data, looking for Data GAPS.

Recap - Now that the data  has been cleaned, we removed columns containing sensitive information, and changed the data types from string to date for further analysis.

Next - Sally goes through the information in the dataset, this time to make sure the data is understandable and ready for analysis.

An image about sally looking for Data gaps
Sally has cleansed the dataset and now needs to ensure the dataset is uploaded to the project, is clear, and understandable

Topic 2: Create Project and Upload Dataset

Perform the steps captured below:


Summary

1

1
In this Milestone, you learned how to navigate the Project page of IBM Watson Studio and also how to upload a new asset to your project.

Next

Continue to the next Milestone.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/WKR9vjqtoqEKrkg-fRLVPbPAJK3e4qjB,
IBM Automation,IBM Robotic Process Automation - Basic I,5,About IBM Robotic Process Automation Studio,4,"Script, Designer and Call Graph",video,,https://learn.ibm.com/mod/video/view.php?id=152630&forceview=1,
IBM Engineering,Quick Start Sessions,16,Quick starts: Publishing for IBM Engineering Requirements Management DOORS Next,16,Publishing for IBM Engineering Requirements Management DOORS Next?,text,"IBM Engineering Lifecycle Optimization – Publishing (PUB), formerly called IBM Rational Publishing Engine (RPE) automates document generation from IBM solutions and select third-party tools. You can use PUB to automate the generation of documents for ad hoc use, formal reviews, contractual obligations or regulatory compliance.

PUB provides:

Documents and reports: Generate high-quality documents with flexible formatting as well as composite reports containing data from multiple sources. Reports may be scheduled or invoked on demand.
Outputs: Support multiple output formats and concurrent document generation to multiple target formats from a single template.
Template editor: A graphical template editing environment for custom report design.
Data sources: Extract data from a single source or combine data from multiple sources for cross-domain reporting
PUB consists of the following applications:

PUB Launcher – a standalone application for generating reports using predefined templates
Engineering Document Generation (EDG) – wizard-based document generation from within the tools themselves (for example from within Rhapsody)
PUB Document Studio – A graphical environment for designing and testing templates
PUB Document Builder – A web-based application that allows dynamic creation of reports by assembling templates, as well as automated, scheduled document generation
In the labs, you get to:

Report on modules in DOORS Next with PUB
Generate and configure a template
Design document templates for DOORS Next with PUB
The estimated time to complete the labs is 2 hours.",https://learn.ibm.com/mod/page/view.php?id=165968,
IBM Security,Getting Started with Threat Intelligence and Huntin,14,Module 2 - Threat Hunting,2,Summary & Resources,text,"
Module 2



Reflecting on what we learned from this lecture and planning the work ahead

Summary

1

1
The key to a good defense is to know your enemy. 

2

2
In the cybersecurity realm defenders must understand how attackers operate to better protect against and counteract their attempts.

3

3
MITRE ATT&CK™ for Enterprise is an adversary model and framework for describing the actions an adversary may take to compromise and operate within an enterprise network.

4

4
Threat hunting can be defined as “the act of aggressively intercepting, tracking and eliminating cyber adversaries as early as possible in the Cyber Kill Chain.”

5

5
Before you start to hunt, you need to understand what to look for i.e. your Prioritized Intelligence Requirements (PIR)

Resources

1. Featured Resources

The following resources were used to develop the material included in this lecture:

IBM Security WhitepaperIBM Security X-Force Cyberattack Preparation and Execution Frameworks
IBM Security Whitepaper



IBM Security X-Force Cyberattack Preparation and Execution Frameworks




X-Force IRIS Attack Framework.pdf
11 MB
2. Additional Resources

The following resources are complementary material to those provided in this lecture.

IMPORTANT: You don't need to access the material included below to complete this course, but just in case you are eager to go beyond the boundaries of the content included in this course, below are some additional resources which you can pursue to learn more.

The URLs included above are pointing to resources hosted outside our domain and beyond our control. Therefore, that material is provided as-is, and no support will be given if the links are removed from public access by the content owners.

List of resources

Intelligence analysis
https://www.ibm.com/security/intelligence-analysis

Cyberattack preparation and execution frameworks
https://securityintelligence.com/media/ibm-x-force-iris-cyberattack-preparation-and-execution-frameworks/

Mitre Attack
https://attack.mitre.org

Art and science of threat hunting
https://securityintelligence.com/maturing-your-security-operations-center-with-the-art-and-science-of-threat-hunting/

Know Your Enemy: The Art and Science of Threat Hunting
https://securityintelligence.com/know-your-enemy-the-art-and-science-of-cyberthreat-hunting/",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/vM_ADtxyrflac-ASD6jpfHaEyJzmbD9i,
Data Science,Getting Started with Enterprise Data Science,15,Module 2 - Data Science on the Cloud,2,Topic 4: The role of the data analyst in an integrated Cloud environment,text,"The role of the data analyst in the data science lifecycle 

Introduction

In this topic, we'll explore the role of the data analyst within the data science lifecycle. 

Throughout this topic, we will attempt to answer the following questions:

What role does the data analyst play in the data exploration phase? 
1. What role does the data analyst play in the data exploration phase? 

In practice, several people work on a team to build data products. 

Your analyses will only be as good as the team that is responsible for collecting, building, and analyzing the underlying data. 

Data Analyst role in the integrated environment

A woman's profile named Sally along with her job and what she does.
Sally
Data Analyst

Her Job:
Captures domain knowledge for successful business alignment.

What she does:

She works with the domain experts to understand the business problems.
Provides domain knowledge that Maria and Tom use to develop custom data models.
Sally uses data refinery tools to cleanse and curate the raw data.
Makes data visualizations to depict insights to sponsoring users.
Because data scientists are involved in each step of the journey in building data products, they tend to bring a holistic view to solving problems with data. However, they can’t be experts in everything—this is where their team can help. Skilled data analysts, scientists, data engineers, developers, and business analysts are transformative figures in modern business. They are the beating heart of the big data economy. It’s not just that they are designing new systems; they are going to bat for new sources of data and new ways to use that data. Of course, IT still must build the system, but the data science professionals are the ones who help departments collaborate to solve problems and speed innovation. 

Predicting fraud with Data

Sally
Data Analyst

Problem
Organizations are trying to predict fraud or suspicious activity and their patterns to help drastically reduce losses.

Challenge

How to make sense of the structured data?
Where is the signal and where is the noise?
Solution 
Data Refinery tools !





Woman (Sally) interacting with a man in a business setting.
The Data Analyst: Consider the following use case. 

During its fifty-year history, an insurance company has been struggling to detect potentially fraudulent activity and has turned to data science to predict fraud within insurance claims before the claim is settled.

Before long, a data analyst is given a giant spreadsheet of a thousand rows and hundreds of columns. The Analyst takes the spreadsheet and begins the task of data wrangling, cleansing, and curating the records within. Are there any missing values? Should they be removed, or the missing value must be provided? Are any of the columns redundant can they be combined? Or removed altogether?











The Profile tab of the Data Refinery tool can help in highlighting those redundant columns, or noise if you will, from the data set. These characteristics of the data set are best revealed with summary statistics such as min, max, median and standard deviation. Summary statistics make the job of the data analyst much easier and accurate by visually revealing the noise from the signal. 

Statistical Analysis

An image showing statistical analysis
Explanation: Sally first compared the amount of days since the loss happened (traffic accident) with the amount of days left for policy or license expiration, then the column SUSPICIOUS_CLAIMS_FLAG was built to support analysis of Fraudulent Claims using logistic regression as a binary (two-class) classification problem.

An image showing statistical analysis
Analysis outcome: Used for future prediction

Sally through graphs, and statistics, found the following patterns:

Fraud is more likely to occur with folks that submit multiple claims in a year.
The greater the claim amount, the more likely of fraudulent behavior.
Dynamic Dashboards – making insights available to all

An image showing dynamic dashbords.
An image showing dynamic dashbords.
A dashboard showing graphs.
A dashboard showing graphs.
Our brains are naturally more adept at processing visual data as opposed to strings of numbers. Therefore, data scientists commonly use data visualizations (graphs, charts, etc.) to quickly view relevant features of their datasets and identify variables that are likely to result in interesting observations. 

By displaying data graphically-for example, through scatter plots or bar charts-users can see if two or more variables correlate and determine if they are good candidates for more in-depth analysis. 

There are four primary types of Exploratory Data Analysis:


Univariate non-graphical
+

Univariate graphical
+

Multivariate nongraphical
+

Multivariate graphical
+
Summary

1

1
The data analyst brings the claims data into Watson Studio where she begins work on analyzing, cleaning, and wrangling the data into something that can be understood by a machine learning model. 

Next 

The role of the data scientist in an integrated Cloud environment 

Sources:
[1] IBM SmartPaper. The Data Science Lifecycle: From experimentation to production-level data science.
[2] IBM.com. IBM Watson Studio",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/XzQyDjAwGQw0scn1Y5yrXCztN-0NJywO,
IBM Automation,IBM Robotic Process Automation - Basic I,21,Advanced Commands,20,Foreach,video,,https://learn.ibm.com/mod/video/view.php?id=152663,
IBM Security,Getting Started with Threat Intelligence and Huntin,9,Module 1 - Threat Intelligence,1,Quiz,text,,https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/tDULJwjqIXUVUDFGvngBcfoX5ADCevq2,
Data Science,Getting Started with Enterprise Data Science,22,Module 3 - Detecting Pattrns of Fraud wit Data Analytics,3,Case Study - Episode 1,text,"Let's deep dive into the insurance industry and explore how teams can use data science to solve real problems.

Introduction

In this topic, we will be introduced to a case study focused on an Auto Insurance company, we will explore the business profile, the challenges related to existing business problems within the claims fraud department, and the involvement of the data science team to attempt to find a solution.

Auto Insurance Company

5,000 employees, 650 million in operating revenue last year.

Providing insurance policies to individual car owners across the continental United States.

Drivers usually sign a six-month policy with an auto insurance policy. Each month, or all at once, the driver pays a fee, or premium, to the company.

Policy Cost Factors

Type of car insured - particularly its safety record and how expensive it is to repair. 

Driver's record - average speeding tickets and age (teenagers cost more to insure because they're less experienced drivers, and therefore a bigger risk.) 

Lower-cost premiums are enjoyed by drivers with fewer accidents and tickets on their records, part-time drivers, people who take driver education courses, and families with multiple cars.

Project Objective

An insurance company has been struggling to detect potentially fraudulent activity and has turned to IBM for their data science and AI offerings to predict fraud with insurance claims before the claim is settled.  

So what are some of the hypotheses that may flag the administrator that a fraud may be underway?

Such hypotheses or assumptions come about after extensive conversations with subject matter experts in the field of auto insurance. In this example, the following hypotheses are what the data scientist will attempt to prove or refute from this data set:

Identify auto insurance claims filed after the expiration of the insurance policy
Claims filed after the license expiration date
Excessive claim amount, which is over $10,000 in value
An image about Bob, Fraud claims Department manager's business goals.
Bob lays out clear objectives and business goals. The following Milestones will focus on the third goal - Excessive claim amount

The Business Sponsor

Bob is our Business Sponsor - he will be in direct contact with the client, to ensure their needs are met by the Data Science Team.

An image about Bob, Fraud claims Department manager's background details
Bob

Fraud Claims Dept. Manager

Background

36 years old
8 years of experience
Bob oversees the Fraud Claims Department
Situational

After meeting with the VP of the Auto Claims Division
He has 30 days to provide analysis and a remediation plan to reduce insurance claim fraud
Meeting with the Data Science Team next week
Bob connects with the Data Science team

Business Sponsor - Plan, define KPIs, and provide Feedback
Developer - Development, Deployment & Monitoring
Data Engineer - Integration & Refinement
Data Scientist - Analysis & Modeling
Data Analyst - Analysis & Visualization
Group of Data science team connected for a meeting.
The Business Sponsor, Bob, will host routine meetings to ensure the Data Science Team is on track with the client's needs
",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/VH6hxv-ATqKUWzioPDAjQB5HFvUkBopE,
IBM Engineering,Quick Start Sessions,17,Quick Starts: Getting Started with Product Line Engineering in ELM Using pure::variants,17,What is Product Line Engineering in ELM Using pure::variants?,text,"The International Council on Systems Engineering (INCOSE) defines Product Line Engineering as an approach for engineering a portfolio of related products in an efficient manner, taking advantage of products' similarities while managing their differences.

The IBM Engineering Lifecycle Management platform provides powerful capabilities for componentizing engineering artifacts across the lifecycle and facilitating change and configuration management of those artifacts. It also facilitates efficient re-use of those artifacts both at the local level but also ‘globally’. These capabilities are described in the Getting Started with Global Configuration Management Quick Start. If you have not been through that lab then it is highly recommended you do so before completing this one.

pure::variants by pure-systems provides a powerful way to model the features and variants of a product line. It also integrates with the ELM platform allowing requirements, models and tests to be mapped to those features and variant models. Powerful automation then allows for new variants to be easily and quickly generated, either for a single domain (for example requirements) or for an entire product.

In this Quick Start you will see how product lines may be quickly and effectively generated from a ‘150%’ model of the product, using IBM Global Configuration Management to capture and organize the requirements, designs and tests and pure::variants to create the feature model and generate the variants.
",https://learn.ibm.com/mod/page/view.php?id=177093,
Artificial Intelligence,Create a Node-RED starter application,1,Implementing ETL flows with Node-RED,1,Implementing ETL flows with Node-RED,text,"Implementing ETL flows with Node-RED
Extract data from any source, transform the data, and load it to any destination. Are you looking for an easy Extract Transform Load (ETL) tool? Consider Node-RED. In an ever-changing world of data integrations, we found Node-RED to be a low-cost, reliable, and secure platform that is capable of moving millions of records.

Node-RED is a low-code programming platform for event-driven applications (particularly for IoT apps). In essence, it’s a Node.js-based application with a browser-based editor that makes it easy to wire together flows and develop the top-level application logic using a graphical user interface.

You won't typically find Node-RED in the lists of ETL tools. Yet, we have used it as our primary ETL tool since about two years ago, and I feel it deserves the consideration.

For me, Node-RED is a low-barrier flow-based development platform because:

It uses fewer resources than other ""low-code"" ETL systems (such as IBM DataStage or Hitachi Vantara Pentaho).
It is easier to find JavaScript developers to do the work.
I would even go further and call it a platform for ""pretty NodeJS applications."" Don't underestimate ""pretty"" in software development! It directly translates to lower costs of the development lifecycle. Of course, the opposite is true as well: You can still end up with spaghetti code in Node-RED.

As in any software house, developer skills come first, technology second. The GUI does help new developers to get familiar with the code quicker and speeds up maintenance, but only if the flow was written well in the first place.

Moving to Node-RED as an ETL system
We moved to Node-RED from Pentaho Data Integration. Our typical small-integration Pentaho jobs used to take at least 5 minutes to complete and had to run on a permanent 32GB RAM in a Kubernetes pod instance. The first success we experienced was when the same ETL jobs implemented in Node-RED ran in seconds and on a couple hundred megabytes of RAM.

The key to our success was to move away from a monolith ETL architecture to the use of micro-pipelines, running the jobs as temporary container instances. Dockerized jobs immediately free up all resources once finished, provide full isolation, and are measurable (assessing the resource usage of a Pentaho job is quite challenging).

The following figure shows an example process section in Pentaho.

Screenshot shows example process section in Pentaho

The following figure shows an example of a full process in Node-RED.

Screenshot shows example of full process in Node-RED

With Node-RED, developer productivity skyrocketed, version control is fully transparent, handovers are quicker, and troubleshooting has massively improved thanks to clear logging.

How to use Node-RED as an ETL tool
Node-RED only provides the data pipeline part of an ETL system. To successfully manage thousands of integrations, you will need to add a scheduler, a source of configurations, and a management user interface.

In our case, all these components were already available in our SaaS platform (called Your Learning). Our job scheduler runs as a service and starts individual jobs as Kubernetes deployments. It provides APIs for managing the jobs (primarily through the administrator user interface), plus logging and environment integration. To make the solution more robust, we added an automatic retry framework based on the job exit status.

Each data integration, or ""connector"" as we call it, has its own repository and produces its own Docker image that is registered in the job scheduler. A versioning policy ensures straightforward upgrades. Connector descriptors provide configuration templates of all inputs that are expected from the configuration service. As such, any connector version can be reused among various tenants of our SaaS product.

The job management user interface then pulls all of it together and provides easy configuration, management, and monitoring.

The following figure shows an example of our job management interface.

Screenshot shows example of job management interface

Why is Node-RED so attractive for us?
I could say that we are able to create a production-level integration in hours. I could argue that our developers prefer Node-RED against Pentaho. I could say that the low footprint and large libraries of Node.js trump Pentaho any time. But the real hidden Node-RED gem is the ability to add custom nodes as reuseable modules.

An individual skilled developer can easily make any data pipeline work in their own tool of choice. But working as a team, it's immensely valuable to be able to drag and drop a ready-made generic node. Our nodes have not just been battle-tested but also provide an easy graphical configuration of key functionalities that are important for our business. And because we own them all, we can constantly improve our portfolio and adapt to changes easily. If there's no node for a given purpose, a function node is used (with full flexibility and libraries available in Node.js). The second time we need the same code, we create a new node or extend an existing one.

These are some examples of nodes we have collected over the past two years:

Image shows nodes collected over past two years

File management nodes -- Read, convert, stream, merge, create, decrypt, decompress files (JSON, CSV, TSV, GZ, ZIP)
Utility nodes -- Reliable integrations using re-tryable API calls
Storage nodes -- Work with the IBM Db2 pool of connections
Configuration nodes -- Inject configurations into a flow and exit data integration with formatted output and error handling
Services nodes -- Work with our product-specific APIs, calculate deltas, bulk uploads
It's the best example of code reuse in its “prettiest” form.

The following figure shows an example of our custom node configuration.

Image shows example of our custom node configuration

It is best to document the nodes well, set sensible defaults, and make iterative changes rather than shoot for perfection at the start. We took extra care to make the logging minimal but clear and consistent so that the job output log reads well.

Key implementation considerations
From our experience, these are key implementation considerations:

Set up back pressure
Implement unit testing
Develop locally using Docker
Set up back pressure to fight out-of-memory errors
It's far too easy to get out of memory when working with JSON arrays in Node.js. Creating back pressure (controlling the flow rate such as by streaming data) is critical to processing larger data sets. Node-RED offers some limited capability out of the box, such as the rate-limit node or linked loops. Embedding RED.events listeners (an internal Node-RED functionality) into our custom nodes proved to be the cleanest option for us for creating back pressure. And, of course, you can also use any publish-subscribe library of your choice (such as pubsub-js).

Implement unit testing
The easiest way to test your flows is to branch the flows where data is retrieved and exported, then provide static checking of the transformations that happen in between. A more advanced way would be to provide mock configurations and inputs directly in your custom nodes so flows don't have to split unnecessarily. Either way, decide on your test mode trigger convention early in the process, ideally as an attribute in the global space.

Develop locally using Docker
Working with a Docker container enables us to be sure that our locally developed code behaves exactly the same in the test or production Kubernetes cluster. Developers start the Node-RED instance in a Docker container with source directories mapped to the local file system.

Limitations of Node-RED as an ETL tool
The key limitation of Node-RED is that code reviews are difficult. Pull requests are hard to verify because all application code is embedded in a one-line JSON file. Initially, we have partially addressed this issue by using an extension that keeps the file format as YAML. This year I wrote an extension that splits the JSON file into individual JavaScript files. More complex changes still require a reviewer to run the application but static code analysis tools, like SonarQube, can now be used to check your source code.

I should add that before V3 of Node-RED, there was no code linting provided by the UI. Make sure you start with the latest version where you can use the built-in formatting function when you write code in your nodes. Linted code is an important practice to produce more maintainable code.

What about “xyz” platform?
Data pipelines are a massive industry, and there were a lot of alternatives from big data processors to smaller systems that we evaluated before we made our decision to use Node-RED as our ETL tool. These ETL systems are what we came up with in our initial research:

ETL as a service:

AWS Glue, Amazon Kinesis Data Streams, IBM Cloud Code Engine
Java-based ETL systems:

Logstash, Apache NiFi, Pentaho, Talend, Oracle Data Integrator
Alooma, Blendo, Heva, Matillion, Xplenty, Microsoft SQL Server Integration Services, DataFlux
Apache Spark (Scala JVM), Apache Flume, Apache Chukwa, Apache Flink, Splunk
Apache Hive, Apache Pig, Cascading, Embulk
Python-based ETL systems:

Apache Airflow, Singer, GitLab Meltano (Singer EL + DBT), Spotify Luigi, Dagster, Digdag, Bonobo, Mara
GO-based ETL systems:

Reflow, SciPipe, Telegraf, Cadence (used by Uber), Flogo, Mozilla Heka (deprecated)
Other notable systems:

C: Mozilla Hindsight, Rust: Vector
Node.js: NoFlo
Multi-language: Apache Beam (Java, Python, Go), Pachyderm (K8S cluster – pipelines in Python, GO, Scala)
Other approaches to ETL:

ELT (Extract, Load, and Transform - makes use of SQL for transformations, for instance in the dbt framework)
Although this is not a comprehensive list, I hope it can be useful to any readers standing at a similar crossroads. If Python or GO skills are readily available in your group, I'd recommend a deeper look at Singer and Flogo, respectively.

You already know the result of our quest to find a lightweight, flexible, low cost-of-ownership ETL tool that fits our organization’s skills. Now after two years of production use of Node-RED as our ETL tool, we still think we made the right choice. This does not mean we won't change in the future. The most likely next shift will lean toward real-time processing, so fewer batch jobs. Whether it will influence our ETL choices is a question for another article and another time.

Summary
Node-RED deserves a spot in the lists of ETL tools. It can be used to extract data from any source, transform the data, and load it to any destination. And it does it well.

In this article, I tried to cover all key aspects of building Node-RED ETL pipelines and hopefully inspired some of you to give Node-RED a try. If you have any questions or would like to provide feedback, feel free to contact me at ondrej.lehota@uk.ibm.com.

Legend

Categories
Data management
Node-RED

Trials

Table of Contents",https://developer.ibm.com/articles/implementing-etl-flows-with-node-red,
IBM Automation,IBM Robotic Process Automation - Basic II,5,PDF,5,5 - PDF - use case II,video,,https://learn.ibm.com/mod/video/view.php?id=152674,
IBM Security,Getting Started with Threat Intelligence and Huntin,17,Module 3 - Threat Map Worldwide View with IBM X-force Exchange ,3,Milestone 2: Visualizing the Cyber Threat Activity Map,text,"Locate and follow threat attacks
in real-time 

Introduction

In this Milestone, we will navigate through the threat activity map. This map provides a world-view, in real-time, showing countries that are under attack.

Topics:

Gain an understanding of the topics covered in this Module by going through the simulated lab and reading material included in the section below: 

         Exploring Malicious Activity Map

Exploring Malicious Activity Map 

Perform the steps captured below:


Summary

1

1
The threat activity map is an important part of cybersecurity. gives a visual of where threats are in different parts of the world. his way we can monitor these attacks in real-time.

2

2
X-Force Exchange is useful in finding IP addresses that are connected to spam attacks. It also gives the ability to follow these attacks.

Next Steps

You've completed this module! We are now ready to move to the next steps. Click the Continue button to reach the next page.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/KZ_NLBoQB0Bx0WhLEy8S2ZW0_5HsCu9Y,
IBM Security,Overview,0,IBM Security,0,What is IBM Security?,text,"What is IBM Security?
IBM Security develops intelligent enterprise security solutions and services that allows businesses to put security everywhere to thrive in the face of uncertainty. IBM’s four-pronged approach to security allows enterprises to align, protect, manage, and modernize cyberthreat protection in an increasingly hybrid, multi-cloud world.

Why is cybersecurity important?
Cybersecurity is everyone’s business. Cybersecurity skills are required across every industry segment, government organization, and other institutions.  IBM Security offers hands-on access to leading enterprise security solutions for the purpose of learning and developing highly sought-after cybersecurity skills.

Courseware
Software
Resources
IBM Security Learning Academy
Get free technical training for IBM Security products. You can explore the course catalog and build your own curriculum by enrolling in courses.

Getting Started with Threat Intelligence and Hunting
This is a foundational course, exposing the learner to threat intelligence concepts including: Attack trends by geography, threat intelligence tools and real-world use cases.

IBM QRadar SIEM Foundations
IBM QRadar SIEM provides deep visibility into network, user, and application activity. It provides collection, normalization, correlation, and secure storage of events, flows, asset profiles, and vulnerabilities.

IBM QRadar SIEM Advanced Topics
Using the skills taught in this course, you will be able to configure processing of uncommon events, work with reference data, and develop custom rules, custom actions, and rules.

Developing Secure Software
Learn the security basics to develop software that is hardened against attacks, and understand how you can reduce the damage and speed the response when a vulnerability is exploited. [...]

Important skills in cybersecurity
Curiosity, collaboration and communication are critical keys to success in Security.  Learn from industry experts, explore IBM Security products and use our learning materials to understand the importance of these 3 C's.",https://www.ibm.com/academic/topic/security,
IBM Cloud,Journey to Cloud: Envisioning your Solution,16,Module 2 - Cloud Adoption Journey : Ideation Practices,2,Quiz,text,,https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/uJFfAOotUuHnzKoKtncGpmiWa_xv9e6S,
IBM Automation,IBM Robotic Process Automation - Basic I,3,About IBM Robotic Process Automation Studio,2,Studio Organization,text,"IBM Robotic Process Automation Studio is the integrated development environment (IDE) of IBM RPA. To learn how to use it, see IBM Robotic Process Automation Studio.",https://learn.ibm.com/mod/page/view.php?id=200087,
IBM Automation,IBM Robotic Process Automation - Basic II,1,Assets,1,1 - Assets,video,,https://learn.ibm.com/mod/video/view.php?id=152668,
IBM Security,Getting Started with Threat Intelligence and Huntin,3,Module 1 - Threat Intelligence,1,About this Module,text,"Module 1 

Threat Intelligence

Introduction

Through this module, you will gain familiarity with the taxonomy of cyberattacks, understand the current impact of cybersecurity threats across different geographies and industries, exploring enterprise security domains.  

This page covers the following module details: 

Duration
Objectives
Instructor
Topics
Approach
Duration

This module can be completed in an average of 110 minutes

Objectives

In this module you will learn the following concepts:

1

1
Understand the effect of today’s public opinion about AI in its mainstream adoption

2

2
Research global cybersecurity trends in different geographies

3

3
Familiarize with the taxonomy of cyber attacks

4

4
Explore the enterprise cybersecurity domains

Skills 

Upon completing this module you will have acquired the following skills:

Cyber Attack Types
Malware Families
Malware Lifecycle
Cyber Attack Trends
Enterprise Security Domains
About the Instructor 

Meet the IBM subject-matter expert, who will guide you through the topics on this module.   

A man in suit smiling.
Jeff Crume 

Chief Technical Officer

Distinguished Engineer

IBM Security Americas





Biography



Jeff Crume is a Distinguished Engineer and CTO for IBM Security Americas as well as an Assistant Professor at NC State University. 



He is an IBM Master Inventor with a PhD in Cybersecurity and is the author of a book entitled “Inside Internet Security: What Hackers Don’t Want You To Know” as well as a contributing author to the “Information Security Management Handbook.” 



Jeff is a member of the inaugural class of the NC State University Computer Science Alumni Hall of Fame and has served on the school’s Strategic Advisory Board. 



He currently serves on the editorial board for the “Information and Computer Security” research journal and is a member of the IBM Academy of Technology. 

He holds CISSP and ISSAP professional IT Security certifications as well as Distinguished Chief IT Architect credentials from The Open Group.



Jeff lived in Beijing on assignment in 2006 and has worked with clients in more than 40 countries.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/pGEBXms9Z4gKAm-4JwXlaIFTH_4vL3RW,
IBM Automation,IBM Robotic Process Automation - Basic I,7,Basic Commands and Variables,6,Variables,video,,https://learn.ibm.com/mod/video/view.php?id=152635,
IBM Automation,IBM Robotic Process Automation - Basic II,8,Database,8,8 - Database - bulk manipulation,video,,https://learn.ibm.com/mod/video/view.php?id=152678,
IBM Engineering,Quick Start Sessions,11,Quick Starts: IBM Engineering Workflow Management for MBSE,11,What is IBM Engineering Workflow Management for MBSE?,text,"IBM Engineering Workflow Management (EWM), formerly called IBM Rational Team Concert, is a team collaboration tool that integrates development tasks, including:

Planning and task management
Change management
Defect tracking
Source code control
Build automation
Model management (Rhapsody Model Manager is an extension to EWM)
Reporting
EWM also integrates with many commonly used tools such as Git and Jenkins. EWM is available ‘on prem’ but also as a SaaS service called Track and Plan on Cloud.

This lab introduces the first of the capabilities mentioned above: planning and task management. EWM supports many different practices such as Agile, Scrum, Kanban, SAFe, Waterfall etc. and is used by many different industries and domains. This lab focuses on using EWM for a Model Based Systems Engineering project using a predefined template, although it is worth noting that agile practices are equally applicable to such projects and the Aviary demonstrator illustrates the use of SAFe alongside MBSE.

In the labs, you get to:

Create a project
Create work items
Create plans
The estimated time to complete the lab is 2 hours.",https://learn.ibm.com/mod/page/view.php?id=165956,
Artificial Intelligence,Create a Node-RED starter application,0,Introducing Node-RED 1.0,1,Introducing Node-RED 1.0,text,"6 years after it was originally open sourced by IBM, we're excited to see Node-RED reach the major milestone of its 1.0 release. This release reflects the maturity of the Node-RED project whose community has continued to grow from strength to strength with over 2 million downloads, 2200 third-party add-on nodes available, and more and more companies adopting it as part of their own products and services.

What is Node-RED?
Node-RED is a low-code programming environment for event-driven applications. It uses flow-based programming to let you draw a visual representation of how messages should flow through the application.

It's ideally suited to run on devices such as the Raspberry Pi for creating IoT solutions, as well as in the cloud for any event-driven type workload, such as providing REST APIs and integrations between systems.

Node-RED embodies a “low code” style of application development, where developers can quickly create meaningful applications without having to write reams of code. The term low code was coined by the Forrester Research company in a report published in 2014, but it clearly embodies a style of development that goes back further than that.

Three key benefits of low-code application development, all of which are seen first-hand with Node-RED, are:

It reduces the time taken to create a working application.

It is accessible to a wide range of developers and non-developers alike.

The visual nature helps users to see their application.

You can find out some more about the background and philosophy of Node-RED's low-code approach to application development in this previous blog post.

What does 1.0 bring?
This release brings a number of useful feature enhancements that you can read about on the nodered.org blog. In this blog, I wanted to highlighting some of the bigger changes.

While the emphasis is on stability, the Node-RED project has taken the opportunity of a major version change to make some updates that weren't suitable for smaller maintenance releases.

Asynchronous by default
For end users, the main change is that flows are now fully asynchronous, which allows for fairer handling of messages across multiple flows. It also unlocks a number of exciting features that are further on in the roadmap, including the ability to pause and debug flows as one would with a traditional code debugger.

It is possible that some existing flows have been written to take advantage of the sometimes-synchronous, sometimes-asynchronous nature of the current runtime. So this change does have the potential to affect existing flow behavior.

The Node-RED project has done a lot of work to minimize any potential impact and have written a number of blog posts to help users understand the changes: Making flows asynchronous Cloning messages in a flow

Overhauled CSS
The current Node-RED editor had CSS classnames dating back to the very first day of its development 6 years ago. It has evolved over time without a lot of consistency. This made it hard to produce custom themes or to embed the editor into another page without a lot of tedious work.

With this release, the editor's entire CSS has been completely overhauled to ensure consistency and ease of use. The Node-RED project has also provided tooling to help produce custom themes and there's already a ubiquitous dark theme available from the community.

Docker images
The Node-RED Docker images are a popular way of using the Node-RED project. However they were built on base images that are no longer maintained. This has meant, among other issues, that we've not had an image suitable for the Raspberry Pi with the current 10.x version of the Node.js runtime for a while now.

Thanks to the community, the Docker images have been completely redesigned, with proper multi-architecture images now available.

New Look for the Node-RED Flow Library
The Node-RED Flow Library is a place where all third-party contributed nodes are listed. It's also a place where users can share useful flows that they have created. With over 2200 contributed nodes and 1000+ flows, there's a lot of great stuff in the library. The challenge is often finding what you're looking for.

To coincide with the 1.0 release, the flow library has had a make over and a new feature added: the ability for users to create and share collections of things. This is a way to help bring some order and curation to the flow library. For example, there is a collection of extra nodes for the Node-RED Dashboard project.

Getting started with Node-RED
If this 1.0 release of Node-RED has caught your interested to find out more, you have a number of choices. You can follow the Node-RED project documentation for installing it on your local machine or a device like a Raspberry Pi.

You can also find many more articles, tutorials, and code patterns featuring Node-RED on IBM Developer.
",https://developer.ibm.com/blogs/introducing-node-red-version-1-0/,
IBM Cloud,Journey to Cloud: Envisioning your Solution,23,Final Remarks,3,Final Remarks,text,"Key Takeaways

If you went through all the activities included in this course, you have now started your journey towards better understanding the impact of Cloud Adoption in the Enterprise.  

With your newly acquired Cloud Adoption foundational knowledge, you can evaluate what areas within this vast domain you would like to investigate further, and eventually take a deep dive to master Cloud Computing -relevant skills that could help you in your current or future job. 

Below are some relevant takeaways, we believe summarize what you have been exposed to during this course experience. 

1

1
Cloud is one of the leading drivers of digital transformation but cloud adoption is more than simply purchasing software from a cloud service provider. The cloud migration journey involves re-examining the way a business operates in order to continuously deliver innovative solutions and stay ahead of the competition. 

2

2
Successful cloud adoption approaches put the end user at the forefront of design. Truly transformative organizations understand what their customers want and use the cloud to deliver.

3

3
There is no single path to cloud adoption. Organizations must carefully assess their existing capabilities and limitations in order to choose the cloud solution that offers the greatest long-term benefits.

4

4
The job market for Cloud Computing is rapidly changing and many are now studying and learning Cloud, acquiring practices and technical know-how. So it is up to you to stay ahead by leveraging a future insight on what's to come and having practical knowledge on the adoption of these revolutionary technologies.

5

5
Constantly stay in the know by exploring research papers on this field. There are countless scientific publications from our research teams around the globe that you can access in our IBM Research Publications portal.

6

6
Keeping a real feel of the technology while gaining unique and marketable skills by accessing Cloud software tools and technologies; work through our open-source IBM Research tools and tutorials and the IBM premium enterprise software tools on the Cloud showcased throughout this course.

7

7
And more importantly gain real fieldwork, adopting these practices in your line of work or applying for new positions in new areas of interest.

Remember to promote achievement in social media

Once you complete this course you will be eligible to receive an IBM official digital badge - recognizing the level of knowledge you have achieved.

Follow the instructions below to make the best of your digital badge - IBM Badges FAQ - https://www.ibm.com/training/P430723G05904L37

Courses we recommend you take next

Once you have completed this Foundational course, you will have two series of courses ahead of you:

1. Explore other Foundation-level courses - You can explore other technology focus areas such as Cloud, Cybersecurity, or Data Science, by taking other Foundational courses available within this IBM education program. 


Click to flip
Learn more at:

https://www.credly.com/org/ibm/badge/getting-started-with-enterprise-data-science



Click to flip

Learn more at:

https://www.credly.com/org/ibm/badge/getting-started-with-enterprise-grade-ai.1





Learn more at:

https://www.credly.com/org/ibm/badge/getting-started-with-threat-intelligence-and-hunting




2. Start your track specialization - You can choose AI as your technology focus area and start going deeper, acquiring more skills in this domain by taking the Intermediate and Advanced self-paced courses, and/or completing the Practitioner-level course available within our Artificial Intelligence roadmap.


Click to flip
Learn more at:

http://www.youracclaim.com/org/ibm/badge/building-cloud-based-mobile-solutions-for-the-enterprise


Click to flip

Learn more at:

http://www.youracclaim.com/org/ibm/badge/devops-for-enterprise-business-agility




Learn more at:

http://www.youracclaim.com/org/ibm/badge/ibm-cloud-computing-practitioner-instructor-certificate.1



For more information about the IBM Academic Initiative and our IBM Skills Academy program, please visit: research.ibm.com/university",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/hcZ2nlcGdB8fu83pVeHosqnn2N7nO8Ia,
IBM Engineering,Quick Start Sessions,2,Quick Starts: IBM Engineering Lifecycle Management Quick Tour,2,What is IBM Engineering Lifecycle Management?,text,"The IBM Engineering Lifecycle Management (ELM) solution is an open architecture, open standards-based offering that helps to manage and speed delivery of complex products, especially those in regulated environments such as Aerospace and Defense, Automotive and Medical Devices. The platform integrates engineering information such as requirements, system and software models, and test plans and test results throughout the product lifecycle. This provides transparency and traceability, providing value in two main areas. First, this helps to understand the impact of proposed changes, as well as improving management of changes across the development team.  Second, this traceability makes it much easier to meet regulatory requirements - compliance becomes a by-product of your development process.

In this Quick Start lab you will take a brief tour of some of the major capabilities of the ELM solution:

Planning and Task Management with IBM Engineering Workflow Management (EWM)
Requirements Management with IBM Engineering Requirements Management – DOORS Next
Architecture Management with IBM Engineering Systems Design – Rhapsody Model Manager (RMM)
Test Management with IBM Engineering Test Management (ETM)
Reporting with Jazz Reporting Services Report Builder (JRS)
Analysis with IBM Engineering Lifecycle Optimization - Engineering Insights (ENI)",https://learn.ibm.com/mod/page/view.php?id=177053,
Data Science,Getting Started with Enterprise Data Science,5,Module 1 - Data Science Landscape,1,Topic 2: What is Data Science?,text,"Defining data science and its evolution as an independent field of study. 

Introduction

In this topic, we'll define data science and discuss its emergence as a field of study. 

Throughout this topic, we will attempt to answer the following questions:

What is data science? 
What factors led to the rise of data science as an independent field? 
1. What is data science? 

Data science combines the scientific method, math and statistics, specialized programming, advanced analytics, AI, and even storytelling to uncover and explain the business insights buried in data.

What is data science?

Data science is a multidisciplinary approach to extracting actionable insights from the large and ever-increasing volumes of data collected and created by today’s organizations. Data science encompasses preparing data for analysis and processing, performing advanced data analysis, and presenting the results to reveal patterns and enable stakeholders to draw informed conclusions.

Data preparation can involve cleansing, aggregating, and manipulating it to be ready for specific types of processing. Analysis requires the development and use of algorithms, analytics and AI models. It’s driven by software that combs through data to find patterns within to transform these patterns into predictions that support business decision-making. The accuracy of these predictions must be validated through scientifically designed tests and experiments. And the results should be shared through the skillful use of data visualization tools that make it possible for anyone to see the patterns and understand trends.As a result, data scientists (the umbrella term for any data science practitioner) require computer science and pure science skills beyond those of a typical data analyst. 

A data scientist must be able to do the following:

Apply mathematics, statistics, and the scientific method
Use a wide range of tools and techniques for evaluating and preparing data—everything from SQL to data mining to data integration methods
Extract insights from data using predictive analytics and artificial intelligence (AI), including machine learning and deep learning models 
Write applications that automate data processing and calculations
Tell—and illustrate—stories that clearly convey the meaning of results to decision-makers and stakeholders at every level of technical knowledge and understanding 
Explain how these results can be used to solve business problems
This combination of skills is rare, and it’s no surprise that data scientists are currently in high demand. 

2. What factors led to the rise of data science as an independent field? 

Evolution of data science

Bringing it all together

In the video below your instructor will guide you through these concepts:


Summary

1

1
Data science encompasses preparing data for analysis and processing, performing advanced data analysis, and presenting the results to reveal patterns and enable stakeholders to draw informed conclusions. 

2

2
A data scientist must be able to do the following:

Apply mathematics, statistics, and the scientific method
Use a wide range of tools and techniques for evaluating and preparing data—everything from SQL to data mining to data integration methods
Extract insights from data using predictive analytics and artificial intelligence (AI), including machine learning and deep learning models
Write applications that automate data processing and calculations
Tell—and illustrate—stories that clearly convey the meaning of results to decision-makers and stakeholders at every level of technical knowledge and understanding
Explain how these results can be used to solve business problems.
3

3
Data science arose in the late 19th century as a means of doing statistical analysis. From there, it evolved with the introduction of computers and the capacity to collect and process ever-growing amounts of data. 

Next 

The 'Data' in Data Science

Sources:
[1] Cleveland, William. Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics. 2001 
[2] Computer History Museum. Timeline of Computer History. 
[3] Greenberger, Martin. The Computers of Tomorrow. May 1964. 
[4] Hinton, Geoffery, et al. A Fast Learning Algorithm for Deep Belief Nets. November 8, 2005. 
[5] Regalado, Anthony. Who Coined 'Cloud Computing’? MIT Technology Review. October 31, 2011. 
[6] Tukey, John. The Future of Data Analysis
[7] Turing, Alan. On Computable Numbers. May 28, 1936
[8] Washington Post. Timeline: 50 Years of Hard Drives. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/ZDcrIV98iNp6SWptuBY6SR65YchB2CWO,
IBM Automation,IBM Robotic Process Automation - Basic I,11,Basic Commands and Variables,10,Log message,video,,https://learn.ibm.com/mod/video/view.php?id=152644,
IBM Cloud,Journey to Cloud: Envisioning your Solution,1,course_overview,0,About this course,text,"Getting Started with Enterprise Digital Modernization through Cloud Adoption

Why study Cloud Adoption for the Enterprise?

In the foundational course, we start by exploring the digital transformation drivers made possible by cloud technologies and services. We cover how cloud works, its capabilities, types, and delivery models (IaaS, SaaS, and PaaS). We then delve into digital transformation strategies such as Agile practices, the IBM Garage Method, and Enterprise Design Thinking in order to highlight some of the practices for helping organizations get started on their transformation journey. Lastly, we will deploy a test pilot cloud application for a fictional client using IBM Code Engine. 

Cloud Adoption impact in the job market

Market Insights

Cloud computing is becoming synonymous with “computing.” In a 2019 survey of nearly 800 companies, 94% were using some form of cloud computing. Many businesses are still in the first stages of their cloud journey, having migrated or deployed about 20% of their applications to the cloud, and are working out the unique security, compliance and geographic implications of moving their remaining mission-critical applications. 

How completing this course could benefit you?

Acquiring skills in Cloud Computing will open a wide range of job opportunities, as roles in every industry will be cloud. 

Cloud is everywhere, and most jobs today require Cloud Computing skills 

Looking for a job? –  Gain a new set of skills <details about the course skills>, and join a new wave of data-savvy professionals with access to millions of jobs available in the market.

Looking for a better job? – If you already have a job and even some experience in this field, use this course to select a specialization and advance your career, by playing different roles within a Cloud team, solving real challenges within the enterprise, leveraging Cloud Computing technologies. 

Objectives

This course has the following learning objectives

1

1
Describe the role that cloud computing plays in the digital modernization journey of organizations today.

2

2
Explore the market disruptions brought by cloud adoption in the enterprise.

3

3
Understand the key technical and organizational challenges of cloud adoption.

4

4
Articulate the concepts, characteristics, delivery models and benefits of cloud computing.

5

5
Use transformation strategies such as the IBM Garage Method and Enterprise Design Thinking to create a user empathy map  and business framing exercise.

6

6
Deploy a pilot cloud application using IBM Code Engine.

Pre-requisites  

Basic IT Literacy
Basic IT Literacy - Refers to skills required to operate user-level operating system environment such as Microsoft Windows® or Linux Ubuntu®, performing basic operating commands such, copying and pasting, using menus, windows, through the mouse and keyboard. Additionally, users should be familiar with internet browsers, search engines, page navigation, fill and submit forms. 

Recognition

This course grants the following digital credentials upon completion

Completing this course you will earn the Journey to Cloud: Envisioning Your Solution - Foundational-level badge. 

What are the completion requirements?

Complete all the modules included in this self-paced online course
Pass the knowledge checks included on every module
More details about this digital badge are included below:

Badge websitehttps://www.credly.com/org/ibm/badge/journey-to-cloud-envisioning-your-solution.1


Badge website

https://www.credly.com/org/ibm/badge/journey-to-cloud-envisioning-your-solution.1


Skills Covered: 

API
Business Agility
Cloud Adoption
Cloud Computing
Digital Reinvention
Hybrid-cloud
IaaS
IBM Cloud
On-Prem
PaaS
Private Cloud
Public Cloud
SaaS
Toolchain
Badge Description: 

This badge earner has completed all the learning activities included in this online learning experience, including hands-on experience, concepts, methods, and tools related to the cloud computing domain. The individual has demonstrated knowledge and understanding of the foundations of cloud for the enterprise including: Consumer applications, Enterprise adoption, Delivery models and Industry Cloud Adoption. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/DJ6JtOBAX5iRbg3RUr0RmuB7u1Ej8Qu9,
IBM Security,Getting Started with Threat Intelligence and Huntin,19,Module 3 - Threat Map Worldwide View with IBM X-force Exchange ,3,Quiz,text,,https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/-3OfqhZA8N2Weoiu5tkLr_e2DHZlAU0V,
IBM Security,Getting Started with Threat Intelligence and Huntin,5,Module 1 - Threat Intelligence,1,Topic 2: Global Threat Trends,text,"Let's gain a better understanding of the threat landscape understanding attack trends on a global scale. 

Introduction

In this topic, we will investigate the trail of attacks led by cybercriminals organizations around the world. 

Throughout this topic, we will attempt to answer the following questions: 

What is the current status of the global threat landscape?
What are the current trends on ransomware attacks? 
Which are the top infection vectors? 
1. What is the current status of the global threat landscape? 

Understanding the attack landscape can assist security teams in prioritizing resources, drilling for the most likely scenarios, and identifying shifts in attacker techniques. 

IBM X-Force Exchange Global Trends Report 

IBM X-Force research teams analyze data from hundreds of millions of protected endpoints and servers, along with data derived from non-customer assets such as spam sensors and honeynets. 

IBM Security Research also runs spam traps around the world and monitors tens of millions of spam and phishing attacks daily, analyzing billions of web pages and images to detect attack campaigns, fraudulent activity, and brand abuse, to better protect our customers and the connected world we live in.

Let's review below the top findings from our latest IBM X-Force Threat Index Analysis to gain a global insight on global cyber attack trends. 

A poster of ""X-Force Threat Index"" with a keyboard and hologram of bar graphs
IBM Security releases the IBM X-Force Threat Intelligence Index annually, summarizing the year past in terms of the most prominent threats raised by our various research teams to provide security teams with information that can help better secure their organizations. Data and insights presented in this report are derived from IBM Security managed security services, incident response services, penetration testing engagements, and vulnerability management services. To update security professionals about the most relevant threats, IBM X-Force regularly releases blogs, white papers, webinars, and podcasts about emerging threats and attackers’ Tactics, Techniques and Procedures (TTPs).

Global Landscape 

Last year a global pandemic, economic turmoil impacting millions of people’s lives, and social and political unrest. The reverberations from these events affected businesses in profound ways, with many making a major shift to distributed workforces.

In the cyber realm, these extraordinary circumstances handed cyber adversaries opportunities to exploit the necessities of communication networks and provided rich targets in supply chains and critical infrastructure.

Top attack trends 

1) Ransomware #1 threat type

Among the trends that we tracked, ransomware continued its surge to become the number one threat type, representing 23% of security events X-Force responded to. 

Ransomware attackers increased the pressure to extort payment by combining data encryption with threats to leak the data on public sites.

The success of these schemes helped just one ransomware gang reap profits of over $123 million annually, according to X-Force estimates.

Play Video
Ransomware screen: Petya virus message on computer screen, cyberattack, hackers demanding money

2) Spear phishing campaigns - COVID-19 vaccine supply chain

The manufacturing industry overall was the second-most targeted, after finance and insurance. IBM X-Force discovered sophisticated attackers using targeted spear phishing campaigns in attacks against manufacturing businesses and NGOs involved in the COVID-19 vaccine supply chain.

3) Linux malware - affecting critical cloud infrastructure

Threat actors were also innovating their malware, particularly malware that targeted Linux, the open source code that supports business-critical cloud infrastructure and data storage. Analysis by Intezer discovered 56 new families of Linux malware, far more than the level of innovation found in other threat types.

Technical IT expert working on a cloud datacenter
Attack Trends Analysis 

The following sub-sections will provide insights on the top attack trends X-Force identified. Ransomware is undeniably the top attack type, followed distantly by data theft and server access attacks. 

Ransomware: 23% of total attacks
Data theft: shown 160% increase
Server access: shown 233% increase
The bar charts illustrate the top 3 attack forms and their initial vectors
Breakdown of attack types shown as the percentage of total attacks observed

In terms of initial attack vectors, scan and exploit rose to first place, followed by phishing and credential theft. 

Scan-and-exploit: 35% of attacks vs. 30% in previous years 
Phishing: 33% of attacks vs. 31% in previous years 
Credential theft: 18% of attacks vs. 29% in previous years
The bar chart illustrates the top initial attack vectors, with scan and exploiting vectors becoming the most common and BYOD the least.
Percentage breakdown of seven initial attack vectors observed IBM Security X-Force Incident Response teams

“Attacks” and “incidents” are used interchangeably throughout this course. An incident refers to an organization’s hotline call to the X-Force Incident Response team that results in the investigation and/or remediation of an attack or suspected attack.

Bringing it all together 

In the video below your instructor will guide you through these concepts 


2. What are the current trends on ransomware attacks? 

Ransomware attacks made up 23% of all incidents observed in X-Force engagements, up from 20% the year prior, suggesting that more cybercriminals are finding ransomware to be profitable.  

Ransomware business boomed 

Most successful ransomware groups created ransomware-as-a-service (RaaS) cartels, outsourcing key aspects of their operations to cybercriminals that specialize in different aspects of an attack. 

Why do they succeed? 

Ransomware actors are finding greater success in attacks by expanding their attack chain.

Threat actors carried out ransomware attacks predominantly by gaining access to victim environments via remote desktop protocol, credential theft, or phishing—attack vectors that have been similarly exploited to install ransomware in prior years.

Double-extorsion strategy
Since organizations can opt to recover from backups and not pay the ransom, 59% of ransomware attacks used a double extortion strategy where attackers have shifted tactics to not only encrypt data and render it impossible to access. They also stole it, and then threatened to leak sensitive data if a ransom was not paid. Certain ransomware providers even held auctions on the dark web to sell their victims’ stolen sensitive information.

What is the business impact?

The threat of reputational loss due to sensitive data being leaked has the potential to cause significant damage to the business and its customers, which could lead to lawsuits and hefty regulatory fines in addition to the costs of a lengthy recovery. 

When ransomware attackers publicly disclose sensitive data on leak sites, these the breaches are often picked up by press, further adding to the reputational harm associated with these attacks. X-Force analysis of public breach data indicates that ransomware-related data leaks make up 36% of public breaches.

Customers practiced responding to a mock data breach at IBM’s X-Force Command Center in Cambridge, Mass., in February.
New York Times - Customers practiced responding to a mock data breach at IBM’s X-Force Command Center in Cambridge, Mass., in February.

Sodinokibi most common ransomware type

The top two ransomware types observed included Sodinokibi (22% of ransomware incidents) and Nefilim (11%) – both of which blend data theft with ransomware attacks. Conservative estimates places total Sodinokibi ransom revenue at $123 million yearly through the use of extortion tactics.

Additional ransomware types frequently seen by X-Force were RagnarLocker (7%), Netwalker (7%), Maze (7%), Ryuk (7%) and EKANS (4%), while the remaining 42% of ransomware attacks were comprised of small samples of other types such as Egregor, CLOP, Medusa and others.

Attack patterns identified from our research:

Most attacked geographies: Top target USA with 58% of cases, next the UK with 8% of attack cases.
Most targeted industries: Manufacturing, professional services and wholesale, potentially because Sodinokibi actors assessed organizations in these industries have a low tolerance for downtime—perhaps especially during the pandemic—or house especially sensitive data.
Ransom demands from Sodinokibi tended to be around 1% - 5% of the victim organization’s total yearly revenue, and in one case was $42 million.
Nearly two-thirds of Sodinokibi victims paid a ransom and around 43% had their data leaked, according to X-Force estimates.
What are the recommendations for responding to a ransomware attack? 

Three key practices in preparation for a potential ransomware attack




Reflect and further investigate 

As an optional activity, if you are interested to dig deeper into this topic, follow the resources below.

Cover of The definitive guide to ransomware: Readiness, response, and remediation
IBM Report

The definitive guide to ransomware: Readiness, response, and remediation

The definitive guide to ransomware Readiness, response, and remediation.pdf
1.8 MB
3. Which are the top infection vectors? 

15% of incidents were directly related to Citrix vulnerability CVE-2019-19781 – over 15 times more than any other vulnerability – which allows an attacker to perform arbitrary code execution on a vulnerable Citrix server.

Scanning and exploiting vulnerabilities jumped into first place 

Driven by the heavy exploitation of CVE-2019-19781 they became the most common initial infection vector employed by threat actors, at 35% of all incidents. 

Why do they succeed?

Scan and exploit attacks generally require few resources and can be automated and scaled to target a wide variety of victims, which may account for why this vector saw such a high volume.

Other ways attackers used this infection vector included:

Citrix vulnerability 
Heartbleed vulnerability
Vulnerable or misconfigured management protocols
and, exploitation of the cryptographic vulnerability CVE-2017-9248.
Citrix vulnerability CVE-2019-19781

This vulnerability, disclosed in December 2019, affects the Citrix Application Delivery Controller (ADC), Citrix Gateway, and NetScaler Gateway. The vulnerability allows an attacker to perform arbitrary code execution on a vulnerable Citrix server. Allowing access to almost any computer in the system. 

Citrix Server refers to Citrix's line of desktop virtualization products: XenDesktop and XenApp. These products allow IT departments to host centralized desktops and applications, respectively. These products enable users to access applications from anywhere, no matter what hardware they are using, including tablets. Source: techopedia.com

Scan and Exploit connection with Ransomware

X-Force was aware of multiple threat actor groups taking advantage of CVE-2019-19781, including state-sponsored threat groups as well as financially motivated cybercriminals.
These include:

Hive0088 (AKA APT41; suspected Chinese state-affiliated)
Sodinokibi (AKA REvil) ransomware actors
Maze ransomware actors 
Top 10 most exploited vulnerabilities

The following is a list of the top 10 vulnerabilities exploited. Of note, just two of the vulnerabilities on this list were actually disclosed in the same year, underscoring the continuing threat from old vulnerabilities.

These include:

CVE-2019-19871: Citrix Application Delivery Controller
CVE-2018-20062: NoneCMS ThinkPHP Remote Code Execution
CVE-2006-1547: ActionForm in Apache Software Foundation (SAF) Struts
CVE-2012-0391: ExceptionDelegator component in Apache Struts
CVE-2014-6271: GNU Bash Command Injection
CVE-2019-0708: “Bluekeep” Microsoft Remote Desktop Services Remote Code Execution
CVE-2020-8515: Draytek Vigor Command Injection
CVE-2018-13382 and CVE-2018- 13379: Improper Authorization and Path Traversal in Fortinet FortiOS
CVE-2018-11776: Apache Struts Remote Code Execution
CVE-2020-5722: HTTP: Grandstream UCM6200 SQL Injection 
Phishing campaigns
Phishing was the second most commonly used infection vector, employed in 33% of attacks—slightly up from 31% last year—suggesting that attackers’ changing techniques and defensive mechanisms against phishing are keeping pace.

In October, X-Force Threat Intelligence observed a wave of phishing emails targeting individuals, organizations and supranational entities having a potential interest in technologies associated with the safe distribution of a COVID-19 vaccine.

Attacks on COVID-19 vaccine cold chain

The uncovered activity imitates the United Nations Children’s Fund’s (UNICEF) and Gavi Vaccine Alliance Cold Chain Equipment Optimization Platform (CCEOP) used to distribute vaccines globally. While currently unattributed, nation state-sponsored attackers were potentially behind these attacks.

This was a well calibrated phishing campaign designed by an adversary who was likely seeking to gain advanced insight into the transport and distribution processes of a COVID-19 vaccine, through credential harvesting.

Targets included the European Commission’s Directorate-General for Taxation and Customs Union, as well as organizations within the energy, manufacturing, website creation, and software and internet security solutions sectors. These are global organizations headquartered in Germany, Italy, South Korea, Czech Republic, greater Europe, and Taiwan.

Copy of a phishing email sent to executives in organizations related to the COVID-19 vaccine supply chain.
Phishing email sent to executives in organizations related to the COVID-19 vaccine supply chain.

Credential theft

Credential theft accounted for only 18% of attacks, a significant drop from 29% last year, suggesting that threat actors are using scan and exploit techniques in place of credential theft for many compromises, most likely due to greater success rates for scan and exploit attacks.

One of the most common approaches for credential theft is Spoofing.

Spoofing occurs when someone attempts to access your system by pretending to be within a system that you normally trust within your own network. You need to protect any interfaces that are linked to a public network from this type of attack. 

Top spoofed brands 

X-Force continued to track the top spoofed brands used in malicious domains. These are brands that threat actors attempt to mimic, capitalizing on their popularity and trust with users to trick victims into opening an email, clicking on a link, or divulging sensitive information that can then be used in an attack.

Technology and social media organizations continue to be at the top of the list for spoofed brands, with Google, Dropbox, and YouTube leading in terms of the percentage of brands spoofed.

The graph presents the yearly and cumulative percentage breakdown of ICS threats from 2011 until the end of 2020.
Google continues to be the leading spoofed brand. Adidas and PayPal also made it into the top 10, along with several top spoofed brands from the year prior: Amazon, Apple, Microsoft, and Facebook.

PayPal’s launch into the top 10 is most likely related to financially related cybercriminals seeking to steal credentials or funds.

Threat actors probably gravitate toward spoofing technology and social media organizations based on their popularity and users’ expectation of accessing these assets digitally.
In addition, spoofing email and email-associated platforms such as Google Gmail or Microsoft 365 is a common threat actor technique, judging from X-Force incident response data. These brands are also easily monetized by threat actors, as compromised accounts associated with these popular platforms can be easily sold on the dark web for a profit.

Protection techniques from phishing, spoofing and credential theft 

A recent study determined that approximately 33% of cybersecurity breaches could be blocked by a recursive DNS-based system with malicious sites blocking capabilities such as Quad9.net.

How does it work?
Nearly every transaction on the Internet begins with a name lookup. A browser, mobile device, application, or IoT system tries to establish a connection to a name (“www.example.com”) as the start of a page load or other interaction. However, names are meaningless to Internet-connected systems – they require connecting to an IP address (“10.10.2.3”) instead.

So a name-to-number mapping system known as the Domain Name System (DNS) is used to look up those names and then discover what IP addresses are associated with them.

The client device needs to communicate with what is known as “recursive DNS server” or “resolver” to perform this lookup. Usually, this resolver is provided by the ISP or the local network administrator or by the home router device – it is a server that sits somewhere nearby on the network.

The client connects to the resolver and gives it the name and then, after a fairly complex set of lookups that may span much of the Internet, the resolver hands back to the client the IP address needed.

Quad9 replaces the local resolver, performing exactly the same function, but adding a blocking list of domains known to be malicious. If the client asks for a malicious host, then the Quad9 resolver refuses to answer with the IP address, preventing the client from connecting to the malicious destination.

Why does it help?
Even when you open a fraudulent e-mail it will try to connect to a malicious server which can imitate the look and feel of your favourite site, and prompt you to enter your credentials so they can steal your information. A DNS resolver will know that site is malicious and block it automatically.

Summary

1

1
Ransomware is the top priority threat for organizations as they use a double-extorsion strategy to coerce their victims into paying the ransom.

2

2
Phishing is the second most common infection vector, recently used to gain insight into the transport and distribution processes of the COVID-19 vaccine.

3

3
Key protection techniques include the use of a recursive DNS service to avoid your system to connect with malicious sites.

Next

Let's gain a better understanding of the threat landscape by analyzing the attacks in different industries on a global scale.

Source:
[1] IBM Security. IBM X-Force Threat Intelligence Index 2021
[2] Security Intellligence - Threat Research - Article ""IBM Uncovers Global Phishing Campaign Targeting the COVID-19 Vaccine Cold Chain""
[3] IT Biz Advisor. Evolving cyber threat landscape.
[4] The New York Times - Article - ""Data Breaches Keep Happening. So Why Don’t You Do Something?"" by Christopher Mele
[5] Techopedia.com - Definition - What is a Citrix Server?
[6] IBM Services & Consulting - What is a cyber attack?
[7] Quad9 - Threat blocking - how it works?",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/bv5e_IEVuv53CHo_GfAMjCHxpLVO8tld,
IBM Security,Getting Started with Threat Intelligence and Huntin,4,Module 1 - Threat Intelligence,1,Topic 1: Reasons Behind Cyber Attacks,text,"Cyber attackers have an arsenal of sophisticated weapons at their disposal, let's explore the most used ones 

Introduction

In this topic, we will explore what is a cyber threat, why do they happen, and which methods do 'bad actors' use to carry out attacks on organizations. 

Throughout this topic, we will attempt to answer the following questions: 

Why are cyber threats so important? 
Why do cyber attackers, attack?  
1. Why are cyber threats so important?

Cyber threats are deliberate exploitations of computer systems and networks using malicious software (malware) to compromise data or disable operations.

Cyber attacks enable cyber-crimes like information theft, fraud and ransomware schemes. 

Why does this matter? 

Let's understand the risk through an example.

Your intelligent building or the entire city could be hacked today.

Given that most modern office buildings have switched to electronic access controls, hackers could simply deactivate all locks across the building, instantly rendering the entire facility unsecured, doors flapping in the breeze, and causing mass panic among his guards.

Or, they could move to paralyze the entire city, cutting power to every major building, while activating fire alarms across the city and manipulating traffic signals to cause massive traffic accidents and trap first responders helplessly across the city and preventing them from reaching their next appointments. 

Yet, here’s where things get far more worrying.

When they step on the elevator to change floors, those hackers could disable the elevator system and trap them, disrupting his visit and generating media images of him being helplessly dragged up a ladder to safety.

Or they could trigger the fire alarms or overheat a piece of equipment to cause a real fire and activate the sprinkler system, leading to images of a soaked and miserable leader cutting their visit short.

Now imagine that happening in a hospital, in your children's school, in a nearby prison or other critical buildings such as airports, dams or nuclear plants.

How could this happen? 

Cyber attacks don’t always originate outside organizations. 

""According to white hat Dark Web professionals at Black Hat, it appears that many hackers are certified professionals who operate as trusted time bombs and have already penetrated most organizations.""

ITBizAdvisor

Cyber attacks don’t always originate outside organizations. 

""According to white hat Dark Web professionals at Black Hat, it appears that many hackers are certified professionals who operate as trusted time bombs and have already penetrated most organizations.""

ITBizAdvisor

Types of Attacks 

Let's explore below the most common types of attacks identified by our cybersecurity teams: 

Physical Access 

Incidents where the attacker acquires access to the physical system, this could include from phones, computers or servers, to ATMs, elevators, cars, airplanes, CCTV, homes, and health monitors).

A person hacking the system through his phone
When Cybersecurity meets Physical Security, hackers could monitor all of the traffic cameras in the area to watch the head of state’s movements in real-time and monitor his or her schedule second by second. As they enter a building, the local CCTV cameras throughout that building could be used to surveil their movements and compile an intelligence list of everyone they meet with. 

Brute Force 

Use of trial and error to obtain a username and password for a valid account on an application to access sensitive data such as credit card numbers.

An attacker with his gloves on breaching a credit card number and translating it to the hacking system
A brute force attack can manifest itself in many different ways but primarily consists of an attacker configuring predetermined values, making requests to a server using those values, and then analyzing the response. For the sake of efficiency, an attacker may use a dictionary attack (with or without mutations) or a traditional brute-force attack (with given classes of characters e.g.: alphanumerical, special, case (in) sensitive). Considering a given method, the number of tries, the efficiency of the system which conducts the attack, and the estimated efficiency of the system which is attacked the attacker is able to calculate approximately how long it will take to submit all chosen predetermined values. 

Misconfiguration or Human Error

Incidents where attackers gain access to vulnerable systems, left exposed by inexperienced administrators or users (e.g., default factory settings).  

An attacker hacking through his laptop with his suit on
Misconfigured Clouds, Phishing and Other Insider-inflicted Weaknesses

With mobility and bring-your-own-device (BYOD) trends being the norm in today’s workplace and productivity, many say that everyone is an insider threat. Does this outlook materialize in real-world security incidents? The numbers paint a grim picture.

As gleaned from information on publicly disclosed breaches, there were several high-profile breaches eventually attributed to the errors of inadvertent insiders.

Some of the most common scenarios included basic misjudgment. These include employees storing intellectual property on their own insecure personal devices and end systems and employees and insiders falling for phishing emails that resulted in account takeover or access to sensitive data. In addition, erroneous permission-level attribution on cloud services and networked backups exposed sensitive data through weak or non-existent authentication.

The following sections provide further details on the most prominent incident types attributed to inadvertent insiders affecting organizations

Misconfigured clouds

Misconfigured cloud servers, networked backup incidents and other improperly configured systems were responsible for the exposure of more than 2 billion records, or nearly 70 percent of the total number of compromised records tracked by X-Force. There were 424% more records compromised as a result of these types of incidents year on year. 

Malvertising 

Using sophisticated tools to conceal malware within objects or images in advertising network ads, getting into user’s computer, even if they don’t click on the ad. 

An attacker hacking the system on his computer. On the background, there are two plastic bottles and another computer
How Malvertising Works

In most cases, threat actors create fake advertisements laden with malware and try to slip them past security checks at large ad networks. These infected ads can then sneak malware onto a web user’s computer, even if he or she doesn’t click on the ad. These so-called drive-by downloads are particularly effective against users who don’t regularly update their software.

The cost of malvertising is huge: A report from ad verification vendor GeoEdge estimated that the threat costs the online advertising industry more than $1.1 billion a year, and anticipated the cost rising another 20–30 percent in the upcoming years. 

Watering Hole 

A cyber attack in which the attacker seeks to compromise a specific group of end users by infecting websites that members of the group are known to visit. 

An attacker hacking the system on his computer. On the background, there are other computer screens
For example - recent reports surfaced indicating that attackers used a watering-hole attack by compromising the web servers of a Polish financial regulator, the website of the National Banking and Securities Commission of Mexico, and the website of a state-owned bank in Uruguay. With suspected links to the threat actor known as the Lazarus Group, attackers targeted more than 100 entities and successfully infected at least 30 organizations with malware that was used to exfiltrate data and money from their internal networks over an encrypted tunnel. 

Phishing 

Tricking a user into providing protected information or downloading malware by typically using the email that appears to be from a trusted or reputable source. 

An attacker hacking the system on his PC.
Losses due to cybercrime are a growing issue for financial organizations across the globe, and seeing this sector top the chart is not a surprise. Attackers are committing direct monetary theft from bank accounts by using phishing and credential-stealing malware, as well as running malicious code to intercept online transactions. Attacks on the financial sector more commonly target bank customers, but organized crime gangs are also after the enterprise networks of those organizations. 

SQL Injection 

The attack inserts database (where data is stored) commands in client applications, allowing the hacker to read and modify sensitive data, execute database administration operations. 

An attacker hacking the system on his screen in the dark
A SQL injection attack consists of the insertion or ""injection"" of a SQL (Structured Query Language is a language used to manipulate data) request via the input data from the client to the application. A successful SQL injection exploit can read sensitive data from the database, modify database data (Insert/Update/Delete), execute administration operations on the database (such as shutdown the DBMS, acronym for Database Management System), recover the content of a given file present on the DBMS file system and in some cases issue commands to the operating system. SQL injection attacks are a type of injection attack, in which SQL commands are injected into data-plane input in order to affect the execution of predefined SQL commands. 

Denial of Service (DDoS) 

These attacks overload online networks and systems with massive traffic consuming resources and bandwidth, eventually shutting down their online capabilities. 

An attacker hacking the system on his laptop
Malware 

Malicious software programmed to attack a target computer. It can block access, steal data, make systems inoperable, and even physically destroy them. 

An attacker hacking the system in a computer lab along with other attacker
Malware is the chief weapon of a cyber attack and includes viruses, worms, trojans, ransomware, adware, spyware bots, bugs and rootkits.

It installs when a user clicks a link or takes an action, once inside, malware can block access to data and programs, steal information and make systems inoperable.

Ransomware is malware used to blackmail victims by threatening to publish sensitive information or locking users out until a ransom is paid — often in cryptocurrency such as Bitcoin. IBM estimates ransomware attacks cost companies more than $8 billion globally.

Zero-day exploits introduce malware through vulnerabilities unknown to the maker or user of software or systems. It is “zero-day” because developers have had zero-time to address or patch the vulnerability. 

There are also sub-types of malware based on their lifecycle.

Malware offspring - A new version of malware that is thought to be created by the same developer as another type of malware.
Overlay malware - A type of mobile malware designed to mimic the look and feel of a legitimate, target application.
Malware family - Malware is thought to be linked to botnets and other malware operators.
Malware hybrid - New malware incorporating characteristics of two types of malware.
Evaluate your understanding

Think about the concepts covered above and evaluate your understanding by completing this sorting puzzle

2. Why do cyber attackers, attack? 

Cyber attacks are perpetrated for financial gain through crimes like fraud or extortion, as with ransomware. There are cases where sabotage or revenge are factors. Think disgruntled employee. Cyber attacks also have a political dimension and are used in cyber-warfare. 

Reasons behind cyber attacks 

Reasons for the attack can range from politics to bragging rights. Military and government targets include power grids and military bases. Civil and private sector corporations are targeted on their web servers and databases. 

Photograph of a person in the dark hacking the system
Some of the main reasons for cyber attacks are listed below: 


Military
+

Civil/private sector 
+

Hacktivism
+

Legitimate research
+

Bragging rights
+
What is the impact on businesses? 

The cost to businesses from cyber attacks and their consequences, such as data breaches, are devastating. According to the latest Cost of a Data Breach Study by Ponemon Institute, the average total cost of a data breach is close to $4 million USD.

Additionally, companies attacks can have the following impact:

Damage brands and reputations
Erode and even decimate customer loyalty
Result in loss of intellectual property
Put companies out of business
Invite regulatory penalties
Impair security for governments and states
Increase potential for future attacks
How can companies defend themselves? 

Year-by-year shifts in the cybercrime arena do not necessarily mean that much is changing in the way online fraud works or the tools cybercrime gangs are using to work it. 

The real change has to come from the defenders’ side 

The cybercriminal lifecycle has to be shortened to render it less and less lucrative over time. The faster we react to cybercrime findings and share them across the entire community, the less time each malware variant will realize successful fraud attacks.

With increased vigilance, stronger detection, and quicker reaction times, criminal operations can become much less financially viable for attackers.

Fraudsters will be forced to abandon the field for lack of profit. 

The slide contains an image of a stopwatch and a description stressing the importance of defenders as the key to shortening the cybercrime lifecycle
Bringing it all together 

In the video below your instructor will guide you through these concepts 


Summary

1

1
Cyber threats are deliberate exploitations of computer systems and networks using malicious software (malware) to compromise data or disable operations. 

2

2
There are many types of ways in which malicious hackers can perform cyber attacks beyond traditional Physical Access, some of them are using Malware, DDoS, Brute Force, Phishing, and/or Misconfiguration. 

3

3
With increased vigilance, stronger detection, and quicker reaction times, criminal operations can become much less financially viable for attackers. 

Next

Let's gain a better understanding of the threat landscape by analyzing the attacks in different industries on a global scale.

Sources:
[1] IBM. What is a cyber attack? 
[2] IBM Security. IBM X-Force Threat Intelligence Index. 
[3] Leetaru, Kalev. Forbes. When Cybersecurity Meets Physical Security.
[4] OWASP. Brute Force Attack. 
[5] Gross, Grant. Security Intelligence. How to defend against Malvertising Drive-by Attacks.
[6] BadCyber. Several Polish banks hacked, information stolen by unknown attackers. 
[7] The Security Lion. “Watering hole-style cyber attacks on the rise” warns High-Tech Bridge. 
[8] Zors, Zeljka. Help Net Security. Banks around the world targeted in watering hole attacks.
[9] OWASP. SQL Injection.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/8y5I23ypCUkbuvzhax4F2WggsiXiFRDv,
Artificial Intelligence,Watson Academy,1,Introduction,1,Artificial Intelligence Solutions,text,"AI-powered Solutions
Unlocks the value in your organization with AI. Change how your employees work and redefine the way your business operates
Do you want to learn about our training options?
Learn More
What is Watson?
IBM’s portfolio of business-ready tools, applications and solutions, designed to reduce the costs and hurdles of AI adoption while optimizing outcomes and responsible use of AI.
Why Watson?
Operationalize AI and transform how work gets done with our proven capabilities and experience with +100 million users.
Maximize Watson with IBM Cloud Paks
An open, faster, more secure way to move more workloads to cloud and AI

IBM Cloud Paks give developers, data managers and administrators an open environment to quickly build new cloud-native applications, modernize existing applications, and extend the AI capabilities of IBM Watson into their business in a consistent manner across multiple clouds.

Explore Cloud Pak Training and Credentials 
Choose a solution area you are interested in
AI & Machine LearningExplore Training
IBM WatsonExplore Training
IBM Cloud PaksExplore Training
AI role-based learning
Our role-based learning is designed to enhance your skills in the marketplace. Progressing through the three levels of our role-based learning paths will help you earn the skills required to compete in this ever evolving, fast paced world of AI.

All of our role-based learning starts with an Associate level, comprised of free learning to get you started. The Professional and Advanced levels are offered in two delivery options: Live Delivery or Digital as part of a paid subscription.
Did you know that you can get all the training you want and save with a subscription?
Learn More
badge image
Machine Learning Specialist
Build Machine Learning skills to create and build AI solutions, build foundational knowledge and expand to more advanced topics.
Explore learning path
badge image
Data Scientist with IBM Watson
Build skills to discover the use of IBM Watson to build AI solutions, build foundational knowledge and expand to more advanced topics.
Coming Soon
badge image
AI Specialist
Learn essential AI skills to create and build AI solutions, build foundational knowledge and expand to more advanced topics.
Coming Soon

Work with an expert
Data and AI Learning from IBM is developed by data scientists and technical experts, grounded in open source, design thinking, and advanced technology. Our in-depth learning is flexibly delivered how, when, and where you want it.",https://www.ibm.com/training/artificial-intelligence,
Data Science,Getting Started with Enterprise Data Science,7,Module 1 - Data Science Landscape,1,Topic 4: Data science domain,text,"
The skills needed to be a data scientist 

Introduction

In this topic, we'll explore the 3 main skill sets needed to be a data scientist.   

Throughout this topic, we will attempt to answer the following question:

What skills are needed to be an effective data scientist? 
1. What skills are needed to be an effective data scientist? 

A data scientist must possess three skills : hacking skills, math/statistics knowledge, and substantive expertise. 

Each of these skills on their own is very valuable, but when combined with only one other is at best simply not data science, or at worst downright dangerous. 

Science, Technology, and Data are intrinsically connected

An Image showing how Science, technology, and data are connected.
Three topics have Data Science and AI available for many companies right now

The evolution of data: A factor contributing to the massive adoption of AI is the exponential growth of available data. With the introduction of the Internet, social media, the proliferation of sensors and smart devices, combined with the fact that data storage became cheaper, it became more accessible than ever before.
The mechanization of scientific advances into applicable algorithms: Algorithms have been around since we could write. Recently, the development of more advanced algorithms has helped AI and Machine Learning become more powerful and efficient.
The evolution of technology: Another major factor in AI’s current success is its computing power. Back when AI was just beginning to be developed, the computing power was minimal. Computers nowadays can take much more data and heavier algorithms than in the 1950s. 
Bringing it all together

In the video below your instructor will guide you through these concepts:


Three domains involved in a Data Science project

A diagram showing the three domains which involves in Data science projects
Categories behind data science

A diagram showing categories behind Data science
Data Science

Machine learning
Statistical modeling
Experiment design
Statistics, research, mathematics 
Data Analysis

Domain expertise
Strategic problem solving
Business acumen
Communication skills
Visualization skills
Decision making based on insights 
Data Engineering

Database and data storage 
Scripting language
Artificial Intelligence
Cloud Infrastructure
Statistical computing
A data scientist must possess all three skills. Let me explain why.

For better or worse, data is a commodity traded electronically; therefore, in order to be in this market, you need to speak hacker. This, however, does not require a background in computer science—in fact—many of the most impressive hackers I have met never took a single CS course. Being able to manipulate text files at the command-line, understanding vectorized operations, thinking algorithmically; are the hacking skills that make for a successful data hacker. Once you have acquired and cleaned the data, the next step is to extract insight from it. In order to do this, you need to apply appropriate mathematical and statistical methods, which require at least a baseline familiarity with these tools. This is not to say that a Ph.D in statistics is required to be a competent data scientist, but it does require knowing what an ordinary least squares regression is and how to interpret it.

Data plus math and statistics only gets you machine learning, which is great if that is what you are interested in, but not if you are doing data science. Science is about discovery and building knowledge, which requires some motivating questions about the world and hypotheses that can be brought to data and tested with statistical methods. On the flip-side, substantive expertise plus math and statistics knowledge is where most traditional research falls. Doctoral level researchers spend most of their time acquiring expertise in these areas but very little time learning about technology. Part of this is the culture of academia, which does not reward researchers for understanding technology. That said, I have met many young academics and graduate students that are eager to buck that tradition.

Finally, a word on the hacking skills plus substantive expertise ""danger zone"". This is where I place people who, ""know enough to be dangerous,"" and is the most problematic area of the diagram. In this area, people who are perfectly capable of extracting and structuring data, likely related to a field they know quite a bit about and probably even know enough R to run a linear regression and report the coefficients; but they lack any understanding of what those coefficients mean. It is from this part of the diagram that the phrase ""lies, damned lies, and statistics"" emanates, because either through ignorance, or malice this overlap of skills gives people the ability to create what appears to be a legitimate analysis without any understanding of how they got there or what they have created. Fortunately, it requires near - willful ignorance to acquire hacking skills and substantive expertise without also learning some math and statistics along the way. As such, the danger zone is sparsely populated, however, it does not take many to produce a lot of damage.

Bringing it all together

In the video below your instructor will guide you through these concepts:


Summary

1

1
Three factors are leading to the growth and popularity of using AI systems in Data Science: 

The sheer volume of data being produced
Mechanization of systems and the advent of sophisticated AI algorithms
Advancements in technology allow for more data to be collected, stored, and processed.  
Next 

Data Science Roles  

Sources:
[1] IBM Research. Beyond the hype: A guide to understanding and successfully implementing artificial intelligence within your business. October, 2018.  
[2] Conway, Drew. The Data Science Venn Diagram, September 30, 2010. 
[3] IBM Blogs. Build Your Data Science Dream Team. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/HXNdnGsWtz9BHQ7E8AehEZcVAvv3AIRL,
IBM Security,Getting Started with Threat Intelligence and Huntin,13,Module 2 - Threat Hunting,2,Topic 3: Threat Hunting Methodology,text,"
Where human expertise, science, and technology work together to address the most dangerous cyber attacks.

Introduction

A deep dive into the threat hunting methodology.  

Questions

Gain an understanding of the concepts covered in this lecture, by going through the videos, reading material and, slides included in the sections below: 

What is Cyber Threat Hunting?
What are Tradecrafts?
What is the Art and Science of Threat Hunting?
Cyber Threat Hunting: How it Works?
1. What is Cyber Threat Hunting?

Threat hunting can be defined as “the act of aggressively intercepting, tracking and eliminating cyber adversaries as early as possible in the Cyber Kill Chain.”

The practice uses techniques from art, science, and military intelligence, with internal and external data sources informing the science of statistical and cognitive analysis. Human intelligence analyzes the results and informs the art of a response. 

Last year, 91% of security leaders reported improved response speed and accuracy as a result of threat detection and investigation

SANS Institute.

The threat landscape is constantly changing

In the physical world, new crime rings and terrorist cells continue to form; in the cyber world, threat actors are constantly crafting new ways to steal data, disrupt business and destroy reputations. Those in charge of hunting down physical and cyber threats—whether it’s the chief information security officer, the threat intelligence director, the director of fraud detection, the chief of police, or a national security leader—have a tough job to do.

One of the greatest challenges security and intelligence analysts face is being overwhelmed by massive volumes of data from different sources. And they often lack tools that give them the ability to predict and prevent physical and cyber threats. This is a crucial gap because, for every advanced threat, there’s a human behind it.

Threat Hunting provides a solution

Threat Hunting and Intelligence solutions offer cutting-edge analytics and intelligence platform that enables advanced threat-hunting with a human-versus-human approach. The solutions go beyond policy-based capabilities, to help security and intelligence analysts not only understand, but also anticipate, when and where the next threat actor will strike—whether it’s in the IT network, at the national border, or in the local streets.

This image presents a threat hunter persona looking at many screens representing the threat hunter role
The Threat Hunter

These solutions empower a new kind of cybersecurity analyst, the threat hunters, who fight cyber criminals across the public and private sector with the military-grade tools they need to outthink threat actors, arming security and intelligence analysts with tools and technology that enable them to detect, disrupt and defeat advanced cyber and physical threats by correlating and analyzing disparate data sources in near-real-time.

So, how do Threat Hunting works?

Threat hunting is not defined by solutions, although tools and techniques can significantly improve efficiency and outcomes. Instead, it’s defined by a widely accepted framework from Sqrrl. These are the four stages of Sqrrl’s Threat Hunting Loop:

Create a hypothesis.

Investigate via tools and techniques.

Discover new patterns and adversary tactics, techniques, and procedures (TTPs).

Inform and enrich automated analytics for the next hunt.

The goal for any security team should be to complete this loop as efficiently as possible. The quicker you can do so, the quicker you can automate new processes that will help find the next threat.

What are the primary elements needed to conduct threat hunting and how can I define the threat landscape?

Threat detection doesn't start with data. It starts with asking questions:

What is the problem that an analyst needs to solve?
Is it searching for a malicious insider who's secretly moving intellectual property data out of our network?
Are we trying to shut down a ring of drug dealers?
Is it searching for clandestine transactions that could indicate money laundering?
The picture below depicts the main elements to consider when looking at the dimensions considered while conducting Threat Hunting.

This image describes the different dimensions of Actionable Intelligence
Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


2. What are Tradecrafts?

Threat hunting isn’t simply a person or a piece of software. It’s a combination of person and machine, of art and science. Threat hunting requires a great deal of skill, concentration, collaboration, and more than a little creativity.

These are the areas where the human mind excels. But threat hunting also requires the right technology to sift intelligence from a vast seat of data, spot anomalies in system logs, and automate the process using a global network of threat intelligence.

The key to Threat Hunting

Threat hunters use data, analytics, and visualization tools the way an artist uses a palette, putting the pieces together until a clear picture emerges. Once they create that image, threat hunters then use their investigative skills to look for potential cyber threats. As they uncover new threats, hunters rely on their expertise and close collaboration with their teams to quarantine and safely remove the threat. When the threat is over, that experience is reported and shared with security colleagues to enrich their joint threat intelligence and prevent future attacks of a similar nature.

This image represents the different domains of the science behind threat hunting including people, Investigate, Create, Uncover, Report & Enrich
Meet a Threat Hunter

Watch a Senior Threat Researcher with IBM Security explain his role as a Threat Hunter


Plan, Prioritize and Prepare

It’s easy enough to spot a lion in a herd of zebras, but what do you do when everything looks like a lion?, Security analysts know that a successful hunt starts with knowing your prioritized intelligence requirements. 

Start by asking the right questions (below), then discover what data is likely to hold the answer.

What are the main security risks we face as an organization? Data exfiltration? Ransomware? Denial-of-service attacks?

Who needs the most protection in our organization?

Which of our data assets hold the most value to criminals: competitive data, customer data, financial data, etc.?

Where are we most exposed: our network, our employees, our partners?

What kind of technology is deployed in our network, how is it used and how can it be abused?

91% of security leaders believe threat hunting increases the speed and accuracy of their response to cyber threats.

The problem is deeper than you think

Cyberattacks often seem to present themselves all of a sudden. In reality, the most dangerous attacks often lurk undetected in your network for months before they manifest. Dwell time — the period between a cyberattack’s entry into the network and its eventual detection — is more than six months on average. It might seem that threat hunters have plenty of time to capture and kill cyber threats before they activate, but the fact that detection takes so long underscores the difficulty of doing that.

The depth of the cyber threat problem doesn’t reflect a deficiency in security technology so much as it reflects the growing proficiency of the cybercriminal community. Fraud, ransomware, and attacks-for-hire are a billion-dollar industry, and cybercrime organizations treat their operations like profitable businesses. They innovate, they disrupt and they’re constantly on the lookout for ways to increase their ROI. They also use multi-pronged attacks outside the view of traditional security systems to plant the seeds for their nefarious activities.

Organizations that master the art and science of threat hunting can reduce dwell time from months to minutes. Experience shows that the more time a cyberattack spends in your systems, the more damage it does: to your customers, your business, your brand reputation, and your bottom line.

Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


3. What is the Art & Science of Threat Hunting?

The science of threat hunting is a holistic discipline that integrates internal data, external data and intelligence, statistical analysis, and intelligence analysis to give threat hunters better visibility into hidden threats.

You can think of this technology as the threat hunter’s night-vision goggles; without it, they’re looking for a black hat in a dark forest.

How to combine art and science to perform Threat Hunting?

Standard security tools are great at detecting and blocking the known 80 percent of cyberattacks that come flooding into your network every day. But it’s the perilous 20 percent you don’t know about that presents the biggest risk to your organization. How do you find what you don’t know? By giving threat hunters better tools to hunt with, including data from different sources and advanced analytics to help them identify new patterns, models, and behavior.

Threat Hunting Elements

Threat hunting requires the combination of different elements to be brought together so the threat hunter can perform their analysis and consideration, the main elements threat hunters utilize to perform their work are listed below:

1. Internal Data and Systems
2. External Data and Intelligence
3. Statistical Analysis
4. Intelligence Analysis

Interact with the activity below to learn more about each one of the key elements used by Threat Hunters in their threat hunting process.





How do we get started?

Before you start to hunt, you need to understand what to look for, those would be your PIR or Prioritized Intelligence Requirements, to identify those you need to start by asking the following questions:

What are your key risks?

Who are you most worried about protecting in your organization?

What is your biggest exposure?

(Assets, People, Systems, Networks)

How do those outside see you – both physically and virtually?

The below table shows an example of the PIR analysis results of a Threat Hunter:

Prioritized Intelligence Requirement (PIR)
Indicator
Named Area of Interest (NAI)
Last Time Information of Value (LTIOV)
Reporting
Is there evidence of additional weaponized ransomware leveraging Equation Group exploits?
External: community discussion, DW planning, Internal: scanning, related open ports, creation of certain files
Darkweb intelligence feeds, community forums, NW data, VM data
N/A
CCIR#3, FPCON Bravo
Are there indications of a similar nonPetya ransomware attack against similar companies?
External: pharma reporting issues, LE alerts, Internal: similar scans, targeted industry phishing attacks
Darkweb intelligence feeds, community, FBI, DHS, forums, NW data, VM data, Proofpoint
N/A
CCIR#4, FPCON Bravo
What are Prioritized Intelligence Requirements (PIR)?

A PIR is a question that drives intelligence collection, whose answer will ultimately drive a decision affecting the success of an organization.

The image describes the process to use questions and answers to define PIRs
Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


Case Study: Cyber PIRs Real-World Example – Ukraine Case Study

Experts believed NonPetya was a politically-motivated attack against Ukraine by Russia, since it occurred on the eve of the Ukrainian holiday Constitution day and was targeted against a Ukrainian software company. 

What are the questions leading to the identification of our PIRs?

Are there global or geopolitical events that may precipitate an additional attack from Russian state actors?

What software or updates have the potential of enterprise-wide proliferation like M.E.Doc? What software have automated updates that can cause system-wide infection?
Is there any activity on the dark web indicating a new variant of NonPetya or similar ransomware?
Are all the administrators practicing least privilege access in the case of account compromise?
Are there indications of a similar NonPetya ransomware attack against similar companies?
What new proliferation techniques for ransomware are being discussed on the dark web?
Is there evidence of additional weaponized ransomware leveraging Equation Group exploits?
What are the internal PIRs?

Which machines have rebooted in the middle of the day (Infected machines are rebooted within 10-60 mins of infection). Indicators: system logs, server logs, endpoint (EDR) data

What Network Scanning is occurring on SMB ports - Each and every IP on the local network and each server found is checked for open TCP ports 445 and 139. Indicators: netflow, packet capture, vulnerability scanning

What machines have ports 137, 138, 139, and 445 open? Indicators: vulnerability scanning

Which machines are not patched for any of the related Equation Group exploits, specifically Eternal Romance? Indicators: vulnerability scanning

What machines have created pefc.dat? Creating the read-only file C:\Windows\perfc.dat on your computer prevents the file-scrambling part of NotPetya from running, but doesn't stop it from spreading on the network. Indicators: Syslogs, EDR

Which machines are executing PsExec and WMIC in unusual ways, for example, would be low entropy indicated a variation in use when compared to normal Administrator commands? Indicators: command line, EDR, analyzed with statistical analysis tools

Are all the administrators practicing least privilege access in the case of account compromise?

Which software or updates have the potential of enterprise-wide proliferation like M.E.Doc? Which software has automated updates that can cause system-wide infection?

What are the external PIRs?

Is there any activity on the dark web indicating a new variant of NonPetya or similar ransomware?

Are there indications of a similar nonPetya ransomware attack against similar companies on the dark web?

What new proliferation techniques for ransomware are being discussed on the dark web?

Is there evidence of additional weaponized ransomware leveraging Equation Group exploits?

Experts believed nonPetya was a politically-motivated attack against Ukraine by Russia, since it occurred on the eve of the Ukrainian holiday Constitution day and was targeted against a Ukrainian software company. Are there global or geopolitical events that may precipitate an additional attack from Russian state actors?

Reflect and further investigate

Think about the impact of the concepts covered and expand on these topics on your own

Recommended research material:

IBM Whitepaper 
The awakening of cyber threat hunting: Intelligence analysis for security and risk

The awakening of cyber threat hunting.pdf
1.8 MB
Summary

1

1
One of the greatest challenges security and intelligence analysts face is being overwhelmed by massive volumes of data from different sources.

2

2
Threat hunters, need to outthink threat actors, arming with tools and technology that enable them to detect, disrupt and defeat advanced cyber and physical threats by correlating and analyzing disparate data sources in near-real-time.

3

3
Threat detection doesn't start with data. It starts with asking questions. What is the problem that an analyst needs to solve?

4

4
A PIR is a question that drives intelligence collection, whose answer will ultimately drive a decision affecting the success of an organization.

Next 

Summary and Resources

Sources:

1. Security Intelligence. n.d. Maturing Your Security Operations Center With Threat Hunting. [online] 
2. Ibm.com. n.d. Detect, disrupt and defeat advanced physical and cyber threats. [online] 
3. IBM ‘Know Your Enemy’ IBM Threat Hunting workshop
4. IBM Whitepaper - The awakening of cyber threat hunting: Intelligence analysis for security and risk",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/iQqeqV6uMVNpsgJFi3_6mAD5i95Ia-3f,
IBM Engineering,Quick Start Sessions,19,Quick Starts: Continuous Integration and IBM Engineering Lifecycle Management,19,What is Continuous Integration?,text,"Continuous integration (CI) is a culture, a set of operating principles, and a collection of practices that enable application development teams to deliver code changes more frequently and reliably (because the deployment steps are automated). The practice is widely known as the CI/CD pipeline (the CD standing for continuous delivery). In addition, CI/CD requires continuous testing because the objective is to deliver quality applications and code to users. Continuous testing is often implemented as a set of automated regression, performance, and other tests that are executed in the CI/CD pipeline.

In this Quick Start you will see how the IBM Engineering Lifecycle Management platform supports Continuous integration and continuous testing. Continuous delivery is out of scope, as it typically automates the delivery of applications to selected infrastructure environments. In our demonstration instance, we have only a single environment.

You will see how Engineering Workflow Management (EWM) manages both source code and (optionally) models. In our case we use IBM Engineering Systems Design Rhapsody to model the application and generate the source code for it. You will see how code delivered into EWM can be built and tested automatically by triggering a Jenkins job that builds the code and then invokes a static code analysis test and functional tests in Engineering Test Management (ETM).",https://learn.ibm.com/mod/page/view.php?id=177072,
IBM Automation,IBM Robotic Process Automation - Basic I,16,Advanced Commands,15,"Queue, stack and list",video,,https://learn.ibm.com/mod/video/view.php?id=152653,
IBM Automation,IBM Robotic Process Automation - Basic I,10,Basic Commands and Variables,9,Flow control,video,,https://learn.ibm.com/mod/video/view.php?id=152642,
IBM Cloud,Journey to Cloud: Envisioning your Solution,12,Module 2 - Cloud Adoption Journey : Ideation Practices,2,Topic 1: Cloud Transformation with the IBM Garage Method,text,"Embracing Transformation with the IBM Garage Method 

Introduction

In this topic, we'll explore the current frameworks and practices that organizations need to consider as they embrace digital reinvention. 

Throughout this topic, we will attempt to answer the following questions:

What are some of the key factors and strategies that an organization should consider when moving toward modernization and digital transformation?
Explain the IBM Garage as a methodology for implementing transformation and cloud adoption.
1. What are some of the key factors and strategies that an organization should consider when moving toward modernization and digital transformation? 

The IBM Garage Method for Cloud is a method that combines practices from Design Thinking, Lean Startup, Extreme Programming, and DevOps to allow teams to quickly realize business outcomes. 

The New Digital Age 

For traditional organizations, Digital Reinvention involves a fundamental ground-up reinvention of strategy, operations, and technology; and, while technology can be a great contributor and innovator on the cloud adoption journey, a successful transformation needs to focus on more than just technology.

Image of people brainstorming on the new ideas for reinvention of the business
Your people, culture, and processes all play critical roles in achieving the greatest impact for your enterprise. Digital Reinvention requires a change in thinking and culture:

Strategically, digitally reinvented businesses maintain an overarching focus on experience rather than production. 
Embrace technological change and disruption and conceive their organization within the context of an overall business ecosystem.
Operationally, digitally reinvented businesses commit to continuous calibration and improvement. 
They foster cultures of everyday innovation among individuals and across the ecosystem, and they work to seamlessly interchange between the physical and digital.
Transform and innovate with speed 

Culture is key to the success of an agile transformation. Your culture must support small, co-located teams (teams sharing the same workspace) that are autonomous and that can make decisions that are based on efficiency and knowledge. Your squads have a diverse set of skills that support your transformation and enable the team to pivot in response to market feedback.

Why change?
One key to culture change is adopting the startup mindset. To move fast, startups eliminate the barriers between innovative ideas and production. They aren’t afraid of redefining everything from development practices to operations, from testing to production, and from tools to management. A radical transformation requires a culture that is focused on innovation.

Cultural evolution enables key business advantages. When you shift to small, empowered teams with the right organizational roles—including product owner, design, and DevOps—team members can gain a deep understanding of the customers who use their products. Each team can deliver new and improved features that the customer wants and change course based on feedback. Pivoting with speed isn't possible when you deliver quarterly or annually.

What changes?
Change is challenging. Overhauling the culture of a team and then expanding that culture shift to an enterprise starts with team organization.

Understand the fail fast philosophy
Teams must recognize that obstacles are often in the way of getting the right idea. The key is failing fast. Experiment with an idea to see whether it's what the customer wants. Implement only what is necessary to prove the idea, get customer feedback, and learn whether the idea is successful. Even if the idea fails, the team learns from the experience.

Adopting the failing fast mentality doesn't mean a lack of fun. People who work in a fun environment enjoy their work more and produce better results. A little laughter goes a long way!

Adopt the Agile methodology
Transformation to continuous delivery starts with understanding agile principles:

Develop frequently, in short intervals, that incorporate feedback and reflective practices 
Satisfying the customer through early and continuous delivery
Delivering new function on a short timescale that the team agrees to
Fostering communications across the team through daily standup meetings and playbacks
Developing at a sustainable pace
Reflecting, at regular intervals, on how the team can improve
Depending on how your team wants to organize and work, you can combine agile principles with aspects of Lean development. For example, your team might deliver time-boxed iterations, as prescribed by an agile approach, or it might work in a Kanban approach. The important thing is that the team decides what works best. 

Transform with Cloud Technologies 

Digital transformation can be empowering at an organizational scale. It can help transform the customer experience, power innovation, increase agility and flexibility, reduce operating costs and drive data-based decision-making.

But outdated IT infrastructures and applications too often can stand in the way. Genuine digital transformation — in which enterprises take full advantage of digital technologies such as artificial intelligence, automation, connected devices, and remote collaboration and communications platforms — benefit from a cloud modernization strategy that involves people, processes, and technology.

While each journey is different, there are consistent patterns across large to midsize enterprises:

Cloud is strategic to virtually every digital transformation journey because it provides rapid access to new technologies from a limitless range of sources, helping speed innovation and time to market. 
Data of all kinds—weather, social, Internet of Things (IoT), patient, partner—is at the heart of every transformation. It fuels the insights needed to help automate business processes, make informed decisions and personalize client experiences.
The blend of hybrid cloud—public and private clouds used together, and multicloud—the adoption of multiple public clouds—is making hybrid multicloud environments the new normal. This first chapter in digital transformation can be characterized as borderless and heterogeneous, embracing technologies and data, on-premises and off, from many providers. 
2. Explain the IBM Garage as a methodology for implementing transformation and cloud adoption. 

IBM Garage is a collection of practices woven together as a methodology to bring all of IBM together in helping a client realize business value at the fastest and most efficient rate possible. 

What is the IBM Garage Method? 

The IBM Garage Methodology guides how you work by bringing together an open set of practices with a human-centric, outcome-first approach. By applying design thinking, agile development and DevOps tools and techniques, you'll learn new skills and master new ways of working.

It's important to bear in mind that there is no one right way for an organization to start on the path to digitally transform its business model. A hundred organizations detailing their digital transformation strategies will likely result in 100 different roadmaps. Still, most successful digital transformations are aligned to a company's business strategy and follow two principles: 

They work backward, starting with the ideal customer experience.
They take a holistic approach to transforming operations.
Enterprises start by envisioning the experience they want customers to have with their product and their brand for as many months or years as they want them to be customers. This objective means they must analyze their market, along with technology trends, to forecast or anticipate how customer needs or expectations may change and to spot opportunities for disruption.

Organizations then determine how they must transform the digital business from end to end, including infrastructure, product development, operations, and workflows. The strength of the Garage Method is its flexibility. Some organizations may find that they wish to undergo a total transformation outlined in the lifecycle; whereas, others may only choose to focus on just a few of the most critical areas. And finally, they bring the customer experience to life and improve it continually in response to opportunity and change. 

Modernize apps with IBM Garage Methodology 


How it Works - IBM Garage Method Lifecycle Phases







For each of our lifecycle phases—Think, Transform, Thrive—we use a variety of practices from Enterprise Design Thinking, Lean Startup, Agile, DevOps, Site Reliability Engineering, and organizational change (we will be diving deeper into each of these concepts later). Related practices are logically grouped into collections of practices represented by the hexagons (see Figure 1). We have a prescriptive approach for integrating practices across the entire lifecycle. As we mentioned earlier, the digital transformation process isn't just about buying the latest technology or hiring a cloud provider. It involves a re-evaluation of organizational practices and taking a look at company culture to ensure that these practices can thrive. In general, the IBM Garage Method can be broken down into three broad phases: Think, Transform, and Thrive. 

What happens in the IBM Garage?


Organizations worldwide have started their cloud adoption journey. They are seeking validation of their approach, wanting to gain the advantages of cloud without exposing their enterprise to additional risk. 

By incorporating the IBM Garage Method into their cloud adoption strategy business create an actionable roadmap across a holistic model with measurable targets to ensure their business will thrive while on their cloud journey and transformation.

A table showing IBM garage journey
Why it matters? 
Cloud adoption should be a marathon, not a sprint. Simply moving an application or workload to the cloud is often not enough. To achieve long-term transformation through cloud, organizations need to examine how they work in order to quickly and efficiently deliver new products and services to customers. Innovation is key to staying ahead of the curve. 

Summary

1

1
Digital Reinvention is not solely about upgrading technology, it also requires a change in thinking and culture. 

2

2
IBM Garage is a collection of practices woven together as a methodology to bring all of IBM together in helping businesses to prioritize and develop a transformation strategy. 

3

3
The IBM Garage Method can be divided into 3 broad phases: 

Think - Brainstorm to develop big ideas, define personas, empathize with target end-users, develop proofs of concept, conduct user research, develop high-level business cases, and create a backlog of prioritized ideas.
Transform - Iteratively build Minimal Viable Products (MVPs) using Agile practices.
Thrive - Focus on scaling in multiple dimensions. Initial MVPs which are well-received are iterated on in subsequent MVPs, adding and testing more features.
Next 

We are going to explore the fundamentals of Enterprise Design Thinking and its application in planning for cloud adoption.  

Sources:
[1] Abdula, Moe, et al. IBM. The Cloud Adoption Playbook. 2018. 
[2] IBM.com/cloud (online). Cloud Adoption and Transformation. October, 2021.
[3] IBM.com/garage (online). IBM Garage. October, 2021.
[4] International Business Machines. IBM Garage Field Guide. 2021. 
[5] Reinitz, Rachel. IBM Cloud. Expansion of IBM Cloud Garage. November 30, 2018. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/AebuG2hNIBhurbsV7QUr9J5DWYuJom9s,
IBM Cloud,Journey to Cloud: Envisioning your Solution,18,Module 3 - Deploy a Pilot Application in IBM Code Engine,2,Case Study 1 : Defining our Minimal Viable Product,text,"Applying Enterprise Design Thinking to create a cloud-based prototype 

Introduction

In this topic, we'll explore the concepts of creating a minimal viable product for our pilot application that we will then build and test in the lab.      

Throughout this topic, we will attempt to answer the following questions:

How are Minimal Viable Product(MVP) essential to the agile/garage methodology to enable leaner cloud development? 
Case Study 
Episode III 

This industry scenario focuses on using the insights gained from Enterprise Design Thinking to create our Minimal Viable Product. 

1. How are Minimal Viable Product(MVP) essential to the agile/ garage methodology to enable leaner cloud development? 

Recap

In Module 2, Acme Airlines reached out to the IBM Garage team to help them get started with modernizing their operations. The Acme team then participated in several ideation exercises involving enterprise design thinking.

The team took a close look at the existing company culture and identified several areas of strength, as well as potential hurdles, along their modernization journey. 
Using industry data on airline passengers, they conducted an empathy mapping exercise to better understand, on a personal level, who their target audience is. 
The decision was made to begin Acme's digital transformation by transitioning its outdated legacy IT architecture to a more modern hybrid cloud infrastructure. 
Using feedback from the To-be scenario, the development team will create a prototype mobile app and pilot it through a beta test program. 
Group of people discussing in meeting
Defining Your Minimal Viable Product 

Enterprise Design Thinking enables your team to gain a clear understanding of your users and empathize with them. Through workshops, you can learn how users do their work today and what they need, to do it better, faster, or more efficiently tomorrow. Based on this new knowledge, you define a minimum viable product (MVP) that addresses their needs.

An MVP is the least amount of design, development, and infrastructure that is needed to prove that your proposed solution meets the users' needs. After you define the MVP, implement just enough architecture so the team can implement the solution. 

No matter the nuance, the intent of MVP is the same: reduce risk while you increase return on investment by building only what is necessary. ""What is necessary"" means the smallest possible version of a product that can be used to run a meaningful experiment to test key hypotheses and determine whether to continue investment.

An MVP is built with minimal investment and features but is focused on a viable solution to an opportunity. It's the foundation upon which to iterate to deliver measurable business outcomes. Each MVP is aligned to a business initiative, where you use a hypothesis-driven approach to achieve incremental business value. An MVP might have specific features that are used to gain insight into high-risk assumptions. It might not address enough key user needs for a scaled product launch.

MVPs are used for both learnings and for delivering rapid business outcomes. Many MVPs are production pilots to get the highest fidelity feedback. You must learn before you're ready to go to scaled production. The product owner works with the squad to evolve the MVP to have enough features and meet enough nonfunctional requirements to satisfy the target business objectives. 

How to Form the MVP 

Image of young millennial woman
Alice - Millennial
End User

Before you write an MVP, identify your assumptions and write hypotheses to test them. That information can help you form the MVP. What exactly is the smallest thing that Acme Airlines can build to test its hypothesis? 

Using our scenario as an example, Acme Airlines wants to engage Alice, a millennial user. After the airline conducted user research and created an empathy map and a to-be scenario, Acme understands that Alice finds the entire travel experience to be stressful. Delays, long lines, and flight scheduling changes can be nerve-wracking.

Acme wants to make the whole process a more seamless experience. The airline believes that if Alice is happy and enjoys flying, she'll opt to fly more often and recommend Acme Airlines to her friends.

Using industry data and the ideas generated from the Enterprise Design Thinking sessions, the airline identified their assumptions and created a hypothesis:

""If we provide Alice, a millennial, with a fun application that gives her a hassle-free, predictable experience, that she can run at the touch of a button, Alice will book more flights. We will see that she internalized the value of airline travel through her continued use of the application."" 

Image showing a generic Airline App.
Hypothesis-driven testing

What else might the airline need to know to test this hypothesis but reduce risk by not writing extraneous code? Alice is a millennial, so she will likely use her smartphone to book hotel accommodations. She also lives in the city, so it's likely that she will use features such as ride-sharing. Add that information: ""A fun, responsive web application that gives her savings options that she can run at the touch of a button by using her smartphone.""

Notice that the additions are in response to what the airline knows about Alice, not the result of business expectations or technical limitations. By staying focused on Alice, Acme Airlines can get the purest tests, which provide more reliable data.

Imagine that Alice lives is in Canada, where applications are often in both English and French. Code the initial MVP in one language, and then code it in another language after the first round of feedback is received. This approach saves time and effort by waiting until the essentials are validated. The MVP now states: ""We will build Alice a fun, responsive web application in English that gives her hotel and car rental recommendations that she can reserve from a single mobile app.""

The airline can ensure that its application for Alice achieves the results that it expects: to see Alice leave positive feedback and continue to use the application. The data from tests will either support that the business and design are going in the right direction or help Acme learn and shift. When Alice is happy and the application is providing her with value, she will continue using it.

By the end of the Enterprise Design Thinking workshop, your team is aligned on a clear MVP statement and can articulate the goals and future goals of the MVP. You're ready to develop the solution architecture for the MVP and move forward to inception as the first step to an MVP build.

Image of cloud developer working on his laptop
Why it matters? 
Developers are under pressure to create new ideas. However, the pressure to deliver can lead to end products that aren't received well by the public and ultimately cost the company time and money invested in a bad idea. Creating a prototype (our MVP) allows companies to innovate rapidly, minimize risk, and scale to meet the users' needs.

Summary

1

1
A minimum viable product is the absolute bare minimum in a delightful experience that your target persona accepts to accomplish a goal. 

2

2
MVPs are used for both learnings and for delivering rapid business outcomes. Many MVPs are production pilots to get the highest fidelity feedback. 

3

3
Employ knowledge about your end-user when forming your hypothesis allows you to narrow the project scope and deliver solutions quickly with minimal risk.   

Next 

We are going to take our MVP from concept to production by deploying a pilot cloud airline application using IBM Code Engine. 

Sources:
[1] Goldberg, Rick & Platenberg, Sarah. Creating a Minimal Viable Product. IBM. Accessed October 22, 2021. 
[2] Golderberg, Rick. Innovate with Enterprise Design Thinking in the IBM Garage. February 10, 2020. 
[3] IBM Cloud Education. Microservices. IBM. March 30, 2021. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/bWpjek8gkKuXvO0COgq-deqcIHbT7bQa,
Data Science,Getting Started with Enterprise Data Science,8,Module 1 - Data Science Landscape,1,Topic 5: Data science roles,text,"
An overview of the key roles within data science

Introduction

In this topic, we'll explore the unique roles of the data analyst, data scientist, and data engineer. 

Throughout this topic, we will attempt to answer the following questions:

How is a Data Science team structured?
What are the primary role and responsibilities of the data analyst?
What are the primary role and responsibilities of the data scientist?
What are the primary role and responsibilities of the data engineer?
How do other supporting roles add value to the data science team? 
1. How is a Data Science team structured?

In practice, several people work on a team to build data products. Your analyses will only be as good as the team that is responsible for collecting, building, and analyzing the underlying data. 

Data Science isn’t just about frameworks, data sources, and pipelines—it’s also about people. Enterprises embarking on a Data Science journey have a much greater opportunity for success when they have executive leadership support and the right talent in key Data Science roles.

Data science is a team sport.

The greatest challenge of the big data revolution is making sense of all the information generated by today’s vast digital economy. It’s well enough for an organization to collect every slice of data it can reach, but how does it extract value from this massive volume of information?

Profiles of people behind Data Science.
Sally

Data Analyst
6 years of experience

Profiles of people behind Data Science.
Maria

Data Scientist
15 years of experience

Profiles of people behind Data Science.
Tom

Data Engineer
10 years of experience

The more data you have, the better the quality of your reports and strategic recommendations, right? Sure...if you can analyze that data intelligently and quickly and make it actionable with valuable insights. Otherwise, more data can mean more problems: messy data, storage woes, security risks, frustrated business teams, and overloaded IT staff. So how do you figure out what you really want from your data—and which data can get you those answers? How do you turn raw metrics and records into information that has (and drives) actual business value? Do you have the skills your business needs to drive results? Successful organizations build data science teams that incorporate different skill sets and responsibilities instead of relying on a few elite individuals.

In practice, several people work on a team to build data products. Your analyses will only be as good as the team that is responsible for collecting, building, and analyzing the underlying data.

What does the data science team bring to the table?

The deeper programming types, such as data engineers and front-end developers, bring consumability to data science. The mathematics and data scientist types use statistical algorithms to find patterns in data. Throughout the process, everything needs to align with the preferred business outcome, which is guided by the eye of the business analyst. Working together, the data science team can outthink today’s challenges and problems to create new opportunities and possibilities for tomorrow. Which talents and abilities define the members of a data science team, and how do they complement each other?

Bringing it all together

In the video below your instructor will guide you through these concepts:


2. What are the primary role and responsibilities of the data analyst? 

Data Analytics involves finding the right data in these data systems, cleaning it for the purposes of the required analysis, and mining data to create reports and visualizations. 

Good Data Analysts select and address the business problems that have the most value to the organization. Armed with data and analytical results, they must present their informed conclusions and recommendations to technical and non-technical stakeholders.

Who is a Data Analyst？

To be a Data Analyst, you need to be in constant communication with the business users. 

Line of business users
This encompasses all users that consume the output of data science analytics, for example, business analysts analyzing dashboards to monitor the health of a business or the end-user using an application that provides a recommendation as to what to buy next.

A data analyst's profile named Sally along with her characteristics and role.
Sally

Characteristics

Great communicator and presentation skills
Critical thinking and agile design
Familiar with visualization tools 
Role

Domain exploration and understanding
Collect and analyze data
Understand data
Explain and visualize results
Communicate with stakeholders
Bringing it all together

In the video below your instructor will guide you through these concepts:


3. What are the primary role and responsibilities of the data scientist? 

Data Scientists take Data Analytics even further by performing deeper analysis on the data and developing predictive models to solve more complex data problems.

Solving problems and answering questions through data analysis is quickly becoming the norm in today’s data-driven world. As real-world experiments become ubiquitous in modern business, the data scientist is evolving into the role that stokes, tweaks, and fuels this operational engine.

Who is a Data Scientist?

Data scientists are often referred to as “unicorns” because they have a rare combination of talents: they handle a variety of responsibilities and skill sets covering mathematics, statistics, domain expertise, communications, and more. 

A data analyst's profile named Maria along with her characteristics and role.
Maria

Characteristics

Is eternally curious - What? Why? How?
Research mindset
Versed in statistics and math
Constantly being in the “know”
Role

Collect and analyze data 
Extract insights from patterns using machine learning models
Explain and visualize results
Basically, the job of the data scientist is to look for hidden patterns. They accomplish this by applying advanced analytics techniques, including (but not limited to) machine learning, modeling, statistics, and visualization. Often, data scientists will construct models to predict outcomes or discover underlying patterns; their game plan is to produce actionable insights that can be used to improve future outcomes.A good data scientist will not just address business problems; he or she will zero in on the problems that have the most value to the organization.

Because data scientists are involved in each step of the journey in building data products, they tend to bring a holistic view to solving problems with data. However, they can’t be experts in everything—this is where their team can help. They experiment continuously by deploying new predictive models, business rules, and orchestration logic into next-best-action-powered applications.

A skilled data scientist explores and examines data from multiple disparate sources. They will pore through all incoming data with the goal of linking new information to historical data to find a relationship or trend that offers a crucial competitive advantage or addresses a pressing business problem. They don’t just collect and report on data; they look at it from many angles, determine what it means, and then recommend ways to apply the findings. They need to make sure their queries are correct and must be able to back up their conclusions with sound models and trusted data—as the data scientist is often expected to present recommendations to management and executive teams.

Top skills for data scientists

Data scientists are distinguished by their strong business acumen, plus the ability to communicate findings to both business and IT leaders in a way that can influence how an organization approaches a business challenge. The data scientist often becomes the liaison between the IT department and C-level executives. Therefore, he or she must be able to speak both “languages” and understand the hierarchy of data—they can’t just be a data expert. This also means data scientists must have a solid understanding of the business as well as the conviction to stand behind their findings in the face of opposition.

Data scientists are inquisitive and curious: exploring, asking questions, doing what-if analyses, and questioning existing assumptions and processes. A data scientist’s technical skills often include multiple programming languages, familiarity with big data management and analysis tools like Apache Hadoop and Spark, and experience with tools that help them visualize data and insights.

Bringing it all together

In the video below your instructor will guide you through these concepts:


4. What are the primary role and responsibilities of the data engineer? 

Data Engineering entails managing data throughout its lifecycle and includes the tasks of designing, building, and maintaining data infrastructures. 

These data infrastructures can include databases – relational and NoSQL, Big Data repositories and processing engines – such as Hadoop and Spark, as well as data pipelines – for transforming and moving data between these data platforms.

Who is a Data Engineer?

The data scientist may be responsible for uncovering hidden patterns in data, but where do you think they get their data, and what happens when their solutions need to scale to thousands of users or handle sensitive information? Data scientists can only go so far without proper support to operationalize their work.

Enter the data engineer. At a high level, data engineers help gather, organize and clean the data that data scientists will ultimately use to build their analysis. If data scientists extract value from data, data engineers make sure data flows smoothly from source to destination so it can be processed.

A data engineer's profile named Tom along with his characteristics and role.
Tom

Characteristics

Tech savvy 
Programming skills (Python, Golang, Java, etc.)
Can perform math and statistics operations
Frequently uses Machine Learning API calls
Familiar with infrastructure architecture 
Role

Transforms raw data into usable pipelines
Utilizes advanced programming techniques
Test and deploy Machine Learning models
Manages the data infrastructure
Data engineers are responsible for setting up systems and processes that other data workers—including data scientists—use and rely on to work with data. Data engineers must understand how to finesse the flow of data to minimize movement latencies and bring agility to analytics. They also work with front-end developers when moving data science projects into production.
In many organizations, a data engineer will oversee integrating data, including designing, building, and measuring data ingestion and integration pipelines for large volumes of temporal data from different sources. Examples include database extracts, application server logs, scanned images, voice recordings, Twitter streams, websites, and health sensor data. Once continuous pipelines are installed—to and from these huge “pools” of filtered information—data scientists can pull relevant data sets for their analyses.

Data engineers are often tasked with laying the groundwork for a data analyst or data scientist to easily retrieve the data needed for their evaluations and experiments.

Top skills for data engineers

Data engineers are hard-core engineers who understand the internal workings of database software. They compile and install database systems, write complex queries, scale them to multiple machines, manage backups and deploy disaster recovery systems. They develop, construct, test, and maintain architectures such as databases and large-scale data processing systems.

Good data engineers are always learning and thinking about which new technologies will help them drive the business forward. This prompts them to develop a deep programming background, as well as cultivate familiarity with Hadoop-based technologies such as MapReduce, Hive, and Pig. Data engineers usually have significant experience with SQL-based technologies and NoSQL technologies, as well as data warehousing methodologies and solutions such as extract, transform, load (ETL).

Bringing it all together

In the video below your instructor will guide you through these concepts:


5. How do other supporting roles add value to the data science team? 

Data Science isn’t just about frameworks, data sources, and pipelines—it’s also about people. Enterprises embarking on a Data Science journey have a much greater opportunity for success when they have executive leadership support and the right talent in key Data Science roles.

Here are suggestions from IBM concerning who should be on a Data Science team. Our experience suggests that these specific roles should be filled to get buy-in on the project and create a successful solution.

Team behind Data Science

Here are suggestions from IBM concerning who should be on a Data Science team. Our experience suggests that these specific roles should be filled to get buy-in on the project and create a successful solution.

Table showing the details of the team behind Data science.
Data science team support

2 men and 2 women discussing at work.
Business Sponsor
Plan, define KPIs, and provide Feedback

Developer
Development, Deployment & Monitoring

Data Engineer
Integration & Refinement

Data Scientist
Analysis & Modeling

Data Analyst
Analysis & Visualization





PRODUCT OWNER
DEVELOPER
THE BUSINESS ANALYST
The Product Owner providing business guidance, user research support, and serving as a consultant in terms of understanding the business need of the company in relation to the Data Science project in scope. 

Summary

1

1
The responsibility of the data analyst involves finding the right data in these data systems, cleaning it for the purposes of the required analysis, and mining data to create reports and visualizations.

2

2
The job of the data scientist is to look for hidden patterns. They accomplish this by applying advanced analytics techniques, including (but not limited to) machine learning, modeling, statistics, and visualization. Often, data scientists will construct models to predict outcomes or discover underlying patterns; their game plan is to produce actionable insights that can be used to improve future outcomes.

3

3
Data engineering entails managing data throughout its lifecycle and includes the tasks of designing, building, and maintaining data infrastructures. These data infrastructures can include databases – relational and NoSQL, Big Data repositories and processing engines – such as Hadoop and Spark, as well as data pipelines – for transforming and moving data between these data platforms.

Next 

Summary & Resources

Sources:
[1] Bradshaw, Paul. How to be a Data Journalist.
[2] IBM Research. Data Science is a Team Sport: Do You Have the Skills to be a Team Player?",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/3qkKN5wkqoPxLE1EFpA5rxUbnrEuVFwl,
Data Science,Getting Started with Enterprise Data Science,13,Module 2 - Data Science on the Cloud,2,Topic 2: End-to-end data science with IBM Watson Studio,text,"
Introduction to data cleansing tools 

Introduction

In this topic, we'll explore the reasons for using data cleaning tools such as IBM Watson Data Refinery. 

Throughout this topic, we will attempt to answer the following questions:

Why might a data scientist use a data cleansing tool?
What are the capabilities of data cleansing tools such as IBM Watson Data Refinery? 
1. Why might a data scientist use a data cleansing tool?

In order to apply AI, the first step of the workflow starts with connecting and accessing data. 

Data scientists spend up to 80% of their time finding and preparing data, and 57% of data scientists said that cleaning and organizing data is the least enjoyable part of their job. The problem isn’t just limited to data scientists. Business analysts face similar struggles obtaining the data they need to build reports — often having to wait weeks for their IT team to extract data from the source systems.

Watson Studio – tools for supporting the end-to-end data science workflow







To address the issue, we provide the integrated capability to refine and wrangle data with Data Refinery, a tool that makes fast, self-service data preparation a reality. Watson Studio comes with more than 35 data connectors to the most popular data sources, whether they are in the IBM Cloud, 3rd party Clouds or application, or on-premises. 

Watson Studio – tools for supporting the end-to-end data science workflow

A table/chart displaying tools and applicaitons needed for Data Science.
Organizations are now tapping data science and artificial intelligence (AI) as a technology-enabled business strategy. Experimentation is accelerating across multiple clouds. The need for speeding through data preparation and exploration, modeling, and training has never been higher. 

To succeed in enterprise AI, a business must:

Bring your algorithms to wherever data resides.
Increase productivity of data scientists, analysts, developers, and subject matter experts.
Operationalize the data science lifecycle from insight to prediction and optimization.  
Together with IBM Watson Machine Learning, IBM Watson Studio is a leading data science and machine learning platform built from the ground up for an AI-powered business. It helps enterprises simplify the process of experimentation to deployment, speed data exploration, and model development and training, and scale data science operations across the lifecycle. 

IBM Watson Studio empowers organizations to tap into data assets and inject predictions into business processes and modern applications and then optimize business value with visual data science and decision optimization. It's suited for hybrid multi-cloud environments that demand mission-critical performance, security, and governance — in public clouds, in private clouds, on-premises, and on the desktop, including IBM Cloud Pak™ for Data. 

Bringing it all together

In the video below your instructor will guide you through these concepts:


2. What are the capabilities of data cleansing tools such as IBM Watson Data Refinery? 

Watson Studio – differentiating capabilities

Bringing it all together

Watch the video below to better understand this product offering:


Summary

1

1
Data cleaning is often the most time consuming and least enjoyable part of a data scientist's work. 

2

2
Data cleansing tools use AI algorithms to make the process of data cleansing faster and more efficient. 

Next 

Tools to support the Data Science Lifecycle - Exploration Phase 

Sources:
[1]  IBM Cloud. IBM Watson Studio. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/b70NQrWHMF5t88Gxgg5O3LZSDJaCJPDX,
IBM Cloud,Journey to Cloud: Envisioning your Solution,4,Module 1 - Digital Transformation with Cloud Computing,1,Topic 1: The New Digital Age,text,"Exploring the concept of digital transformation and its business impact.  

Introduction

In this topic, we will In this topic, we'll explore what digital transformation is, the key transformation drivers, and the technologies at the forefront of enterprise digital transformation.   

Throughout this topic, we will attempt to answer the following questions:

How have industries benefitted from adopting digital transformation practices?
What role have technologies such as AI, Cloud, and IoT played in the digital revolution?  
Image showing gear and a key person in middle.
1. How have industries benefitted from adopting digital transformation practices? 

We are at one of the tipping points in history where the impact of technology on business is significant enough to transform the whole way that business gets done. 

The New Digital Age 

It’s no secret that the pace of innovation and change has accelerated rapidly over the last few years. From the explosion of social media to online shopping and apps to track your driver, digital transformation has quickly become an integral part of daily life.

Abstract background with connected lines and futuristic digital background for AI and Cloud technology
In every industry, business leaders realize customer expectations have created tremendous pressure to change the way they set their strategies and run their organizations. Yet, because they have to manage existing, often traditional, offerings and operations, new requirements to incorporate information and interactivity quickly drive up costs and complexity.

Business leaders have long used information technology (IT) to improve productivity and efficiency, reach new markets and optimize supply chains. What’s new is that customer expectations have also changed. People everywhere are using social networks to find jobs and restaurants, lost friends and new partners—and, as citizens, to achieve common political goals. They are using the internet for entertainment, shopping, socializing, and household management.

At the same time, businesses are undertaking their own digital transformations, rethinking what customers value most, and creating operating models that take advantage of what’s newly possible for competitive differentiation. The challenge for business is how fast and how far to go on the path to digital transformation.

What is Digital Transformation?

Collage of transformation drivers like using mobile devices, social media, IoT devices, Cloud services
Digital transformation takes a customer-driven, digital-first approach to all aspects of a business, from its business models to customer experiences to processes and operations. It uses AI, automation, hybrid cloud, and other digital technologies to leverage data and drive intelligent workflows, faster and smarter decision-making, and real-time response to market disruptions. And ultimately, it changes customer expectations and creates new business opportunities.

While many organizations have undertaken a digital transformation in response to a single competitive threat or market shift, it has never been about making a one-time fix. According to MIT Sloan Management Review, ""Digital Transformation is better thought of as continual adaptation to a constantly changing environment."" Its goal is to build a technical and operational foundation, to evolve and respond in the best possible way to unpredictable and ever-changing customer expectations, market conditions, and local or global events. 

Digital Transformation Drivers 

Customer expectations have always been the prime drivers of digital transformation. It began when a rush of new technologies made new kinds of information and capabilities accessible in new ways, such as: 

Mobile devices
Social media
The internet of things (IoT)
Cloud computing
Pioneers—disruptors—such as Amazon and Netflix snatched market share from their competition by adopting these technologies to:

Reinvent business models (e-commerce, electronic delivery).
Optimize processes (supply chain management, new feature development).
Constantly improve the customer experience (in-context customer reviews, personalized recommendations).
Competitors adapted to provide even more capability and convenience (or they struggled and maybe even disappeared). Today, customers expect to conduct all business digitally, wherever, and whenever, using any device, with all the supporting information and content they need close at hand. Ultimately, digital transformation is about meeting these ever-escalating expectations. But often, an organization's entry point is a transformation initiative that addresses a specific means to this end, such as:


Automating business processes
+

Defending against disruption
+

Dealing with change effectively
+

Enabling on-demand access to more resources, with fewer limits
+
COVID-19 pandemic: The ultimate driver? 

Graph/ Chart representing Digital Transformation caused by Covid 19.
In 2020, the COVID-19 pandemic laid bare every organization's digital transformation efforts and progress (or lack thereof). Manufacturers learned just how quickly and effectively they could get new products to market. Retailers scrambled to provide customers with new and safer ways to shop. Employers adopted or expanded technologies that let employees work from home.

Among operational workflows, supply chains were the most glaringly exposed. Supply chains are always vulnerable; according to the McKinsey Global Institute, supply chain disruptions lasting one month or longer occur every 3.7 years. But shortly after the pandemic began, the United States suddenly imported almost 50% less from major trading partners. Companies were forced to undergo years or decades' worth of supply chain transformation in weeks or months.

The arrival of the pandemic forced us to reconsider the capabilities we had at our fingertips and adapt them to the circumstances we now face daily. Take video conferencing, for example. We’ve had this capability for years, but we never quite realized its full potential. Now, video calls are an essential part of our day, proving their worth far beyond hosting meetings remotely.

Patients can have a video consultation with their doctor and have their prescription made available at their closest pharmacy. It benefits the doctor, the patient, and everyone who may come into contact with the patient. This highlights the rapid acceleration of digital transformation out of necessity – but also how the right technology can enhance employee and customer safety. 

2. What role have technologies such as AI, Cloud, and IoT played in the digital revolution? 

Enterprises will look back at the start of the 2020s as a moment when they either seized opportunities or naively hoped old ways of doing business would survive. Smarter businesses realize that it’s a critical time to meet people’s evolving needs and give their customers exceptional experiences. Business transformation is the key to accomplishing these goals, studies show.

As disruption and volatility may continue for the foreseeable future, the organizations that apply human-focused strategies Business transformation will determine who thrives in the new landscape ahead and the new capabilities of emerging and advancing technologies will be positioned to lead in the “new normal” to come.

Transformational Technologies

Virtually any digital technology can play a role in an organization's digital transformation strategy. But technologies that figure to play a central role today and in the near future include: 


Digitization
+

Artificial intelligence and automation
+

Cloud Computing 
+

Microservices
+

Internet of Things
+

Blockchain 
+
Combining multiple technologies, including cloud, cognitive AI, mobile, and the Internet of Things (IoT), Digital Reinvention rethink customer and partner relationships from a need-, use- or aspiration-first perspective. Digital Reinvention helps organizations create unique, compelling experiences for their customers, partners, employees, and other stakeholders. These benefits arise regardless of whether enablement or fulfillment of the experience involves the direct provision of products or services or orchestration of products or services from partner organizations by way of a business ecosystem. The most successful digitally reinvented businesses establish a platform of engagement for their customers, acting as an enabler, conduit, and partner. 

How Cloud Fits into the Bigger Picture 

The ongoing digital revolution affects individuals and businesses alike. Increasingly, social networks and digital devices are the default means for engaging government, businesses, and civil society, as well as friends and family members. People use mobile, interactive tools to determine who to trust, where to go, and what to buy. This means that the last best experience that people have anywhere becomes the minimum expectation for the experience they want everywhere, including in the enterprise. Given the competitive landscape, this means that enterprises must undertake their own digital transformations, rethink what their customers value most, and create operating models that take advantage of what is newly possible for competitive differentiation. The challenge for the enterprise is how fast and how far to go down the path to digital transformation and cloud adoption.

Increasing customer expectations and a more competitive business context have placed tremendous pressure on business leaders to change the way they set their strategies and run their organizations. New requirements to incorporate more information and greater interactivity quickly drive up costs and complexity.

Futuristic digital background connecting different technologies to cloud.
Business leaders have long used information technology to improve productivity and efficiency, reach new markets, and optimize supply chains. What is new is that customer expectations have changed.

How can enterprises best respond to this shift? 
How can they take advantage of the opportunity to innovate and grow through technology adoption?  
How can they do all this cost-efficiently?
This is the domain of digital transformation and its intersection with cloud adoption. Digital transformation incorporates the change associated with the application of digital technology in all aspects of society. Cloud Adoption is the way in which businesses implement digital transformation.

In our work with clients, we have found that enterprises that can develop and effectively execute a digital transformation strategy and take full advantage of new technologies, such as cloud are able to transform their business models and set a new direction for entire industries.

We believe the most crucial decision that a company can make to successfully pursue a digital transformation strategy is to wholeheartedly yet thoughtfully adopt the cloud as the IT platform of choice. We have observed many companies that have successfully used cloud adoption to rapidly advance their digital transformation strategy. We have also seen companies make unsuccessful cloud adoption decisions that have hampered or set back their pace of digital transformation. What we will show you in this book is how to model your decision-making process after the successful transformations while avoiding the common pitfalls we’ve seen in the unsuccessful transformations.

How the hybrid cloud can create the world's computer


Digital Transformation in Action

Image showing a truck which is ready to ship goods representing Convoy App
Convoy looks to disrupt the USD $700billion trucking industry Convoy, a Seattle-based start-up, has developed an Uber-like app interface to give individual truckers a more efficient way to connect with individuals and businesses that want to ship goods.

By matching truck drivers with clients directly, Convoy disintermediates traditional brokers. The company and other entrants such as Los Angeles-based Cargomatic intend to reduce the average transportation prices and delivery times while increasing average loads and trucker income.

Lending Club challenges traditional banks in their backyard San Francisco-headquartered Lending Club offers better borrowing and lending rates than traditional banks. 

Operating online, Lending Club automates peer-to-peer loans across its digital platform. With operating costs that are 60 percent lower than those of traditional banks, Lending Club has experienced massive growth over the past five years. Lending Club still has substantial space for growth in an area of the market that has historically produced more than 4 billion in profits annually. 

Women sitting at home with the mobile/laptop ready to lend money from the Lending app
Image showing fields covered with solar panels producing clean energy.
Clean Energy Collective devises new modes of generation and consumption Founded in 2010, the Colorado-based Clean Energy Collective (CEC) builds, operates, and maintains community-based clean energy facilities.

Using remote metering technologies that integrate seamlessly with utility billing systems and installing solar panels either on customer’s property or elsewhere, CEC is able to track and apply for clean energy production credits directly on customer’s bills. 

By the end of 2015, more than 27 gigawatts of solar capacity was online in the United States, enough to power more than 5.4 million homes. 

Why it matters? 
Since the rise of the internet in the late 1990s, companies under increasing pressure to stay ahead of the curve. Social media, big data, artificial intelligence, and cloud computing are just a few of the technologies at the forefront of the digital revolution. To remain competitive, organizations must continually adapt and innovate in order to meet customer expectations.

Summary

1

1
Digital Transformation/ Reinvention uses AI, automation, hybrid cloud, and other digital technologies to leverage data and drive intelligent workflows, faster and smarter decision-making, and real-time response to market disruptions. 

2

2
The biggest disruptors are: 

Mobile devices
Social media 
The internet of things (IoT)
Cloud computing
3

3
Digital Reinvention requires a change in thinking and culture:

Maintain an overarching focus on experience rather than production
Embrace technological change
Commit to continuous improvement
4

4
Four initial steps can set organizations on the path toward Digital Reinvention.    

Envision possibilities
Create pilots
Deepen capabilities
Orchestrate ecosystems
Next 

We are going to explore the fundamentals of cloud computing and the role it plays as a digital transformation driver. 

Sources:
[1]  Abdula, Moe, et al. IBM. The Cloud Adoption Playbook. 2018.
[2] Bassinder, Jeremy. IBM- United Kingdom. How COVID-19 has accelerated digital transformation – and how to stay ahead in 2021. January 15, 2021.
[3] Berman, Saul J, et al. IBM. Digital Reinvention in Action. 2016.
[4] IBM.com. Services and Consulting.  What is Digital Transformation?",,
IBM Cloud,Journey to Cloud: Envisioning your Solution,3,Module 1 - Digital Transformation with Cloud Computing,1,About this Module,text,"
Module 1 

Digital Transformation with Cloud Computing

Introduction

In the foundational course, we start by exploring the digital transformation drivers made possible by cloud technologies and services. We cover how cloud works, its capabilities, deployment models, and delivery types. 

This page covers the following lecture details:

Duration
Objectives
Skills
Instructors
Topics
Duration

This module can be completed in an average of 90 minutes

Objectives

In this module you will learn the following concepts:

1

1
Describe the role that cloud computing plays in the digital modernization journey of organizations today 

2

2
Explore the market disruptions brought by cloud adoption in the enterprise 

3

3
Understand the key technical and organizational challenges of cloud adoption 

4

4
Articulate the concepts, characteristics, delivery models, and benefits of cloud computing 

Skills

Upon completing this module you will have acquired the following skills: 

Digital Transformation 
Digital Transformation Drivers 
Cloud Computing
Legacy IT Architecture
Public Cloud 
Private Cloud 
Hybrid Cloud
IaaS
PaaS
SaaS
About the Instructors 

Meet the IBM subject-matter experts, who will guide you through the topics on this module.   

Instructor photo of JJ Asghar


JJ Asghar

Developer Advocate
IBM    





Biography


JJ works as a Developer Advocate representing the IBM Cloud all over the world. He mainly focuses on the IBM Kubernetes Service and OpenShift trying to make companies and users have a successful onboarding to the Cloud Native ecosystem. He’s also been known in the DevOps tooling ecosystem and generalized Linux communities. If he isn’t building automation to make his work streamlined he’s building the groundwork to do just that.
 

 



Image showing instructor Andrea Crawford


Andrea Crawford 

Distinguished Engineer (DevOps)
IBM Garage for Cloud             





Biography



Andrea has more than 20 years of cross-industry experience in application development, architecture, and accelerated delivery. Andrea has made significant contributions to IBM’s own DevOps transformation and to clients’ DevOps journeys. Andrea provides technical oversight and leadership of technical enablement, solution design, and offering innovation for which focuses on accelerated application development with Agile, DevOps, and Cloud-Native Development. Andrea applies her expertise in accelerated development across all industries, heterogeneous technologies, and hybrid cloud scenarios.


Inclusiveness begets a diverse workforce and groundbreaking, innovating thinking and ideas. Andrea has a passion for supporting and enabling under-represented minorities in IT.



Image showing instructor Mihai crivati


Mihai Crivati  

CTO Cloud-Native and Red Hat Solutions 
IBM  





Biography



Mihai Criveti is the CTO for Cloud-Native and Red Hat Solutions at IBM. He is an STSM, TOGAF, and Red Hat Certified Architect, a member of the IBM AoT, and an advocate for Open-Source development. 


He helps clients increase their innovation, business agility, and accelerate time to market through Digital Transformation and Infrastructure Modernization. He develops solutions that help clients adopt DevSecOps, shift-left operations, SRE models – and automated operations across hybrid and multi-cloud environments that take advantage of Kubernetes, OpenShift, and modern development techniques.


Mihai leads the Ireland AoT affiliate (ITEC) and the OpenShift Solution Guidance initiative and is passionate about growing the technical community in Ireland, mentoring, coaching, and supporting academic initiatives.



Image showing instructor Tom Creamer


Tom Creamer     

OCP Technical Elite Team 
IBM





Biography



Tom is the OCP Technical Elite team and IBM Global Markets Americas CTO.  He is an IBM Distinguished Engineer, Open Group Distinguished IT Specialist, and IBM Master Inventor with over 190 patents granted.  Tom is devoted to the growth of the IBM Technical community and is the chairman of the IBM Global Markets Americas Distinguished Engineer Board.

Tom spent 12 years as the Technical Director and IBM CTO to Verizon and has 35 years with IBM working predominately in the Telecommunications industry.

Tom is a member of the IBM Telecom, Media & Entertainment CTO community, and the IBM Edge Computing and 5G Multi-access Edge Compute (MEC) architecture teams.



Image showing instructor Hardy Groeger


Hardy Groeger    

IBM Distinguished Engineer  
IBM





Biography



Hardy Groeger is an IBM Distinguished Engineer and the Director for Competitive Insights. He brings extensive technology and automotive industry experience into every engagement, based upon his roles as CTO Integrated Account Daimler, Lead Architect for Mobile Solutions, Collaboration Solutions, and Websphere / Websphere Portal. 

Hardy's technology journey started in IBM's software development labs in the US. He holds a Master of Science in Business Computing from the University of Paderborn, Germany.



Image showing instructor Dr. Thalia Hooker


Dr. Thalia Hooker    

Global OCP Elite Team
IBM 





Biography



Dr. Hooker is an innovative Thought Leader dedicated to customer success with over 20 years of experience in IT. She works across IBM and complex customer organizations with their technology strategy and recommends solutions that solve customer needs while bringing the right IBM capabilities to bear, whether software, hardware, and/or services. She helps drive OpenShift Container Platform (OCP) adoption and IBM Cloud Paks deployment on OCP in her current role. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/YRAmfoaA5UssgccZGTJvpOr4Q3Dm6Epk,
Data Science,Getting Started with Enterprise Data Science,3,Module 1 - Data Science Landscape,1,About this Module,text,"Introduction

Understand the evolution and relevance of data science in the world today. Exploring the scientific method for science projects, and the data science team key roles. 

This page covers the following lecture details:

Duration
Objectives
Instructor
Topics
Skills
Duration

This lecture can be completed in an average of 60 minutes

Objectives

Upon completing this lecture you should be able to achieve the following objectives:

1

1
Understand what data science is 

2

2
Explore the history and evolution of Data science 

3

3
Understand the factors contributing to the growth of data science  

4

4
Explore the domains involved in the field of data science

5

5
Understand the critical roles involved in data science projects

About the Instructor

Meet the IBM subject-matter expert, who will guide you through this lecture

Image profile of Dr. Ray Lopez


Dr. Ray Lopez 

Manager 

Data and AI Curriculum Development at IBM





Biography



Dr. Ray Lopez is a Manager and the Senior Curriculum Architect for the IBM Data and AI Expert Labs group.  In this role, he manages a team of amazing data science experts who build educational content on the use of IBM Watson tools and APIs for data science tasks and infusing AI into enterprise processes.



Dr. Lopez has over 25 years of experience in software engineering, system administration, enterprise architecture, and consulting for a very wide variety of clients in the public and private sectors, and also has over 30 years of experience as a researcher and professor.  



He is a Professor of Practice at the University of Texas at San Antonio (Enrollment=34,000; Carnegie Classification R2, High Research Activity) where he teaches courses on statistics, research design, cognitive science, sensation and perception, and ethics.  

 

Education



Dr. Lopez received his BA from the University of Texas at Austin, and MS and Ph.D. from the University of Texas at Arlington, where his pioneering research on midbrain mechanisms of pain processing became the basis for many topic areas of pain research.



Background



Raised in rural south Texas, Dr. Lopez is the eldest of five siblings and very proud of his Mexican and Tejano heritage.  He currently lives in San Antonio, Texas with his family and is an active volunteer for several organizations.  In his spare time, he enjoys reading about history, science, and philosophy, playing chess, vintage computing, camping and hiking, and spending time with the family.

Topics

Let's explore the topics included in this Module


Topic 1: The urgency of science 
+

Topic 2: What is Data Science?
+

Topic 3: The 'Data' in Data Science
+

Topic 4: Data science domains
+

Topic 5: Data science roles
+

Summary & Resources
+

Quiz
+
Skills:

The Scientific Method 
Data Science history
Data domains
Data science
Data engineer
Data scientist 
Data analyst ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/XX-yJcGM3yJkkrvOHwAml6hqG_NiIN2a,
Data Science,Getting Started with Enterprise Data Science,17,Module 2 - Data Science on the Cloud,2,Topic 6: The role of the data engineer in an integrated Cloud environment,text,"
The role of the data engineer in the data science lifecycle 

Introduction

In this topic, we'll explore the unique role of the data engineer within the data science lifecycle. 

Throughout this topic, we will attempt to answer the following questions:

What are the primary tasks of the data engineer, and how do they differ from the data analyst and data scientist?
1. What are the primary roles of the data engineer, and how do they differ from the data analyst and data scientist? 

Big Data Engineers focus on collecting, parsing, managing and analyzing large data sets, in order to provide the right data sets and visual tools for analysis to the data scientists. 

They understand the complexity of data and can handle different data varieties (structured, semi-structured, unstructured), volume, velocity (including stream processing), and veracity. They also address the information governance and security challenges associated with the data. They have a good background in software engineering and extensive programming and scripting experience. 

Roles integrated environment

A data engineer named Tom and his Job details.
Tom

Data Engineer

His Job: 
Architects how data is organized and ensures operability.

What he does:

Uses tools for data munging and data wrangling to develop pipelines that Maria can use to apply statistical methods and build models.
Works with Maria to transform research models into production quality-systems.
Builds data infrastructure and ETL pipelines. Works with Spark, Hadoop, and HDFS.
Responsibilities:

Manage the Model lifecycle end-to-end.

A table/chart displaying tools and applicaitons needed for Data engineering.
Data preparation: Extract and transform data into usable formats

Data engineers must understand how to finesse the flow of data to minimize movement latencies and bring agility to analytics. They also work with front-end developers when moving data science projects into production. In many organizations, a data engineer will be in charge of integrating data, including designing, building and measuring data ingestion and integration pipelines for large volumes of temporal data from different sources. Examples include database 3 The data engineer extracts, application server logs, scanned images, voice recordings, Twitter streams, websites and health sensor data. Once continuous pipelines are installed—to and from these huge “pools” of filtered information—data scientists can pull relevant data sets for their analyses.  

Data Science integrated environment layers

A table showing Data science integrated environment layers.
Data engineers are often looking to deliver actionable insights through the marriage of structured and unstructured data. Unfortunately, you can’t assume that the data—even structured data—is perfect and ready to use. It may need some work:

Data may be incomplete, with missing or incorrect values
Data can be corrupted or incoherent, with broken lines or fields in the wrong place
Data may need to be transformed or standardized into an algorithm- or model-friendly format
Data may have too much “noise”: random, irrelevant data that throws off analytics
First, you have to capture and preserve the data structure as you move it into your Hadoop environment. Next, you’ll need to bring along your metadata, particularly business metadata around critical terms, semantic meaning and business context. The goal is to relate different data sets to find insights. One approach is data tagging.

Tagging data before handing it off to Hadoop, Spark or another implementation medium for analysis can save an enormous amount of time. The trick is to process the data “just enough” so that it is useful when loaded into a big data environment—able to be discovered, accessed and joined with Data preparation: Extract and transform data into usable formats other data. Later, if you’re looking to operationalize a useful analysis, it may be worthwhile to do more data preparation work at that time.

Watson Studio

Screenshots showing open source tools
Data science is a rapidly evolving discipline that leverages an ever-widening array of tools and capabilities to learn and exploit data. Because of such inherent complexities surrounding adoption, integration and support, the work of the data scientist can be daunting.

That complexity is one of the reasons IBM several years ago set out to bring clarity and uniformity to the otherwise disparate data discovery and analytics process.

The goal: create a solution that leveraged the best capabilities available, in an integrated, collaborative platform that was easy to access and use. With it, everyone from data scientists to business analysts would be able to not only tackle the discipline but conquer it.

Up until the time IBM rolled out the popular IBM Watson Studio, if someone wanted to engage in data science, he/she would have to search the web, review components like Jupyter notebooks, or development platforms like Scala and R, big data tools like Hadoop, and much more – and then learn how to use them. Not unexpectedly, the wide variety of tools and programs led to relatively slow adoption, challenging integration, and cumbersome support.

In addition, IBM research showed that once up and running, most data scientists’ workflows were often fragmented, requiring them to toggle between a variety of workspaces and tools to complete a job. For example, they might use Data Shaper to clean data, Jupyter for modeling and MatPlotLib for visualization. These tools support a linear process, but data scientists’ workflows are more cyclical.

When IBM launched the IBM Watson Studio in 2016, users for the first time had a solution that integrated the most sought-after development, notebook and analytics tools, in a simple-yet-scalable web-based platform. It also enabled users to connect live with IBM for all support.

In addition to these goals, the vision for IBM Watson Studio was that it would accommodate agile workflow, simplify the experience of working with data, and bring all the tools into a unified data ecosystem. IBM Watson Studio, in function and form, helps simplify the data science universe. Today, Data Science Experience is one of the premier data science systems available on the market, with thousands of users worldwide.

Notice that with Watson Studio as an integrated environment, you can run your choice of Python or R as your analysis package.
Additionally, you can customize your compute environment moving from CPU to GPU and modifying storage capacity.
You can seamlessly incorporate Visual Recognition service within Watson Studio to work with images
That includes the ability to customize the machine learning flow regarding nodes and hyperparameters.  
Employ latest neural networks to predict and build patterns

Screenshots showing latest neiral networks which predict and build patterns.
You can submit your deployment to the Watson Machine Learning service and let the system recommend the optimal algorithm such as eXtreme Gradient Boost or Random Forest Trees. 

Summary

1

1
Big Data Engineers focus on collecting, parsing, managing and analyzing large data sets, in order to provide the right data sets and visual tools for analysis to the data scientists. 

Next 

 Tools to support the Data Science Lifecycle - Production Phase 

 Sources:
[1] IBM Research. The Data Science Lifecycle: From experimentation to production-level data science. 
[2] IBM Cloud. IBM Watson Studio.  ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/qt17w5IXYE_who8K3UTo5S3Ww6tRdYVN,
IBM Security,Getting Started with Threat Intelligence and Huntin,8,Module 1 - Threat Intelligence,1,Summary & Resources,text,"Module 1

Reflecting on what we learned from this lecture and planning the work ahead.

Summary

1

1
Global financial crime is the biggest motivation for cybercriminals and their organizations to attack companies around the world.

2

2
Cyber attacks enable cyber-crimes like information theft, fraud, and ransomware schemes.

3

3
Malware is malicious software. It’s the chief weapon of a cyber attack and includes viruses, worms, trojans, ransomware, adware, spyware bots, bugs, and rootkits.

4

4
The real change has to come from the defenders’ side. The cybercriminal lifecycle has to be shortened to render it less and less lucrative over time.

5

5
Organizations need an integrated approach to protect from cyber attacks that go beyond the boundaries of their enterprise into the extended ecosystem. 

Resources

1. Featured Resources

The following resources were used to develop the material included in this lecture:

A poster of ""X-Force Threat Index"" with a keyboard and hologram of bar graphs
IBM Report 



X-Force Threat Intelligence Index 2021  


X-Force Threat Intelligence Index 2021.pdf
1.1 MB
2. Additional Resources

The following resources are complementary material to those provided in this lecture.

IMPORTANT: You don't need to access the material included below to complete this course, but just in case you are eager to go beyond the boundaries of the content included in this course, below are some additional resources which you can pursue to learn more.

The URLs included above are pointing to resources hosted outside our domain and beyond our control. Therefore that material is provided as-is and no support will be given if the links are removed from public access by the content owners.

List of resources

1) IBM Security 

https://www.ibm.com/security

2) Security Intelligence 

https://securityintelligence.com/

3) IBM Uncovers Global Phishing Campaign Targeting the COVID-19 Vaccine Cold Chain 

https://securityintelligence.com/posts/ibm-uncovers-global-phishing-covid-19-vaccine-cold-chain/",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/afJmUJjrN4GMcGxnJxCBO-ad25A5WCIT,
IBM Automation,Overview,0,IBM Automation,0,What is Automation?,text,"What is Automation?
An AI-powered software that helps businesses complete routine and repetitive tasks with low human intervention. IBM utilizes four different types of Automation to help our clients work more efficiently. New to Automation? Check out this Automation Overview!

What are the types of automation?
Basic Automation - Basic automation takes simple, rudimentary tasks and automates them. This level of automation is about digitizing work by using tools to streamline and centralize routine tasks, such as using a shared messaging system instead of having information in disconnected silos. Business process management (BPM) and robotic process automation (RPA) are types of basic automation. Check out this use case from Deloitte to see Basic Automation in action.

 

Process Automation - Process automation manages business processes for uniformity and transparency. It is typically handled by dedicated software and business apps. Using process automation can increase productivity and efficiency within your business. It can also deliver new insights into business challenges and suggest solutions. Process mining and workflow automation are types of process automation. Check out this use case from Credigy to see Process Automation in action.

 

Integration Automation - Integration automation is where machines can mimic human tasks and repeat the actions once humans define the machine rules. One example is the “digital worker.” In recent years, people have defined digital workers as software robots that are trained to work with humans to perform specific tasks. They have a specific set of skills, and they can be “hired” to work on teams. Check out this use case from Cobmax to see Integration Automation in action.

 

Artificial Automation - The most complex level of automation is artificial intelligence (AI) automation. The addition of AI means that machines can “learn” and make decisions based on past situations they have encountered and analyzed. For example, in customer service, virtual assistants powered can reduce costs while empowering both customers and human agents, creating an optimal customer service experience. Check out the journey of the autonomous ship the Mayflower to see AI powered Automation in action.",https://www.ibm.com/academic/topic/ibm-automation,
IBM Cloud,Journey to Cloud: Envisioning your Solution,9,Module 1 - Digital Transformation with Cloud Computing,1,Summary & Resources,text,"Module 1

Reflecting on what we learned from this lecture and planning the work ahead

Summary

1

1
Digital Transformation/ Reinvention uses AI, automation, hybrid cloud, and other digital technologies to leverage data and drive intelligent workflows, faster and smarter decision-making, and real-time response to market disruptions. 

2

2
Cloud computing, sometimes referred to simply as “the cloud,” is the use of computing resources — servers, database management, data storage, networking, software applications, and special capabilities such as blockchain and artificial intelligence (AI) — over the internet, as opposed to owning and operating those resources yourself, on-premises. 

3

3
Six key cloud attributes being used to power business model innovation, which we’ve dubbed business enablers: 

Cost flexibility 
Business scalability
Market adaptability
Masked complexity 
Context-driven variability 
Ecosystem connectivity
4

4
Public cloud is a type of cloud computing in which a third-party service provider makes computing resources—which can include anything from ready-to-use software applications, to individual virtual machines (VMs), to complete enterprise-grade infrastructures and development platforms—available to users over the public Internet. 

5

5
The chief advantage of IaaS, PaaS, SaaS or any 'as a service' solution is economic; a customer can access and scale the IT capabilities it needs for a predictable cost, without the expense and overhead of purchasing and maintaining everything in its own data center.

Resources

1. Featured Resources

The following resources were used to develop the material included in this module: 

Screenshot of The cloud adoption playbook
IBM Research Paper 

The Cloud Adoption Playbook: Proven Strategies for Transforming Your Organization with Cloud 


Proven Strategies for Transforming Your Organization with Cloud .pdf
7.9 MB
2. Additional Resources

The following resources are complementary material to those provided in this lecture.

IMPORTANT: You don't need to access the material included below to complete this course, but just in case you are eager to go beyond the boundaries of the content included in this course, below are some additional resources which you can pursue to learn more.

The URLs included above are pointing to resources hosted outside our domain and beyond our control. Therefore that material is provided as-is and no support will be given if the links are removed from public access by the content owners.

List of resources

1) Digital Transformation 

https://www.ibm.com/topics/digital-transformation

2) Benefits of Cloud Computing 

https://www.ibm.com/cloud/learn/benefits-of-cloud-computing",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/5d5-cwAVR8PgpSLWRJVPafxkNvS2NeMn,
Data Science,Getting Started with Enterprise Data Science,6,Module 1 - Data Science Landscape,1,Topic 3: The 'Data' in Data Science,text,"
The explosion of data in the Information Age  

Introduction

In this topic, we'll define what data science is as well as its emergence as a field of study. 

Throughout this topic, we will attempt to answer the following questions:

What factors have led to the explosive growth of data in the 21st Century?
How have cognitive systems fueled the need for more efficient methods of data collection and analysis?     
Explain the five characteristics of data and why each is important in a business context? 
What's the difference between structured and unstructured data? 
1. What factors have led to the explosive growth of data in the 21st Century? 

The Internet of Things (IoT) is a giant network of connected things and people – all of which collect and share data about the way they are used and about the environment around them. 

That includes an extraordinary number of objects of all shapes and sizes – from smart microwaves, which automatically cook your food for the right length of time, to self-driving cars, whose complex sensors detect objects in their path, to wearable fitness devices that measure your heart rate and the number of steps you’ve taken that day, then use that information to suggest exercise plans tailored to you. There are even connected footballs that can track how far and fast they are thrown and record those statistics via an app for future training purposes. 

We are all digitally connected in real - time, anywhere in the world

An image with a phone, Earth, and an airplane icon.
How does it work?

Devices and objects with built-in sensors are connected to an Internet of Things platform, which integrates data from the different devices and applies analytics to share the most valuable information with applications built to address specific needs. These powerful IoT platforms can pinpoint exactly what information is useful and what can safely be ignored. This information can be used to detect patterns, make recommendations, and detect possible problems before they occur.

For example

If I own a car manufacturing business, I might want to know which optional components (leather seats or alloy wheels, for example) are the most popular. Using Internet of Things technology, I can:

Use sensors to detect which areas in a showroom are the most popular and where customers linger longest
Drill down into the available sales data to identify which components are selling fastest
Automatically align sales data with supply so that popular items don’t go out of stock
The information picked up by connected devices enables me to make smart decisions about which components to stock up on based on real-time information, which helps me save time and money.
With the insight provided by advanced analytics comes the power to make processes more efficient. Smart objects and systems mean you can automate certain tasks, particularly when these are repetitive, mundane, time-consuming, or even dangerous. 

Let’s look at some examples to see what this looks like in real life.

Collecting huge amounts of data about how the world works

An image talking about collecting data about how the world works.
An image talking about collecting data about how the world works.
An image talking about collecting data about how the world works.
From traffic patterns and music downloads to web history and medical records, data is recorded, stored, and analyzed to enable the technology and services that the world relies on every day.

Consider these statistics:
2.5 Quintillion bytes of data created every day. (That’s 2,500,000,000,000,000, 000 bytes)90% of the data in the world today has been created in the last two years alone.

Every minute 1.7 MB of data is created for every person on the planet. All 7.3 billion of us.

We were trying to build a system that can deal with this massive amount of data because human intelligence is not scaling in the way that data is scaling.

Creating a digital representation of the physical world

Industry and science now can come together to explore new solutions to existing challenges

The price of not knowing

What is the price of not curing cancer?
What is the price of not discovering alternative energy?


An image showing planet Earth.
Bringing it all together

In the video below your instructor will guide you through these concepts:


2. How have cognitive systems fueled the need for more efficient methods of data collection and analysis?   

Cognitive Computing are systems that learn at scale, reason with purpose, and interact with humans naturally.

It is a mixture of computer science and cognitive science – that is, the understanding of the human brain and how it works. By means of self-teaching algorithms that use data mining, visual recognition, and natural language processing, the computer is able to solve problems and thereby optimize human processes.

Bringing it all together

Watch the video ""The Future of Cognitive Computing"" to learn about the interaction between data and cognitive computing:


Cognitive systems have the potential to reshape computing and are an area of computing that you should not ignore. It’s very easy when we get into areas of Cognitive Computing and Artificial intelligence to rapidly drop down to Deep Machine Learning and algorithms. These are exciting and fascinating areas of technology, but I think the thing we must keep in mind is “Towards what end?”

What is it that we are trying to do?
It’s all about the outcomes:

Changing the world
Changing entire industries
Seeing things and getting insights that we have never been able to grasp before.
I encourage you to think about what we can do with this technology as well as how can we impact society in ways that we’ve never been able to before?

What are we after here?
It’s also very tempting to talk about what we’re trying to do as replicating what human brain does, but that is not at all what this is about. We were not trying to do what previous AI researchers were trying to do. We were not trying to mimic the human brain. We were trying to do something very simple -- to analyze and garner insight from this massive amount of data.

These devices will need to be cognitive.
They will need to make real-time decisions about the environment based on learning about the environment and learning about driver behavior. Every single industry is being swamped with data. Every industry is trying to access that 80-90% of dark data and get insights to differentiate. We are at an industry inflection point.


Oil & Gas
+

Retail
+

Internet of Things
+

Security
+

Energy
+

Healthcare
+

Transportation
+
Bringing it all together

In the video below your instructor will guide you through these concepts:


3. Explain the five characteristics of data and why each is important in a business context? 

Characteristics of data

The 5 Vs

Volume



Refers to the vast amounts of data generated every second.
 


Click to flip
If we take all the data generated in the world between the beginning of time and 2008, the same amount of data will soon be generated every minute. This makes most data sets too large to store and analyze using traditional database technology. 



Partner volume with value:

To gain the most intrinsic value from your business data, your data team must carry out sampling on specific datasets. But now, thanks to the volume of information provided by big data and the techniques required to manage this, your company should have the capability to perform analysis on data collected in real-time.



The more data you collect in your business, the more insight you can offer to stakeholders, which should speed up the decision-making process, lead to improved customer relationships, and more monetary value gained from your data.


Click to flip
Variety
 

Refers to the different types of data we can now use.


In the past, we only focused on structured data that neatly fitted into tables or relational databases, such as financial data. In fact, 80% of the world’s data is unstructured (text, images, video, voice, etc.). With big data technology, we can now analyze and bring together data of different types such as messages, social media conversations, photos, sensor data, video or voice recordings. 



Partner variety with value:

Structured and unstructured data of all types will be flowing into your business, but to gain the most value from these different varieties, it’s all about having the right data management plan in place.



The numerous varieties of data you collect allow your business to become more diverse. During the collection process, you will no longer encounter just one specific stream for each customer or client.



When successfully collected, these different varieties of data can be amalgamated together to form one large dataset, which your business can use to create customer journey maps, improve engagement and increase retention.


Velocity
 

Refers to the speed at which new data is generated and the speed at which data moves around.


Just think of social media messages going viral in seconds. Technology allows us now to analyze the data while it is being generated (sometimes referred to as in-memory analytics) without ever putting it into databases. 


Veracity



Refers to the messiness or trustworthiness of the data.
 


It’s crucial when presenting your data that you have complete confidence in what’s on the page in front of you. When showcasing the findings in your data to stakeholders, it could mean the difference between new investment or the dismissal of your conclusions.



While it may not be an essential task to dot the i’s and cross the t’s in the early stages of data conceptualization, the further you move into the analysis and visualization process, the more people will see the data you have collected. 



The added trust in your data will allow for better differentiation and provide the chance to show stakeholders how you can gain an edge over your competitors.


Value



Refers to having access to big data is no good unless we can turn it into value.
 


Companies are starting to generate amazing value from their big data. 



If you decide to push ahead with your implementation, detailed knowledge of how to get the most value of your data will help impress shareholders, lead to increased productivity and better business growth.


Bringing it all together

In the video below your instructor will guide you through these concepts:


4. What's the difference between structured and unstructured data? 

All data is not created equal. Some data is structured, but most of it is unstructured. 

Structured and unstructured data is sourced, collected and scaled in different ways, and each one resides in a different type of database. 

Data in the world today

Every industry problem and societal challenge can now be addressed by understanding its underlying data
80% of the world’s data is dark data
A graph showing Data in the world today.
STRUCTURED DATA
UNSTRUCTURED DATA
Structured data — typically categorized as quantitative data — is highly organized and easily decipherable by machine learning algorithms. Developed by IBM in 1974, structured query language (SQL) is the programming language used to manage structured data. By using a relational (SQL) database, business users can quickly input, search and manipulate structured data.



Examples of structured data include dates, names, addresses, credit card numbers, etc. Their benefits are tied to ease of use and access, while liabilities revolve around data inflexibility:



Pros

Easily used by machine learning (ML) algorithms: The specific and organized architecture of structured data eases manipulation and querying of ML data.  
Easily used by business users: Structured data does not require an in-depth understanding of different types of data and how they function. With a basic understanding of the topic relative to the data, users can easily access and interpret the data.
Accessible by more tools: Since structured data predates unstructured data, there are more tools available for using and analyzing structured data.
Cons

Limited usage: Data with a predefined structure can only be used for its intended purpose, which limits its flexibility and usability.
Limited storage options: Structured data is generally stored in data storage systems with rigid schemas (e.g., “data warehouses”). Therefore, changes in data requirements necessitate an update of all structured data, which leads to a massive expenditure of time and resources.
Bringing it all together

In the video below your instructor will guide you through these concepts:


Summary

1

1
Massive amounts of data is being produced globally, in large part because of IoT devices, social media, and cloud technology.  

2

2
Cognitive (AI) systems can assist data scientists in their task of collecting and making sense of all of this generated data, greatly reducing time, money, and effort.

3

3
Data can be filtered into 5 broad characteristics, 'the 5 Vs', to help data scientists make sense of what they are working with.
Volume - Having a sense of the amount of data which needs to be processed.
Variety - What types of data are you working with, and are they unstructured or structured data.
Velocity - How quickly is the data being generated.
Veracity - How accurate is the data.
Value - Not all data holds equal weight for a given tasks, data analysts need to able to filter out the noisy data to find what they are looking for. 

4

4
Data falls into two broad categories: structured and unstructured.Structured Data is is highly organized and easily decipherable by machine learning algorithms. Unstructured Data cannot be processed and analyzed via conventional data tools and methods.

Next 

Data Science Domains 

Sources:
[1] Columbus, Louis. Forbes. Roundup Of Internet Of Things Forecasts And Market Estimates. December 13, 2018.
[2] IBM Cloud Education. Structured v. Unstructured Data: What's the Difference? June 29, 2021. 
[3] IBM Research. The Future of Cognitive Computing. January 13, 2014. 
[4] NASA Jet Propulsion Laboratory. NASA to Launch Fleet of Hurricane-Tracking SmallSats. November 10, 2016. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/hJtzNLQMBrZGUpyVDHNKMDTCPQtUJBzf,
IBM Automation,IBM Robotic Process Automation - Basic I,27,Routines,26,Routines,video,,https://learn.ibm.com/mod/video/view.php?id=152667,
Data Science,Getting Started with Enterprise Data Science,2,Course Overview,0,Release Notes,text,"EXPERIENCE CHANGES



Portal Experience

Responsive design for mobile access and learning on the go.
Overall new design with more information for every activity, including the new course segments detailed below: 
About the course.
Key takeaways, digital credential claiming and posting process.
Information about complementary courses for learning continuity.
CONTENT CHANGES



Module 1 & 2

Complete redesign of the content of the modules to ensure a comprehensive set of topics are covered in alignment with the current trends in the market.
Comprehensive updates on the notes behind slides to ensure proper alignment with the topics included on the slides
Videos featuring SMEs to describe the content on slides have been streamlined to simplify the learning journey.
New questions have been added to the quizzes to align with the new content
Sources have been transitioned to MLA citing format, URLs have been removed to avoid broken links over time.
Slides have been redesigned in alignment with IBM Research Carbon Design Practices
Some slides have been deconstructed into graphics and texts and brought within the web page alignment to allow mobile users to responsive design
Images now have ALT text so they can be used by screen readers to support accessibility.
Module 3

The lab has been integrated into the Rise360 platform, as a plug-in component, with an updated design.
Students will identify patterns within their data, that may be indicators of fraud.
Lab title has been renamed to accommodate content changes.
The case study story has been enhanced to include a comprehensive scenario that flows through the hands-on activities included in the Milestones.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/uqsatzwtW3-g9m8hNf8CHH_Fm5nceiN4,
Artificial Intelligence,Overview,0,IBM Watson,0,What is IBM Watson,text,"What is IBM Watson?
Watson is IBM’s suite of enterprise-ready Artificial Intelligence (AI) services, applications, and tooling.

Why IBM Watson?
Build your competitive advantage with Artificial Intelligence

Watson helps you unlock the value of your data in entirely new, profound ways. By freeing users from repetitive tasks, you can empower your teams to focus on more creative, higher-value work. With insights from Watson, you can predict and shape future business outcomes, while rethinking your practices and workflows. With Watson, you have the AI platform for business.
Understand: with Watson, you can analyze and interpret all of your data, including unstructured text, images, audio, and video.

Reason: with Watson, you can provide personalized recommendations by understanding a user’s personality, tone, and emotion.

Learn: with Watson, you can utilize machine learning to grow the subject matter expertise in your apps and systems.

Interact: with Watson, you can create chatbots that can engage in dialog.
",https://www.ibm.com/academic/topic/artificial-intelligence,
IBM Engineering,Quick Start Sessions,3,Quick Starts: IBM Engineering Systems Design Rhapsody,3,What is IBM Engineering Systems Design Rhapsody?,text,"IBM Engineering Systems Design Rhapsody is an integrated systems and software engineering environment for analyzing project requirements, designing, generating and testing embedded applications. Rhapsody is used by both Systems and Software Engineers, and the models allow a seamless transition between the two.
Rhapsody for Systems Engineers allows requirements to be verified and validated through model execution and allows the specification and validation of complex architectures.

Rhapsody for Software Engineers uses the OMG Unified Modeling Language (UML) to enable rapid requirements analysis and visual, model-based design and code generation. Generated code may be executed and graphically debugged using the design diagrams. Developers may work at either the model or code level and Rhapsody keeps the two in synchronization. Legacy code may even be reverse engineered into a model for visualization, documentation or further rework.

Rhapsody helps engineering teams design and build complex functionality in less time.
There are introductory labs for both Rhapsody for Systems Engineers and Rhapsody for Software Engineers. In this Quick Start however, you will focus on something simpler – getting started. The UML and SysML modeling languages are very rich and Rhapsody (out of the box) does not hide any of that detail. Rhapsody itself is a very powerful tool but again all of that power is available immediately out of the box. All of this means that Rhapsody (and by extension – modeling) can be perceived as being too complex and this can be quite a hurdle for new users. In this lab you will see that this does not have to be the case.

In the labs, you get to:

Explore a model
Build a model
The estimated time to complete the labs is 2 hours.",https://learn.ibm.com/mod/page/view.php?id=166076,
IBM Security,Getting Started with Threat Intelligence and Huntin,20,Summary,4,Final Remarks,text,"Key Takeaways

If you went through all the activities included in this course, you have now started your journey towards better understanding the impact of cybersecurity in the Enterprise.

With your newly acquired cybersecurity foundational knowledge, you can evaluate what areas within this vast domain you would like to investigate further, and eventually take a deep dive to master cybersecurity-relevant skills that could help you in your current or future job.

Below are some relevant takeaways, which summarize, what you have been exposed to throughout this course.

1

1
Study emerging threats using IBM X-Force Exchange.

2

2
The job market for cybersecurity is rapidly changing and many are now studying and learning cybersecurity, acquiring practices and technical know-how, so it is up to you to stay ahead by leveraging a future insight on what's to come and having practical knowledge on the adoption of these revolutionary technologies. 

3

3
Constantly stay in the know by exploring research papers on this field, there are countless scientific publications from our research teams around the globe that you can access in our IBM Research Publications portal. 

4

4
Keeping a real feel of the technology gaining unique, marketable skills, by accessing cybersecurity software tools and technologies; work through our open-source IBM Research tools and tutorials and the IBM premium enterprise software tools on the Cloud showcased throughout this course. 

5

5
And more importantly gain real fieldwork, adopting these practices in your line of work or applying for new positions in new areas of interest.

Remember to promote achievement in social media 


Once you complete this course you will be eligible to receive an IBM official digital badge - recognizing the level of knowledge you have achieved.

Follow the instructions below to make the best of your digital badge please visit our IBM Badges FAQ - https://www.ibm.com/training/P430723G05904L37

Courses we recommend you take next

Once you have completed this Foundational course, you will have two series of courses ahead of you:

1. Explore other Foundational courses - You can explore other technology focus areas such as Cloud, Data Science, or AI, by taking other Foundational courses available within this IBM education program.


Click to flip
Learn more at:

https://www.credly.com/org/ibm/badge/getting-started-with-enterprise-grade-ai




Click to flip

Learn more at:

https://www.credly.com/org/ibm/badge/getting-started-with-enterprise-data-science





Learn more at:

https://www.credly.com/org/ibm/badge/getting-started-with-cloud-for-the-enterprise




2. Start your track specialization - You can choose Cybersecurity as your technology focus area and start going deeper, acquiring more skills in this domain by taking the Intermediate and Advanced self-paced courses, and/or completing the Practitioner-level course available within our Cybersecurity roadmap.


Click to flip
Learn more at:

https://www.credly.com/org/ibm/badge/enterprise-security-in-practice



Click to flip

Learn more at:

https://www.credly.com/org/ibm/badge/security-operations-center-in-practice






Learn more at:
https://www.credly.com/org/ibm/badge/ibm-cybersecurity-practitioner-certificate.1





For more information about the IBM Academic Initiative and our IBM Skills Academy program, please visit: research.ibm.com/university",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/37MMRzM-2i5PxWxRBCCaLnNfUkfDh4vG,
IBM Cloud,Journey to Cloud: Envisioning your Solution,19,Module 3 - Deploy a Pilot Application in IBM Code Engine,2,Milestone 1: Build & Deploy a Pilot Cloud App,text,"Build and Deploy the App as our minimal viable product   

Introduction

In this topic, we'll show you how to log in to your IBM Cloud account using IBM Cloud CLI. We will then clone a GitHub repository and deploy our own app to it. Finally, we will test our app using simulated traffic.  

After participating in the Design Thinking workshops, the decision was made to begin migrating the existing Acme Airlines passenger app from their on-premises environment to the cloud. As the first step in this process, Thomas - Acme's IT Application Developer, is brought in to create the MVP as a limited test pilot.

Thomas recommends using IBM Cloud Code Engine as a way of integrating the existing core application 'UI' & 'Destination' that will hook up to third-party APIs, which are listed as 'Car Rental V1' & 'Hotel V1' in the lab. 

We will start by cloning the git hub repository that is relevant to this lab. Then we will install the Code Engine plugin. This will all be done through the IBM Cloud CLI (Command-line interface). Then we will run our script, ""Build-and-deploy.sh"", which will create the ""Bee Travels"" project. Finally, we will simulate traffic on our application. This will allow us to see how and why applications scale when more users are accessing them. 

Image of App developer working on his laptop
This code pattern introduces you to Code Engine and shows how to deploy a polyglot microservice travel application to the managed serverless platform. The travel application used in this code pattern is a part of the Bee Travels project that focuses on some of the first version services of the application. The services included in this code pattern are:  

Destination v1 (Node.js)
Car Rental v1 (Node.js)
Hotel v1 (Python)
UI (Node.js/React)
Below is the architecture diagram for v1 of the Bee Travels application: 

A diagram showing architecture of Bee travel application
IBM Cloud Code Engine is a managed serverless platform that can run both applications that serve HTTP requests which includes web applications or microservices as well as run batch jobs that run once in order to complete a task. These workloads are run within the same Kubernetes infrastructure and take advantage of open source technology including Knative and Istio. Knative is used to manage the serverless aspect of hosting applications, which includes auto-scaling of them based on incoming load - including down to zero when they are idle. Istio is used for routing and traffic management of applications. In addition, Code Engine is integrated with LogDNA to allow for logging of your applications. As a developer, the benefit of using Code Engine is that this Kubernetes infrastructure and cluster complexity is invisible to you. No Kubernetes training is needed and developers can just focus on their code. 

Architecture

A diagram showing Code engine beung used in Bee travel application
The Code Engine build feature clones the Github repo and builds the container images for the different Bee Travels microservices in the repo based on the provided Dockerfiles.
The newly built container images get pushed to repos on the provided image registry which in this case is Dockerhub.
Code Engine applications are created for the Bee Travels microservices from the newly built container images on Dockerhub.
The IBM load generation tool generates traffic to the Bee Travels application running in Code Engine. The auto-scaling component of Code Engine adjusts the number of running instances of the application based on the amount of incoming traffic.
Steps 

Prerequisites
Clone the repo
Build and Deploy the Code Engine 
1. Prerequisites
 To follow the steps in this code pattern, you need the following:

IBM Cloud account
IBM Cloud CLI
Dockerhub account
2. Clone the repo 
We will need to clone, or download a copy, of the github repository. This repository will have everything we need for the Lab including the Docker files, JSON packages, APIs, and other source files that we will need in order to access the data needed to build and deploy the Bee Travels Project.   

Image of App developer working with cloud developer
Clone the code-engine-microservices repo locally. In a terminal window, run: 

$ git clone https://github.com/IBM/code-engine-microservices
$ cd code-engine-microservices

From a terminal window, log in to your IBM Cloud account using the CLI command: ibmcloud login
Verify you are targeting the correct region, account, resource group, org, and space by running ibmcloud target. To set any of these to new targets, add -h to the command to view the necessary flags for changing the targets.
Install the IBM Cloud Code Engine plug-in for the IBM Cloud CLI by running: ibmcloud plugin install code-engine
Verify the plug-in is installed by running ibmcloud plugin list and seeing code-engine/ce in the list of plug-ins.

3. Build and Deploy the Code Engine  
In order to build and deploy the code engine, we must run the script ""build-and-deploy.sh"" This script will create the project we need, as well as use your input from the IBM Cloud CLI to build the microservices that we will use to interact with that project. After its all created, the script will deploy, which means to place the code engine onto IBM Cloud so that we can view and interact with it. 

Image of App developer - Thomas
1. Open build-and-deploy.sh and let's look at lines 9-31 of the script:

ibmcloud ce project create -n ""Bee Travels""
Line 9 creates a Code Engine project named Bee Travels. A project is a grouping of Code Engine applications and jobs. Creating a project allows for network isolation, sharing of resources (ex. secrets), and grouping together applications and jobs that are related. 

id=$(ibmcloud ce proj current | grep ""Kubectl Context:"" | awk '{print $3}')
Line 10 is responsible for getting the unique ID of the project. Each project has an associated unique ID that is used as part of the endpoints defined for the apps within that project. This is needed for getting the URLs of applications for internal traffic to the project which will be shown later. 

ibmcloud ce registry create -n ""${DOCKERHUB_NAME}-dockerhub"" 
-u $DOCKERHUB_NAME -p $DOCKERHUB_PASS -s https://index.docker.io/v1/ 
Line 11 creates an image registry access secret. For the purpose of this code pattern, the registry we will be working with is Dockerhub.

-n names the image registry access secret
-u specifies the username to access the registry server
-p specifies the password to access the registry server
-s is the URL of the registry server
ibmcloud ce build create -n destination-v1-build -i
${DOCKERHUB_NAME}/destination-v1:latest --src 
https://github.com/IBM/code-engine-microservices --rs 
""${DOCKERHUB_NAME}-dockerhub"" --cdr src/services/destination-v1 --sz small
Line 14 creates a build configuration that will turn the source code from Github into runnable container images for applications in Code Engine. Lines 19, 24, and 29 of the shell script are similar in which they specify the build configurations for the hotel, car rental, and ui microservices.

-n names the build
-i points to where the built container image will be pushed to
--src points to the Github repo where the source code is
--rs the name of the image registry access secret to be used
--cdr specifies the directory in the Github repo where the Dockerfile to be used is
--sz specifies the size for the build which determines the amount of resources used. This is optional and the default value is medium.
ibmcloud ce buildrun submit -b destination-v1-build -n destination-
v1-buildrun -w
Line 15 starts and runs the build configuration that was created on the previous line. Lines 20, 25, and 30 of the shell script are similar in which the start and run the build configurations for the hotel, car rental, and ui microservices.

-b the name of the build configuration to run the build
-n names the build run
-w waits for the build run to complete before moving on to the next line of the shell script. This is optional.
ibmcloud ce app create -n destination-v1 -i ${DOCKERHUB_NAME}/
destination-v1:latest --cl -p 9001 --min 1 --cpu 0.25 -m 0.5G -e 
LOG_LEVEL=info
Line 16 creates an application in our Code Engine project for our destination microservice. An application in Code Engine runs your code to serve HTTP requests with the number of running instances automatically scaled up or down. 

Lines 21 and 26 of the shell script for creating the hotel and car rental microservices are similar because all three services are backend services of the Bee Travels application and do not need external traffic.

-n names the application
-i points to the container image reference
--cl specifies that the application will only have a private endpoint and no exposure to external traffic. This can be used by backend services that do not need exposure to outside traffic and only communicate between other services of an application. By not exposing applications that don't need external exposure, may also save potential security risks
-p specifies the listening port. This only needs to be set when the port used by the application is not the default 8080
--min specifies the minimum number of instances of the application running. The default value is 0
--cpu specifies the amount of CPU resources for each instance
-m specifies the amount of memory resources for each instance
-e is used for each environment variable used by the application
ibmcloud ce app create -n ui -i ${DOCKERHUB_NAME}/ui:latest -p 
9000 --min 1 --cpu 0.25 -m 0.5G -e NODE_ENV=production -e
DESTINATION_URL=http://destination-v1.${id}.svc.cluster.local -e 
HOTEL_URL=http://hotel-v1.${id}.svc.cluster.local -e 
CAR_URL=http://carrental-v1.${id}.svc.cluster.local
Line 31 creates an application in our Code Engine project for the UI microservice. This is the microservice that users will interact with and therefore requires external traffic. Notice how this command does not have the --cl flag. The removal of this flag allows for external traffic and a URL to be generated for the application. The URL is secured automatically. In addition, some of the environment variables for this microservice specify the URLs to communicate with the other microservices. Since the other microservices use internal traffic, Code Engine uses the format <APP_NAME>.<ID>.svc.cluster.local as the entrypoint to an application. APP_NAME for each application is already defined in each ibmcloud ce app create command and ID was gotten from one of the previous commands in this script. 

Notice the minimum number of instances for each application of Bee Travels is set to 1: --min 1. This is due to the fact that we want Bee Travels to always be readily available for traffic without delay and need an instance to be initialized via cold start. Use cases for using the default value of 0 for the minimum number of instances for each application include:

Application does not receive a high volume of traffic consistently
Cold start delays are not a concern 
Interested in conserving resources and costs
For more details and documentation on the Code Engine CLI, go here. 

2. From a terminal window, run the build-and-deploy.sh script to deploy the Bee Travels application to IBM Cloud Code Engine. Provide your Dockerhub username and password when prompted to from the script.

$ cd code-engine-microservices
$ ./build-and-deploy.sh
Dockerhub Username:
Dockerhub Password:
Now, we will run the build-and-deploy.sh script, but first we will be prompted to log into our Dockerhub account we created earlier. This script will create our ""Bee Travels"" project for us. 


Congratulations, you have just successfully ran a shell script using IBM Code Engine. This was all done locally through the IBM Command Line Interface and Dockerhub.

3. When the script finishes, you will notice a URL in the terminal window that has the following format: https://ui.<NAMESPACE>.<REGION>.codeengine.appdomain.cloud This is the entrypoint to the Bee Travels application on Code Engine. Open this URL in a browser to view and interact with the Bee Travels application. 


Summary

1

1
Using IBM Cloud CLI, we cloned the code-engine-microservices repository and built container images for the different microservices provided in the Dockerfiles.

2

2
We pushed those newly created container images onto Dockerhub. Those images were used to create Code Engine Applications.

Next 

In the next milestone, we will use IBM's load generating tool to simulate traffic on our application. This will help to demonstrate the scalability of our app.

Sources:
[1] IBM Cloud. IBM Code Engine. <website> Accessed on November 9, 2021. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/7pz5M0ZYgG6rRRHn1nZz_9_n7zuYv20W,
IBM Security,Getting Started with Threat Intelligence and Huntin,16,Module 3 - Threat Map Worldwide View with IBM X-force Exchange ,3,Milestone 1: Analyzing Botnet Global Threat Reports,text,"
Find Threat Reports containing active ransomware

Introduction

In this Milestone, we will use X-Force Exchange to locate Botnet reports that contain active ransomware.

Topics:

Gain an understanding of the topics covered in this Module by going through the simulated lab and reading material included in the section below: 

         Locating Active Ransomware

Locating Active Ransomware

Perform the steps captured below:


Summary

1

1
X-Force Exchange is constantly monitoring the cybersecurity landscape, making it a reliable way to locate and security issues.

2

2
Botnet reports are an important part of cyber security. Reports can help find infected malware and active ransomware within a network. 

Next Steps

We are now ready to continue to the next Milestone. Click the Continue button to go to the next page.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/wG3Rn0obAqzAb7j2HQBF1W5F7gsPmKL3,
IBM Automation,IBM Robotic Process Automation - Basic II,4,PDF,4,4 - PDF - use case I,video,,https://learn.ibm.com/mod/video/view.php?id=152673,
IBM Engineering,Quick Start Sessions,10,Quick starts: IBM Engineering Requirements Management DOORS Next,10,What is IBM Engineering Requirements Management DOORS Next?,text,"Requirements management consists of documenting requirements and their supporting information in an organized manner. In this way, requirements can be analyzed, interpreted, and referenced in context. This tutorial introduces IBM Engineering Requirements Management DOORS Next and how to use it to collaborate with a team as you capture, elaborate, and trace requirements across the development lifecycle.

In this simple scenario you will respond to a change request (in Engineering Workflow Management) for an existing project, a UAV (Drone) system called the Aviary, which consists of several systems including the drone itself, code-named the Hummingbird. You will see how DOORS Next supports teams to realize the fundamental principles of requirements management such as granularity, organization and collaboration across the lifecycle of a project.

In the lab, you get to:

Browse, search and filter requirements
Modify existing and create new requirements
Analyze and add traceability between requirements
Use changesets to control change in the requirements
Collaborate with other stakeholders through linked work items and conversation threads
View the history and audit trail
Compare differences when changes are made
The estimated time to complete the lab is 2 hours.",https://learn.ibm.com/mod/page/view.php?id=165928,
Artificial Intelligence,Machine Learning for Dummies,1,Understanding Machine Learning,1,Understanding Machine Learning,text,,https://www.ibm.com/downloads/cas/GB8ZMQZ3,
IBM Automation,IBM Robotic Process Automation - Basic I,6,"Publishing, Repository and Revision Control",5,"Publishing, Repository and Revision Control",video,,https://learn.ibm.com/mod/video/view.php?id=152633,
IBM Cloud,Journey to Cloud: Envisioning your Solution,10,Module 1 - Digital Transformation with Cloud Computing,1,Quiz,text,,https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/gL1LddeW-XxNMrWcnAryO55NGGG-Sdgs,
IBM Automation,IBM Robotic Process Automation - Basic I,18,Advanced Commands,17,Use case II - Stack,video,,https://learn.ibm.com/mod/video/view.php?id=152655,
IBM Security,Getting Started with Threat Intelligence and Huntin,11,Module 2 - Threat Hunting,2,Topic 1: Cyber Resilience,text,"
The defender's approach to an ever-present cyber security threat

Introduction

What is cyber resilience and why is developing a strong resilience framework necessary in fighting cyber-crime? 

Gain an understanding of the concepts covered in this lecture, by going through the videos, reading material and, slides included in the sections below:

What is Cyber Resilience?
Why are cyber threat management teams under pressure?
Why are organizations unable to cope with the threat?
What are the key considerations to building a cyber defense strategy?
What is the Cyber Resilience Lifecycle?
1. What is Cyber Resilience?

Resilience is defined as the ability of an ecosystem to return to its original state after being disturbed. Cyber resilience - on the other hand - is an organization’s ability to continue delivering the intended outcomes despite adverse cyber incidents. 

Let's explore some top industry players as they define Cyber Resilience

Cyber Resilience is the ability to anticipate, withstand, recover from and adapt to adverse conditions, stresses, attacks, or compromises on cyber resources
- MITRE Corp

Cyber-resilience is the organization's capability to withstand negative impacts due to known, predictable, unknown, unpredictable, uncertain, and unexpected threats from activities in cyberspace
- Information Security Forum

Cyber-resilience is defined as the ability of systems and organizations to withstand cyber events, measured by the combination of mean time to failure and mean time to recovery
- Word Economic Forum

Why is cyber resilience needed? 

Organizations today face a growing threat of cyber attacks. Cyber attacks are no longer a question of “if” but “when”. The recent global cyber-attacks underline the critical importance of having a proactive and integrated approach towards cyber resilience, bringing together information security, business continuity, and organizational resilience because the cost of a data breach is huge. Breaches can result in reputation damage and regulatory action. 

Quick recovery from breaches can significantly reduce the business impact and addressing the quick recovery needs of applications and infrastructure to meet the availability requirement is challenging. Current DR/backup copies are vulnerable to corruption and are inadequate to handle a cyberattack.

The chart below describes the Top 3 causes of cyber disruptions. The consolidated average per capita cost is $148 USD.

48% Malicious or criminal attack
25% System glitch
27% Human error
The bar graph illustrates the top 3 causes of cyber distruptions, per capita cost for each data breach, with malicious or criminal attack being the main reason, system glitch the second and human error the least
 Per capita cost for three root causes of the data breach. 

Attacks are trying lots of new vectors to infiltrate your organization. 

A Ponemon Study of data breaches found that 61% were the result of phishing and social engineering. Interestingly, only 21% were the result of out-of-date software. 

Many organizations believe that keeping software up to date is the key to prevention, but attacks come from a number of vectors. A study of top executives found that 68% reported they lack the ability to remain resilient in a cyberattack, 66% are not sufficiently prepared, and 75% do not have a comprehensive cyber security incident response plan. 

2. Why are cyber threat management teams under pressure?

New business initiatives, such as digital transformation and cloud adoption are moving at an unprecedented pace, so traditional approaches to cyber threat management can’t keep up.

Complexity is on the rise as the asset landscape changes rapidly, including Cloud, SaaS, BYOD, and IoT.  

This, compounded with the fact that the threat landscape is evolving daily, with advanced threats driving new attack vectors, and motivations, is causing traditional threat management teams to struggle to address these new threat levels.

Organizations are under heavy public scrutiny, they need to protect clients' data, and adhere to growing regulatory emphasis on security and data privacy; data breaches can impact brand reputation, raising the stakes for all organizations. 

Traditional threat management lacks the visibility and responsiveness to contain these risks. 

Let's evaluate the three main factors that contribute to the previous statement:

Cloud and SaaS have accelerated the pace at which new assets can be adopted (and threats arise).
Cloud and SaaS have accelerated the pace at which new assets can be adopted (and threats arise).

Advanced threats are multi-faceted, many times manipulating a complement of vulnerabilities with precision, speed, and great damage.
Advanced threats are multi-faceted, many times manipulating a complement of vulnerabilities with precision, speed, and great damage.

Financial losses garnered by a breach could be catastrophic.
Financial losses garnered by a breach could be catastrophic.

Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


3. Why are organizations unable to cope with the threat?

With the additional burden of monitoring cyber-attacks and preparing for additional recovery scenarios, it is difficult to keep up.

Four main reasons why organizations are currently overwhelmed 

Numbered divider 1
Data Overload

Analysts are only able to keep up with about 8% of the information needed to do their jobs

The slide explains the data overload experienced by analysts. On the right is a pictogram of a data analyst
“I don’t know where to focus my time for the quickest response.”

Numbered divider 2
Unaddressed Threats

The slide explains the unadressed cyber threats experienced by SOC managers and security professionals
“There is so much information out there, it’s impossible to find what’s useful.”

Numbered divider 3
Cyber Skills Shortage

Coupled with the fact that industry-wide there are expected to be 1.8 million cyber jobs that will be unfilled by 2050. 

The slide predicts the possible number of jobs unfulfilled by 2022 due to cyber threats
“Workload is overwhelming and repetitive. and we cannot retain talent nor find the manpower we need.”

Numbered divider 4
Complexity

Operations teams have a very challenging mission. They need to process all the intelligence to stay ahead of the cyber-criminals, protect critical data, users, applications, and infrastructure, and be prepared to respond and recover quickly from incidents.

Process intelligence and out-think cyber-criminals 
Process intelligence and out-think cyber-criminals

Protect critical data, user applications and infrastructure
Protect critical data, user applications and infrastructure

Respond and recover from incidents quickly
Respond and recover
from incidents quickly

Pain points evolve as cyber-attacks increase and change 

What are the main pain points?

Many companies have business continuity/disaster recovery plans but are often not suited to handle destructive cyber-attacks.

Networks that are 5+ years old and hardware/appliance-based are seldom resilient. Companies affected by cyber attacks spend up to $3.6 million on recovery and breach notification, which they could have avoided by spending a portion of their budget on preparation and planning. 

There is a need for a more precise, immediate response to cyber events. Let's review the main pain points that organizations are facing as cyber-attacks increase.  

The slide explains the change in user pain points due to increasing and evolving cyber attacks
The slide explains the change in user pain points due to increasing and evolving cyber attacks
The slide explains the change in user pain points due to increasing and evolving cyber attacks
The slide explains the change in user pain points due to increasing and evolving cyber attacks
4. What are the key considerations to building a cyber defense strategy?

Cyber resilience is not just a security problem. It is not just an IT problem all areas of the business need to be involved and all the key disciplines need to be prepared to work together, share information, and seamlessly share information in real-time to address cyber threats.

Unlike traditional outages, cyber attacks can be targeted attacks that exfiltrate IP from Research & Development, Finance, Production, and other departments.

Important questions to build a cyber defense strategy

Explore the following key questions that all security strategy leaders should ask, and evaluate the proposed considerations:

Key questions and considerations to build a cyber defense including IT infrastructure, networks outdated and the approach of using manual operations if systems go down
Cyber resilience serves a number of IT and risk management disciplines

Cyber Resilience is a business priority that supports “continuous availability” that allows companies to meet their business outcome objectives.

The mindmap illustrates how cyber resilience serve a number of IT and risk management disciplines
Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


5. What is the IBM Cyber Resilience Lifecycle?

As computing technologies have progressed over time, so have the complexities that need to be managed. To solve this expectation and complexity, AI, predictive systems, and orchestration as part of a software-defined resiliency story will be critical.

The evolution of Cyber Resilience

In the first wave, the trend was to have data recovered from weeks with infrastructure recovery in the data center. Typically these were singular centralized systems. 

The expectations in wave 2 for data recovery were reduced to days and cloud began to be a part of the story. 

In wave 3 expectations were that the recovery of the lost data was in the range of hours to minutes using more hybrid environments. 

The trend going forward is an expectation of always-on systems with an expectation of zero downtime which is a major challenge since our systems are now more heterogeneous than ever.

The picture below depicts the evolution timeline for disaster recovery practices

The chart illustrates the importance of cyber resilience as a business priority that supports ""continuous availability"" that allows companies to meet their business outcome objectives
The future is AI and Orchestration

By proactively identifying threats you can predict emerging threats and risks, by modeling behaviors you can gain actionable insights so you can triage and respond quickly with improved accuracy, then you can respond quickly with confidence leveraging orchestration to generate a complete and dynamic response, enabling faster, more intelligent remediation

Cyber Resilience Lifecycle

Based on the NIST cyber security framework, the IBM cyber resilience lifecycle enables organizations to improve their cyber robustness across the five phases of the lifecycle.

Below are the five phases of the Cyber Resilience Lifecycle:

Identify – is about preparing a plan

Protect – discover weaknesses before attackers can

Detect – find unknown threats with advanced analytics and stop attacks before they become entrenched

Respond – coordinate your response so everyone is on the same page and all tasks are completed

Recover – get back up and running quickly and efficiently

Use the interactive activity below to explore each phase and learn more about which tools, methods, and approaches apply to each one of them.









The IBM Cyber Resilience Lifecycle is based on NIST (National Institute of Standards and Technology) Cybersecurity framework

Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


Summary

1

1
As cyber-attacks continue to raise the best defense is to adopt the capacity to ensure your organization can ""survive"" the aftermath of a cyber attack.

2

2
Many companies have business continuity/disaster recovery plans but are often not suited to handle destructive cyber-attacks, they should invest in renewing their infrastructure before is too late.

3

3
Operations teams need to process all the intelligence to stay ahead of the cyber-criminals, protect critical data, users, applications, and infrastructure, and be prepared to respond and recover quickly from incidents.

4

4
To achieve Cyber Resilience organizations can follow the Cyber Resilience Lifecycle, adopting an AI and Orchestration approach to security.

Next 

Threat Hunting - what is it and why is important?

Sources:

1. European Union Agency for Network and Information Security (ENISA)
2. Federal Insider Threat Report IBM X-Force Threat Intelligence 
3. IBM X-Force IRIS Research 
4. IBM X-Force Threat Intelligence Index 
5. Cost of a Data Breach Study - Ponemon Institute
6. National Institute of Standards and Technology Cybersecurity Framework - NIST.GOV/FRAMEWORK",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/KlyiBzf_5c3gnQjNnVl9ziwK_4S1W0rs,
IBM Engineering,Quick Start Sessions,7,Quick Starts: IBM Engineering Test Management,7,What is Engineering Test Management (ETM)?,text,"IBM Engineering Test Management (ETM) is a collaborative hub for business-driven software and systems quality across virtually any platform and type of testing. This software helps teams share information seamlessly that uses automation to accelerate project schedules and report on metrics for informed release decisions.

ETM helps quality assurance teams:

Collaborate: share project information and status updates seamlessly so team members can synchronize teamwork though out the project lifecycle.
Automate: reduce labor-intensive activities to accelerate project schedules.
Govern: understand and report on project metrics to enable accurate, reliable, and timely release decisions.
In the labs, you explore some of the basic ETM features and its automation. You get to:

Create a test plan and use the automations to create test cases directly from linked requirements.
Create a manual test script automatically from the description of a test case.
Execute that script and, when it fails, you see how ETM can raise defects in the planning system (in this case IBM Engineering Workflow Management) and automatically link to all the relevant artifacts in ETM.

The estimated time to complete the labs is 2 hours.",https://learn.ibm.com/mod/page/view.php?id=165787,
IBM Automation,IBM Robotic Process Automation - Basic I,17,Advanced Commands,16,Use case I - List,video,,https://learn.ibm.com/mod/video/view.php?id=152654,
IBM Security,Getting Started with Threat Intelligence and Huntin,18,Module 3 - Threat Map Worldwide View with IBM X-force Exchange ,3,Summary,text,"Reflecting on what we learned from this lecture and planning the work ahead.

Summary

1

1
Understand how Botnet reports work, which helps identify hacked computers and can stop attackers from getting access to the device.

2

2
Locate active ransomware, enabling security professionals to quickly monitor the current cybersecurity landscape. 

3

3
Explore X-Force Exchange and see how beneficial it is for Cybersecurity.

4

4
Utilize threat activity map to view where attacks are in real-time.

5

5
Follow the attacks, helping cyber security specialists monitor threats by being notified if anything changes.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/ala54_kfIC0suuZiKtW_VB0yu6A1Ltam,
IBM Automation,IBM Robotic Process Automation - Basic I,15,Advanced Commands,14,File Manipulation III,video,,https://learn.ibm.com/mod/video/view.php?id=152650,
IBM Cloud,Journey to Cloud: Envisioning your Solution,15,Module 2 - Cloud Adoption Journey : Ideation Practices,2,Summary & Resources,text,"Module 2

Reflecting on what we learned from this lecture and planning the work ahead 

Summary

1

1
The IBM Garage Method can be divided into 3 broad phases: 

Think - Brainstorm to develop big ideas, define personas, empathize with target end-users, develop proofs of concept, conduct user research, develop high-level business cases, and create a backlog of prioritized ideas.
Transform - Iteratively build Minimal Viable Products (MVPs) using Agile practices.
Thrive - Focus on scaling in multiple dimensions. Initial MVPs which are well-received are iterated on in subsequent MVPs, adding and testing more features.
2

2
Enterprise Design Thinking is a user-focused approach to innovate and establish brand differentiation that focuses on user-centric outcomes. 

3

3
Ideate and cluster opportunities - ideation is the process of generating big ideas. Enterprise Design Thinking explains big ideas by contrasting them with features.

4

4
Frame your business opportunity - the goal of the Business Framing Exercise is to align on the identification, prioritization, selection, and definition of business opportunities before creating a prototype for our MVP. 

Resources

1. Featured Resources

The following resources were used to develop the material included in this module: 

Image showing cover page of Enterprise Digital Transformation with Cloud on IBM research paper
IBM Research Paper 

Enterprise Digital Transformation with Cloud  


Enterprise Digital Transformation with Cloud.pdf
189.9 KB
2. Additional Resources

The following resources are complementary material to those provided in this lecture.

IMPORTANT: You don't need to access the material included below to complete this course, but just in case you are eager to go beyond the boundaries of the content included in this course, below are some additional resources which you can pursue to learn more.

The URLs included above are pointing to resources hosted outside our domain and beyond our control. Therefore that material is provided as-is and no support will be given if the links are removed from public access by the content owners.

List of resources

1) Explore the Garage Methodology  

https://www.ibm.com/cloud/architecture/content/course/explore-garage-methodology

2) Learn the Design Thinking Framework 

https://www.ibm.com/design/thinking/page/framework",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/zMBEuIWPu49Oos8zJ0rcQ8h9os9j6Qzm,
Data Science,Getting Started with Enterprise Data Science,21,Module 3 - Detecting Pattrns of Fraud wit Data Analytics,3,About this Module,text,"Module 3

Detecting Patterns of Fraud with Data Analytics

Introduction

Throughout this Module, you will follow in the footsteps of Sally, a Data Analyst, as she works with her team and Watson Studio; to collect and prepare insurance data.

IBM Watson Studio empowers you to scale analysis across your organization to speed development time and simplify collaboration with data scientists, risk analysts, investigators, and other subject matter experts while adhering to strong governance and security posture. In order to respond to new types of fraud, waste, and abuse while minimizing false negatives and accelerating response, the platform continuously accommodates real-time data, monitors and detects fraudulent activities, and adapts as the patterns change and spot anomalies.

This page covers the following Module details:

Duration
Objectives
Milestones
Modality
Duration

This Module can be completed on average in 60 minutes

Objectives

In this Module you will learn the following concepts:

1

1
Create a Watson Studio Project

2

2
Upload and Refine Data

3

3
Data Visualization

Milestones

This module is divided into he following milestones:


Case Study - Episode 1
+

Milestone 1: Data Collection
+

Milestone 2: Data Preparation
+

Milestone 3: Finding fraud patterns with Data Representation
+
Modality

Simulation - This hands-on activity doesn't require access to an external site, special account, or software download, all the activities are performed within the course environment as a click-through simulation. 

Accessing and completing this Module

In order to complete this Module activity, you need to follow the instructions on every Milestone and perform the activities therein.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/1iBch5lZEehgc4B8anQBvFWmevL6AiPA,
IBM Engineering,Quick Start Sessions,6,Quick Starts: Publishing using Document Builder,6,What is publishing using the Document Builder?,text,"IBM Engineering Lifecycle Optimization – Publishing (PUB), formerly called IBM Rational Publishing Engine (RPE) automates document generation from IBM solutions and select third-party tools. You can use PUB to automate the generation of documents for ad hoc use, formal reviews, contractual obligations or regulatory compliance.

PUB provides:
Documents and reports: Generate high-quality documents with flexible formatting as well as composite reports containing data from multiple sources. Reports may be scheduled or invoked on demand.
Outputs: Support multiple output formats and concurrent document generation to multiple target formats from a single template.
In the labs, you get to:

Report on modules in ERM with Document Builder
Report on modules in RMM with Document Builder
The estimated time to complete the labs is 2 hours.",https://learn.ibm.com/mod/page/view.php?id=165981,
IBM Automation,IBM Robotic Process Automation - Basic I,2,IBM RPA main components,1,IBM RPA main components,text,"Learn the main components of IBM Robotic Process Automation (RPA), understanding their purpose and how they work.

IBM RPA main components diagram, where window client components connect to the IBM RPA server.
For more information, about the main components of IBM RPA and how they relate to each other, see the Architecture.",https://learn.ibm.com/mod/hvp/view.php?id=200088,
IBM Security,Getting Started with Threat Intelligence and Huntin,2,Course Overview,0,Release Notes,text,"This section provides an exhaustive description of all the changes to the educational content and structure of this course.

The documented history of changes started on 4th November 2021.  Last update 11th February 2022.

Note: The history of changes appears in chronological order, where the latest changes will appear first and reflect the differences between the previous release of the course.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/icUcWoJqhVVatKCDdzOZNE0p9cCX0PDm,
Artificial Intelligence,IBM AI Education,1,IBM AI Education,1,Prepare Your Students for Their Future Careers,text,"IBM AI Education
Free On-Demand Webinars for K-12 Educators
bg-program-stem-texture@2x.jpg
Prepare Your Students for Their Future Careers
Artificial Intelligence (AI) is all around us, from self-driving cars, digital assistants, rideshare apps, banking apps, and so much more. Experts say AI will change 100% of jobs over the next 10 years, but there is a fear that the next generation isn’t prepared for the shift to AI. 
It's imperative for teachers to learn how to infuse their content and curriculum with the knowledge, skills, and values driving innovation in AI today so that their students are prepared to be successful in the modern workforce, regardless of their career paths.

IBM AI Education is an immersive, online professional learning suite of FREE live and on-demand webinars, crafted by and for educators.  The nine webinars will guide educators through AI’s foundational concepts and K-12 classroom connections, with topics including introduction to AI, natural language processing, ethics, robotics, and more.

Be the first to know about new AI Webinars for Educators
KEEP ME UPDATED
account_tree
Learn about AI’s foundational concepts and K-12 classroom connections.
track_changes
Gain ideas on how to incorporate AI fundamentals into your curriculum.
verified
Earn the IBM AI Education badge when you attend all nine webinars. 
RESOURCES
Why Artificial Intelligence Belongs in ALL K-12 Classrooms
Why Artificial Intelligence Belongs in ALL K-12 Classrooms
Artificial Intelligence, or AI, has been a popular topic and some might even say an education buzzword, but many educators still don’t see the need for AI in their specific classrooms. We’re debunking common myths or roadblocks and giving every K-12 teacher, regardless of subject area or grade level, the top five reasons as to why AI should be on your radar!

Oct 16, 2020
Webinar Topics
Website_OnDemand_IntroToAI.png
Creating a Spark for Artificial Intelligence
Dive into this introductory course and get familiar with the basics of AI. Learn and define five key terms (AI, neural network, big data, algorithm and machine learning) through analogy, interactive games and activities. Build tools to use with your own students to expose them to AI concepts, and hear from an IBM expert about the importance of play in education.

Website_OnDemand_ComputationalThinking.png
AI Here, AI There ... AI Who, What, Where?
Learn how to teach your students to solve real-life problems using AI. Get access to AI teaching tools and resources, such as a sorting activity that activates prior knowledge about AI, an engaging game that teaches computational thinking, and a curated collection of videos that illustrate how AI affects future jobs, shapes social media experiences, helps keep communities safe, and more.

Website_OnDemand_Ethics.png
Ethics in AI – Can You Crack the Moral Code? 
How do ethics come into play in AI? The moral dilemmas in AI are complex, but the issues are critical, generating material for educators across disciplines to initiate authentic and relevant discussions with their students. Hear from an IBM AI expert on the breadth and depth of this topic and walk away with resources and activities to exemplify and frame these conversations.

Website_OnDemand_Robotics.png
Amplifying Human Potential - Beyond Programming and Coding
Address a common fear: Will robots take my job? To answer this question, you will understand where human discomfort with robots comes from, examine the difference between a robot and an artificially intelligent robot, and see robots that help humankind in action. Learn how students can build their own artificially intelligent robot and how to facilitate student creativity with IBM’s TJBot.

Website_OnDemand_DesignThinking.png
AI Can and Should Be Inclusive – Design Thinking Can Help!
Grow your understanding of AI through the lens of equity-centered design thinking, a creative problem-solving process. Familiarize yourself with IBM’s Design Thinking Field Guide activities including empathy mapping and storyboarding, and create a prompt for your students. Examine bias in AI and develop ways to mitigate these preconceived notions. Take this back to the classroom with tools to encourage students to examine and address their own biases.

Website_OnDemand_DiversityAndInclusion.png
Building a Culture of Inclusivity
Examine your own tech experience as you explore the roles of gender, race, and bias in technology and STEM fields. Grow your toolkit with resources for diversifying your own computer science and STEM programs so that your students can gain awareness of how AI affects them, and join the fight against algorithmic bias and injustice.

Website_OnDemand_NLP.png
How Do Siri, Alexa, and Watson Understand Me?
Walk a mile in a machine’s shoes and experience the complexity of human language - and what it takes for a computer to understand us! Like art, AI imitates life - and can hold up a mirror to reveal patterns in human speech. See examples of how NLP can be used for social good - get students thinking about other applications of this powerful technology.

Website_OnDemand_Careers.png
AI and Your Curriculum - The Perfect Marriage
How can we prepare students for a future we can’t see, when our world is being constantly changed by the advancement of technology? In this course you will understand the effect AI is having on the future of work, and how the skills needed to be successful in the future have changed. Practice and identify these skills with a Scratch project via IBM’s “Machine Learning for Kids” and collaborate using pair programming. Build these employability skills in students to prepare them for anything.

Website_OnDemand_ML.png
Machines Can Learn – But They Need Our Help!
Can a piece of paper be intelligent? Answer this question as part of a larger discussion around intelligence, then play the role of a computer to understand how a machine learns. Learn more specific subsets of machine learning by comparing them to how students learn in the classroom, and take away activities to share these concepts with your students.

IBM AI Foundations for Educators Badge
Attendees of all nine online webinar topics (live or on-demand) will have the opportunity to take a certification quiz and earn their IBM AI Foundations for Educators Badge. This badge certifies that the recipient has become fluent and knowledgeable in Artificial Intelligence (AI). Badge recipients will have an understanding of AI applications such as machine learning and natural language processing, along with how it is used to solve problems, collect data and identify bias. They will also have become equipped to share the foundational knowledge of artificial intelligence with their colleagues and students in their classrooms.

Ready to earn your badge? If you’ve completed all nine webinars, click “Access Badges” to learn more about how to access your badge quiz.",https://www.mindspark.org/ibm-ai,
IBM Engineering,Quick Start Sessions,8,Quick Starts: IBM Global Configuration Management,8,What is IBM Global Configuration Management?,text,"Configuration management is a well known practice in software engineering, improving efficiency and reliability of the development process as well as reducing risk and costs through more efficient change management. In this Quick Start, you see how the IBM Engineering Lifecycle Management platform provides change and configuration management for all of the engineering lifecycle artifacts. You see how the platform provides these capabilities locally (for example in requirements management) but also globally, providing the capability to define, document, compare and analyze product variants.",https://learn.ibm.com/mod/page/view.php?id=177082,
IBM Engineering,Quick Start Sessions,12,Quick Starts: Publishing for IBM Engineering Test Management,12,What is IBM Engineering Test Management Publishing?,text,"IBM Engineering Lifecycle Optimization – Publishing (PUB) automates document generation from IBM Engineering Lifecycle Management (ELM) solutions and select third-party tools. PUB may be used to automate the generation of documents for ad hoc use, formal reviews, contractual obligations or regulatory compliance.

PUB provides:

Documents and reports: Generate high-quality documents with flexible formatting as well as composite reports containing data from multiple sources. Reports may be scheduled or invoked on demand.
Outputs: Support multiple output formats and concurrent document generation to multiple target formats from a single template.
Template editor: A graphical template editing environment for custom report design.
Data sources: Extract data from a single source or combine data from multiple sources for cross-domain reporting
PUB consists of the following applications:

PUB Launcher – a standalone application for generating reports using predefined templates
Engineering Document Generation (EDG) – wizard-based document generation from within the tools themselves (for example from within Rhapsody)
PUB Document Studio – A graphical environment for designing and testing templates
PUB Document Builder – A web-based application that allows dynamic creation of reports by assembling templates, as well as automated, scheduled document generation
In the labs, you get to:

Design document  templates for ETM with PUB
Report on linked ELM artifacts
The estimated time to complete the labs is 2 hours.",https://learn.ibm.com/mod/page/view.php?id=166099,
IBM Cloud,Journey to Cloud: Envisioning your Solution,17,Module 3 - Deploy a Pilot Application in IBM Code Engine,2,About this Module,text,"Module 3

Deploy a Pilot Application in IBM Code Engine  

Introduction

Based on insights about our end-user gathered from the design thinking exercises, we will develop our Minimal Viable Product - a pilot cloud application, to see how it performs after being migrated to IBM Code Engine.  

This page covers the following lecture details:

Duration
Objectives
Skills
Instructors
Topics
Duration

This module can be completed in an average of 240 minutes

Objectives

In this module you will learn the following concepts:

1

1
Understand the importance of creating a Minimal Viable Product (MVP). 

2

2
Generate a hypothesis to define an MVP solution. 

3

3
Deploy a pilot cloud application using IBM Code Engine.

4

4
Verify application deployment by generating user traffic. 

Skills

Upon completing this module you will have acquired the following skills: 

API

Application Deployment

Cloud Adoption  

Cloud Computing

Cloud Migration

Migrate a Pilot Cloud Application

Enterprise Design Thinking

IBM Code Engine

IBM Garage Methodology

Microservices

Minimal Viable Product

About the Instructors 

Meet the IBM subject-matter experts, who will guide you through the topics on this module.   

Image showing instructor JJ Asghar


JJ Asghar  

Developer Advocate
IBM





Biography



JJ works as a Developer Advocate representing the IBM Cloud all over the world. He mainly focuses on the IBM Kubernetes Service and OpenShift trying to make companies and users have a successful onboarding to the Cloud Native ecosystem. He’s also been known in the DevOps tooling ecosystem and generalized Linux communities. If he isn’t building automation to make his work streamlined he’s building the groundwork to do just that.  



Image showing instructor Andrea Crawford


Andrea Crawford 

Distinguished Engineer (DevOps)
IBM Garage for Cloud             





Biography



Andrea has more than 20 years of cross-industry experience in application development, architecture, and accelerated delivery. Andrea has made significant contributions to IBM’s own DevOps transformation and to clients’ DevOps journeys. Andrea provides technical oversight and leadership of technical enablement, solution design, and offering innovation for which focuses on accelerated application development with Agile, DevOps, and Cloud-Native Development. Andrea applies her expertise in accelerated development across all industries, heterogeneous technologies, and hybrid cloud scenarios.


Inclusiveness begets a diverse workforce and groundbreaking, innovating thinking and ideas. Andrea has a passion for supporting and enabling under-represented minorities in IT.



Image showing instructor Mihai crivati


Mihai Crivati  

CTO Cloud-Native and Red Hat Solutions 
IBM  





Biography



Mihai Criveti is the CTO for Cloud-Native and Red Hat Solutions at IBM. He is an STSM, TOGAF, and Red Hat Certified Architect, a member of the IBM AoT, and an advocate for Open-Source development. 


He helps clients increase their innovation, business agility, and accelerate time to market through Digital Transformation and Infrastructure Modernization. He develops solutions that help clients adopt DevSecOps, shift-left operations, SRE models – and automated operations across hybrid and multi-cloud environments that take advantage of Kubernetes, OpenShift, and modern development techniques.


Mihai leads the Ireland AoT affiliate (ITEC) and the OpenShift Solution Guidance initiative and is passionate about growing the technical community in Ireland, mentoring, coaching, and supporting academic initiatives.



Image showing instructor Tom Creamer


Tom Creamer     

OCP Technical Elite Team 
IBM





Biography



Tom is the OCP Technical Elite team and IBM Global Markets Americas CTO.  He is an IBM Distinguished Engineer, Open Group Distinguished IT Specialist, and IBM Master Inventor with over 190 patents granted.  Tom is devoted to the growth of the IBM Technical community and is the chairman of the IBM Global Markets Americas Distinguished Engineer Board.

Tom spent 12 years as the Technical Director and IBM CTO to Verizon and has 35 years with IBM working predominately in the Telecommunications industry.

Tom is a member of the IBM Telecom, Media & Entertainment CTO community, and the IBM Edge Computing and 5G Multi-access Edge Compute (MEC) architecture teams.



Image showing instructor Hardy Groeger


Hardy Groeger    

IBM Distinguished Engineer  
IBM





Biography



Hardy Groeger is an IBM Distinguished Engineer and the Director for Competitive Insights. He brings extensive technology and automotive industry experience into every engagement, based upon his roles as CTO Integrated Account Daimler, Lead Architect for Mobile Solutions, Collaboration Solutions, and Websphere / Websphere Portal. 

Hardy's technology journey started in IBM's software development labs in the US. He holds a Master of Science in Business Computing from the University of Paderborn, Germany.



Image showing instructor Dr. Thalia Hooker


Dr. Thalia Hooker    

Global OCP Elite Team
IBM 





Biography



Dr. Hooker is an innovative Thought Leader dedicated to customer success with over 20 years of experience in IT. She works across IBM and complex customer organizations with their technology strategy and recommends solutions that solve customer needs while bringing the right IBM capabilities to bear, whether software, hardware, and/or services. She helps drive OpenShift Container Platform (OCP) adoption and IBM Cloud Paks deployment on OCP in her current role. 
",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/ohnyRTDxC67wi0_OaZrMM7sCPTfP8-zc,
IBM Security,Getting Started with Threat Intelligence and Huntin,7,Module 1 - Threat Intelligence,1,Topic 4: Enterprise Security Domains,text,"Let's explore the different cybersecurity domains that need to be addressed within an enterprise.  

Introduction

In this topic, we will be exploring the framework for cybersecurity for the enterprise. 

Throughout this topic, we will attempt to answer the following questions: 

What are the enterprise security domains? 
1. What are the enterprise security domains? 

There is reason to hope that next year will shape up to be a better year. Trends are notoriously hard to predict, but the one constant thing we can rely on is change. Resilience in the face of rising and falling challenges in cybersecurity requires actionable intelligence and a strategic vision for the future of a more open, connected security. 

It’s not a complete integrated security domain until these capabilities can interact, communicate, and integrate with one another across your hybrid IT environments; Extending beyond your company walls across your entire ecosystem. 

Integrated Security Domains 

Security in a more organized fashion is structured around domains with security intelligence in the middle to make sense of threats using logs, data flows, packets, and different layers of defense start working together sending critical details and providing intelligence necessary to block threats. The collaboration across companies and competitors is critical to understand global threats and capture threat data, which we can use to adapt our security systems to defend our organizations from new threats.

The graphic below depicts the different domains included within the security enterprise framework.

Mindmap showing the components of Cloud and Security Intelligence
Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


Numbered divider 1
Network security 

Network security serves as the first line of defense for governments and organizations. They support our global economy and communications infrastructure in which our society relies on today, that's why they are the most common entry point for every cyber attack

Mindmap showing the components of Cloud and Security Intelligence highlighting Network
Targets:

Targeting from mobile devices and IoT unprotected WIFI
To complex mega server farms supporting our internet backbone.
Attacks:
DDoS, Misconfiguration, and Physical Access

Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


Numbered divider 2
Endpoint security 

Protecting the entry point for billions of users and things connected globally. There we stored our personal data and sense the events happening in real-time in the world we live in.

Mindmap showing the components of Cloud and Security Intelligence highlighting End point security
Targets:

Organizations – That rely on sensor data to drive logistics and operations 
People – Consumers and their personal data
Things – Planes, elevators, cars, homes
Attacks:
Physical access, Misconfiguration, Malvertising, Malware, Phishing

Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


Numbered divider 3
Application security 

Ensuring safe use and operation of all applications (mobile, web, backend). Applications rule all the access points to data and transactions required to interact with different systems.

Mindmap showing the components of Cloud and Security Intelligence highlighting Apps
Targets:

Involving the most sophisticated types of attacks 
Organizations using websites that provide online services 
Development teams creating in-company applications
Attacks:
Malware, SQLi, Watering Hole, Misconfiguration

Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


Numbered divider 4
Data Security, Identity, Access and Fraud  

Protecting the access and usage of Data is the most valuable digital asset today.  It contains confidential information that could be sold, or leveraged as intelligence to commit crimes and financial fraud.  

Mindmap showing the components of Cloud and Security Intelligence highlighting Data, Identity&access
Targets:

Large organizations that store valuable information: Financial institutions, hospitals, government agencies. 
Social media giants that store vast amounts of customer personal data 
Attacks:
Misconfigurations, Phishing, Ransomworms

Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


Numbered divider 5
Cloud security 

Safety cloud mechanisms integrate networks, applications, data and access. Cloud environments help organizations simplify and automate the integration between networks, endpoints, applications, data and establish identify validation and access gateways, and provide powerful management and visualization tools.  

Mindmap showing the components of Cloud and Security Intelligence highlighting Cloud
Targets:

Unskilled IT teams starting their cloud adoption journey 
Companies hosting valuable data in the cloud
Attacks:
Misconfiguration, Ransomware, Malware

Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


Numbered divider 6
Security Intelligence with A.I. 

Using Analytics and A.I. to respond in real-time to attacks finding patterns in thousands of concurrent incidents. Identifying high-risk threats in near real-time. Detecting vulnerabilities, managing risks, and identifying high-priority incidents among billions of data points. Gaining full visibility into the network, application, and user activity. 

Mindmap showing the components of Cloud and Security Intelligence highlighting Security intelligence
Targets:
Government and corporate multinational organizations are challenged to interpret billions of events each day to uncover attacks.

Tools:
SIEM - Security Information and Event Management

In the video below your instructor will guide you through these concepts

Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


Numbered divider 7
Threat Hunting 

Focuses on cybercrime detection, hunting and investigation The act of proactively and aggressively identifying, intercepting, tracking, investigating and eliminating cyber adversaries as early as possible in the Cyber Kill Chain. The earlier you locate and track your adversaries Tactics, Techniques and Procedures (TTPs) the less impact these adversaries will have on your business.

Mindmap showing the components of Cloud and Security Intelligence highlighting Threat hunting
Targets:
Every industry is impacted by cybercrime and affects everything that we do in our lives today, so it is the responsibility of everyone to be part of the solution.  

Tools:
Threat intelligence, human analyst, threat analytics, visualization, and prediction tools

Let's watch the following video with your instructor explaining this topic

(While playing it you can scroll up and revisit the content covered to deepen your understanding)


Summary

1

1
Security at the organization level must be structured around domains, with security intelligence in the middle to make sense of threats using logs, data, flows, packets. 

2

2
Enterprise security doesn't happen in a vacuum, it requires an integrated collaboration across companies and competitors, to understand global threats, sharing data to adapt to new threats.

Next

Go to the summary and resources section to do a recap and revise the concepts covered in this Module 

Sources:
[1] IBM Security Immune System - Smart Paper
[2] IBM Security (Website). Security information and event management (SIEM)
[3] IBM Security (Website). The intelligence analysis solution for national security and defense.
",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/139345/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=8694b16e-0b4d-49e1-8137-c5fd429248bf&activity_id=http%3A%2F%2FSEC-FND-2022-R1%28NOI2%29#/lessons/n8vno_4vVQoFESYvDJIpgVoNiuV44dJk,
Data Science,OpenDS4All,0,OpenDS4All,0,Success Story - One-of-a-Kind Workshop to University of Liverpool Students,text,"Bootcamp-style OpenDS4All workshop where students could enhance their theoretical knowledge with hands-on, industry-focused experience.

Description
OpenDS4All is a project created to accelerate the creation of data science curricula at academic institutions. While a great deal of online material is available for data science, including online courses, we recognize that the best way for many students to learn (and for many institutions to deliver) content is through a combination of lectures, recitation or flipped classroom activities, and hands-on assignments.

OpenDS4All attempts to fill this important niche. Our goal is to provide recommendations, slide sets, sample Jupyter notebooks, and other materials for creating, customizing, and delivering data science and data engineering education.

The project hosts educational modules that may be used as building blocks for a data science curriculum.

Note: The link opends4all-resources takes you to the opends4all curriculum building blocks organized by category.

Note: If you adopt all or some of the content, please add your program's details to the ADOPTERS.csv file.

Audience (Instructor and Student)
The initial modules were designed to target a broad, cross-university audience at both the undergraduate and graduate levels. Modules contain instructor notes and comments intended to aid in the delivery of the material; the expectation is that instructors will be generally fluent in basic database and machine learning concepts.

The perspective of the materials largely comes from computer science, with an emphasis on data wrangling and engineering as well as machine learning and validation. However, prior versions of the content have been used to teach students ranging from freshmen to PhD students, across a wide range of fields. The emphasis is largely on core concepts and algorithms with grounding in today's technologies and best practices.

Students are expected to come in with two major prerequisites:

Comfort and familiarity with programming in Python (writing small functions, importing and calling library functions, using Python data structures).
Familiarity with probability theory and very basic statistical notions.
To some extent, students with a limited background can follow along with this material, but they will likely need to supplement extensively.

How to use
The following topology shows how content is currently organized around categories. This is a living/dynamic taxonomy that is updated as new content is added to the project. taxonomy Each category contains modules and each module consists of one or more of the following components:

instructor notes (Instructor_Notes.md) and guide to files
a set of PowerPoint slides (with presenter notes) ending in .pptx
companion Jupyter notebooks, for students to see the lecture materials ""in context"" and to be able to experiment
sample quiz materials (where applicable)
sample homework assignments (where applicable)
additional documentation (where applicable)
Note: The PowerPoint slides are not directly viewable on GitHub. After you clicked on the link to a set of PowePoint slides you need to select the Download button to download and view the slide deck. Two viewable extracts from the slide decks can be seen by clicking on the links below:

INTRODUCTION-Data-Science-basic.pptx
DATA-WRANGLING-Import-Link-mixed.pptx
There are many ways to interact with this repository:

browse the repository in search of content ( use the 'Find file' search functionality )
download content (PowerPoint slides, Jupyter notebooks, etc.)
contribute content ( become a contributor to the project )
become involved in the day-to-day management of the project ( become a committer )
provide overall direction and leadership to the project ( become a Technical Steering Committee member )
The project's governance principles clarifies the different roles and describes the processes for becoming a contributor, a committer or a TSC member.

Contributing
Anyone can contribute to this repository - learn more at CONTRIBUTING.md. Follow the step-by-step instructions COMMUNITY-GUIDE.md to submit a module for possible inclusion into to repository.

Governance
OpenDS4All is a project hosted by LF AI & DATA. This project has established its own processes for managing day-to-day processes in the project at GOVERNANCE.md.

Reporting Issues
To report a problem, you can open an issue. If the issue is sensitive in nature or a security related issue, please do not report in the issue tracker but instead email opends4all-technical-discuss@lists.lfaidata.foundation.

Contact Us
If you want to contact us, please open an issue and one of the members of the TSC will respond to your request. If you do not feel comfortable opening an Issue, email opends4all-technical-discuss@lists.lfaidata.foundation.

Learn More
If you are interested in collaborating on the project, please open an issue and one of the members of the TSC will respond to your request. If you do not feel comfortable opening an Issue, email opends4all-technical-discuss@lists.lfaidata.foundation.",https://github.com/odpi/OpenDS4All/,
IBM Automation,IBM Robotic Process Automation - Basic II,7,Database,7,7 - Database - basic manipulation,video,,https://learn.ibm.com/mod/video/view.php?id=152677,
IBM Cloud,Journey to Cloud: Envisioning your Solution,22,Module 3 - Deploy a Pilot Application in IBM Code Engine,2,Quiz,text,,https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/pCKM6n0nezhB6kVTKCCsXklHNDvnccCY,
Data Science,Getting Started with Enterprise Data Science,1,Course Overview,0,About this Course,text,"Explore how people, processes, and technology interact in a data science project lifecycle, tackling real-world industry challenges.               Why study data science?
Data science is a multidisciplinary approach to extracting actionable insights from the large and ever-increasing volumes of data collected and created by today’s organizations. Data science encompasses preparing data for analysis and processing, performing advanced data analysis, and presenting the results to reveal patterns and enable stakeholders to draw informed conclusions.

Data preparation can involve cleansing, aggregating, and manipulating it to be ready for specific types of processing. Analysis requires the development and use of algorithms, analytics, and AI models. It’s driven by software that combs through data to find patterns within to transform these patterns into predictions that support business decision-making. The accuracy of these predictions must be validated through scientifically designed tests and experiments. And the results should be shared through the skillful use of data visualization tools that make it possible for anyone to see the patterns and understand trends.

As a result, data scientists (as data science practitioners are called) require computer science and pure science skills beyond those of a typical data analyst. A data scientist must be able to do the following:

Apply mathematics, statistics, and the scientific method
Use a wide range of tools and techniques for evaluating and preparing data—everything from SQL to data mining to data integration methods
Extract insights from data using predictive analytics and artificial intelligence (AI), including machine learning and deep learning models
Write applications that automate data processing and calculations
Tell—and illustrate—stories that clearly convey the meaning of results to decision-makers and stakeholders at every level of technical knowledge and understanding
Explain how these results can be used to solve business problems
This combination of skills is rare, and it’s no surprise that data scientists are currently in high demand.
Data science impact in the job market
Market Insights
In the 2018 August LinkedIn Workforce Report: Data Science Skills are in High Demand Across Industries report, LinkedIn reported that there were more than 151,000 unfilled data scientist jobs across the US, with “acute” shortages in New York City, San Francisco, and Los Angeles. Combined with a 15% discrepancy between job postings and job searches on Indeed, demand for data scientists clearly outstrips supply.

Today, 63% of executives cite a lack of talent as a prime barrier to adopting AI technology
Source: Francesco Brenna, Giorgio Danesi, Glenn Finch, Brian Goehring, and Manish Goyal. “Shifting toward Enterprise-grade AI: Resolving data and skills gaps to realize value.” IBM Institute for Business Value, September 2018.
To make matters worse, there has been a lack of consistency in the skills required from candidates to fill a job. In some cases, job ads are too qualification-intensive, makingit difficult to match skills to the job. In other cases, the candidates without the right qualifications are applying and being recruited as data scientists.

This situation has left companies at risk of losing valuable time and opportunity and disappointing results from poor implementations. And, in worst cases, when data science, ML and AI techniques are used incorrectly, they’re at risk of legal exposure. For aspiring data scientists looking for better jobs, this lack of consistency has left them unsure about which skills to develop to be successful in their careers.

How completing this course could benefit you?
Although data science as a field has existed for several decades, the rapid growth of artificial intelligence (AI) in business in the last five years has generated a demand for data scientists that far surpasses the availability of trained professionals.
This talent gap is an opportunity for aspiring professionals and a challenge for companies striving for a competitive advantage in the market. For prospective data scientists and organizations building a data science team, gaining the necessary skills required can be a formidable obstacle.

Formalizing necessary skills helps academic institutions, data scientists, hiring managers, and resource talent development teams deliver on the promise of data science, machine learning (ML), and AI.

Data today is everywhere, and most jobs today require data science skills
Looking for a job? – Gain a new set of data analytics skills, complement them with low-code AI-powered technologies, and your industry knowledge, to get on your way to join a data science team, and join a new wave of data-savvy professionals with access to millions of jobs available in the market.
Looking for a better job? – If you already have a job and even some experience with data analytics, use this course to select a specialization and advance your career, by playing different roles within a data science team, solving real challenges within the enterprise, leveraging AI-powered technologies.
Objectives
This course has the following learning objectives
1
1
Understand the relevance of Data science projects in supporting the digital transformation of business across multiple industries.
2
2
Acquire a data science cross-disciplinary skillset found at the intersection of statistics, computer programming, and domain expertise.
3
3
Get acquainted with the following roles of a Data Science team: Data scientist, Data engineer, Data analyst, and Business Liaison.
4
4
Access Data science collaboration platforms in the cloud, including IBM Watson Studio and Data Refinery.
5
5
Experience with data ingestion and manipulation using a CSV dataset.
6
6
Explore a real-world data science industry case study focused on an insurance company, and perform the tasks of a Data Analyst in a simulated hand-on scenario.
Pre-requisites
Basic IT Literacy
Basic IT Literacy - Refers to skills required to operate user-level operating system environment such as Microsoft Windows® or Linux Ubuntu®, performing basic operating commands such, copying and pasting, using menus, windows, through the mouse and keyboard. Additionally, users should be familiar with internet browsers, search engines, page navigation, fill and submit forms.
Recognition
This course grants the following digital credentials upon completion

Completing this course you will earn the Getting Started with Enterprise Data Science - Foundational-level badge.
What are the completion requirements?
Complete all the modules included in this self-paced online course
Pass the quizzes included on every module
More details about this digital certificate below
Badge website
https://www.credly.com/org/ibm/badge/getting-started-with-enterprise-data-science
Skills:
Data Analyst
Data Engineer
Data Exploration
Data Refinery
Data Science
Data Scientist
Data Visualization
Fraud Analytics
Watson Studio
Badge Description:
This badge earner has completed all the learning activities included in this online learning experience, including hands-on experience, concepts, methods, and tools related to Data Science roles and their use of technology applied to enterprise projects. The individual has demonstrated knowledge and understanding of the foundations of Data Science including: Data Science Team Roles, Data Analysis Tools, and real-world use cases for the application of the Data Science method.",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/mQl14U6A8a2CToSPog4xsN1twCKtpn-c,
IBM Engineering,Quick Start Sessions,1,Quick Start: Getting Started with Method Composer,1,What is Method Composer?,text,"IBM Engineering Lifecycle Optimization - Method Composer (MEC) is a tool platform that enables process engineers and managers to implement, deploy, and maintain processes for organizations or individual projects. 


MEC provides a knowledge base of intellectual capital which you can browse, manage and deploy. MEC is designed to be a content management system that provides a common management structure and look and feel for all of your content, rather than being a document management system in which you would store and access hard to maintain legacy documents all in their own shapes and formats.

MEC also provides powerful automation to enact practice content by deploying it as a set of work items in IBM Engineering Workflow Management so that tasks may be assigned to engineers. This helps project leads by speeding up deployment of complex projects. Such deployed tasks automatically have links back to the practice content, providing in-context guidance to engineers on their assigned work, helping them get up to speed much faster.

In this Quick Start you will begin by browsing some already-deployed practice content and work items. You will then create new practice content using the Method Composer Eclipse client, before publishing that to the Jazz team server both as browsable content and a set of deployable work items. Lastly you will explore an alternative way of creating practice content by using IBM Engineering DOORS Next as an authoring tool",https://learn.ibm.com/mod/page/view.php?id=208912,
IBM Automation,IBM Robotic Process Automation - Basic II,3,PDF,3,3 - Presentation of PDF commands,video,,https://learn.ibm.com/mod/video/view.php?id=152672,
IBM Automation,IBM Robotic Process Automation - Basic I,26,Advanced Commands,25,Date and time - Use case,video,,https://learn.ibm.com/mod/video/view.php?id=201481,
IBM Cloud,Journey to Cloud: Envisioning your Solution,6,Module 1 - Digital Transformation with Cloud Computing,1,Topic 3: Benefits of Cloud Computing,text,"An overview of the benefits that cloud computing can provide industries and organizations. 

Introduction

In this topic, we'll take a look at the six key benefits of cloud adoption as an operational model. 

Throughout this topic, we will attempt to answer the following questions:

How do organizations benefit from adopting cloud as an operating model?
1. How do organizations benefit from adopting cloud as an operating model? 

After you complete the up-front tasks of translating a business problem into an AI and data science solution and understanding the data needs in support of your business problem, it’s time to prepare the data.

Tapping the power of cloud  

The world is experiencing a digital and mobile transformation, with more information available more quickly in more mediums than ever before. As part of this, consumers have jumped on the social media bandwagon, with many relying on it as their primary collaboration format.  

Image showing people working on the things done with cloud technology
Add to this the advent of new analytics capabilities and the results are sweeping changes in almost every aspect of daily business and consumer life. But how does cloud play into all of this?

Cloud provides a way for businesses to harness the capabilities borne of these digital trends to better meet customers’ needs and drive future growth. In fact, IBM research illuminated six key cloud attributes being used to power business model innovation, which we’ve dubbed business enablers:

Cost flexibility 
Business scalability
Market adaptability
Masked complexity 
Context-driven variability 
Ecosystem connectivity
Cloud Business Enablers  

COST FLEXIBILITY
BUSINESS SCALABILITY
MASKED COMPLEXITY
CONTEXT-DRIVEN VARIABILITY
MARKET ADAPTABILITY
ECOSYSTEM CONNECTIVITY
Cost flexibility is a key reason many companies consider cloud adoption in the first place. More than 31 percent of executives surveyed cited cloud’s ability to reduce fixed IT costs and shift to a more variable “pay as you go” cost structure as a top benefit. Cloud can help an organization reduce fixed IT costs by enabling a shift from capital expenses to operational expenses. IT capital expenses – which typically include enterprise software licenses, servers, and networking equipment – tend to be less fluid, more expensive, and harder to forecast than routine IT operating expenses. With cloud applications, there is no longer a need to build hardware, install software or pay dedicated software license fees. By adopting cloud services, an organization can shift costs from capital to operational – or from fixed to variable. The organization pays for what it needs when it needs it.


This pay-per-use model provides greater flexibility and eliminates the need for significant capital expenditures. Cost flexibility is certainly an appealing cloud attribute for Etsy, an online marketplace for handmade goods. In addition to bringing buyers and sellers together, Etsy also provides recommendations for buyers. Using cloud-based capabilities, the company is able to cost-effectively analyze data from the approximately one billion monthly views of its Web site and use the information to create product recommendations. The cost flexibility afforded through cloud provides Etsy access to tools and computing power that might typically only be affordable for larger retailers.

Image depicting cost flexibility like reducing costs
The benefits of Cloud Computing for Business 


Why it matters? 
Cloud computing provides numerous benefits for subscribers. Because cloud services are often purchased 'on-demand', businesses can easily scale up or down. This flexibility allows businesses to free up time and resources that would otherwise be dedicated to maintaining their own servers and software. 

Cloud is everywhere. This means more connections to other services and customers, allowing organizations easier access to data in order to offer a more cohesive user-driven experience. Another benefit of the ubiquitous nature of cloud is that team collaboration can more easily be facilitated through cloud-based software than on-prem. 

Summary

1

1
Six key cloud attributes being used to power business model innovation, which we’ve dubbed business enablers: 

Cost flexibility 
Business scalability
Market adaptability
Masked complexity
Context-driven variability 
Ecosystem connectivity
Next 

We explore legacy IT architecture and compare the three main cloud delivery models: Public, Private, and Hybrid cloud. 

Sources:
[1] Anderson, Erik. IBM. How to Explain Cloud to Your Spouse. February 1, 2013. 
[2] U.S. General Services Administration. Cloud Information Center. Cloud Capabilities. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/PJfoYzK_OKgZwIFPRHZ7k7oT3wgkY2yy,
IBM Automation,IBM Robotic Process Automation - Basic I,14,Advanced Commands,13,File Manipulation II,video,,https://learn.ibm.com/mod/video/view.php?id=152649,
IBM Automation,IBM Robotic Process Automation - Basic I,1,Introduction,0,RPA and RDA,text,"Robotic Process Automation (RPA) is the automation of computer-centric processes using software robots that can emulate human actions in the computer. These software robots, or bots, can be attended or unattended. 

The processes that are candidates for automation are generally time-consuming and highly repetitive processes, that is, they follow a fixed and long flow, with predefined rules and well-determined input and output parameters. A specialized type of RPA designed to help humans on their daily tasks by automating the mechanical part of their everyday work. These types of bots require human interaction to be able to complete their automation goal. Unattended bots don't need human interaction to complete their automation goal. They might rely on human data input, like documents or email messages.Hyperautomation is the combination of advanced technologies such as machine learning, Artificial Intelligence, and process mining, in combination with RPA, to identify and automate significant processes, and also improve existing automation. • Accuracy, since bots eliminate errors during data entry in systems.

• Bots have high availability with uninterrupted executions.

• Bots can process high volumes of data with concurrent processing.

• Integration with legacy systems without the need for customizations.

• Real-time data visualization with dashboards.",https://learn.ibm.com/mod/hvp/view.php?id=200768,
IBM Cloud,Journey to Cloud: Envisioning your Solution,11,Module 2 - Cloud Adoption Journey : Ideation Practices,2,About this Module,text,"
Module 2

Cloud Adoption Journey: Ideation Practices 

Introduction

In this module, we will focus on cultural transformation as part of the cloud adoption journey. We will explore practices such as Agile, the IBM Garage Method, and Enterprise Design Thinking (EDT) in order to break down the barriers to innovation. We will experiment with Garage and EDT practices by creating an empathy map and a business framing exercise for a fictional client.  

This page covers the following lecture details:

Duration
Objectives
Skills
Instructors
Topics
Duration

This module can be completed in an average of 120 minutes

Objectives

In this module you will learn the following concepts:

1

1
Explain the role of methodologies such as Agile, IBM Garage, and Enterprise Design Thinking as transformational practices.  

2

2
Analyze design thinking practices such as Empathy Mapping and Business Framing from the standpoint of user-centric development. 

3

3
Evaluate the merits of Enterprise Design Thinking from the standpoint of an organization looking to adopt more effective development practices. 

4

4
Describe the three phases of the IBM Garage lifecycle: Think, Transform, Thrive. Describe the Enterprise Design Thinking (EDT) process. 

Skills

Upon completing this module you will have acquired the following skills: 

Digital Transformation 
IBM Garage Methodology
Introduction to Agile Practices
Enterprise Design Thinking
Empathy Mapping 
Business Framing
About the Instructors

Meet the IBM subject-matter experts, who will guide you through the topics on this module.   

JJ AsgharDeveloper Advocate IBMBiographyJJ works as a Developer Advocate representing the IBM Cloud all over the world. He mainly focuses on the IBM Kubernetes Service and OpenShift trying to make companies and users have a successful onboarding to the Cloud Native ecosystem. He’s also been known in the DevOps tooling ecosystem and generalized Linux communities. If he isn’t building automation to make his work streamlined he’s building the groundwork to do just that.  


JJ Asghar

Developer Advocate 
IBM





Biography


JJ works as a Developer Advocate representing the IBM Cloud all over the world. He mainly focuses on the IBM Kubernetes Service and OpenShift trying to make companies and users have a successful onboarding to the Cloud Native ecosystem. He’s also been known in the DevOps tooling ecosystem and generalized Linux communities. If he isn’t building automation to make his work streamlined he’s building the groundwork to do just that.
 

 



Image showing instructor Andrea Crawford


Andrea Crawford 

Distinguished Engineer (DevOps)
IBM Garage for Cloud             





Biography



Andrea has more than 20 years of cross-industry experience in application development, architecture, and accelerated delivery. Andrea has made significant contributions to IBM’s own DevOps transformation and to clients’ DevOps journeys. Andrea provides technical oversight and leadership of technical enablement, solution design, and offering innovation for which focuses on accelerated application development with Agile, DevOps, and Cloud-Native Development. Andrea applies her expertise in accelerated development across all industries, heterogeneous technologies, and hybrid cloud scenarios.


Inclusiveness begets a diverse workforce and groundbreaking, innovating thinking and ideas. Andrea has a passion for supporting and enabling under-represented minorities in IT.



Image showing instructor Mihai crivati


Mihai Crivati  

CTO Cloud-Native and Red Hat Solutions 
IBM  





Biography



Mihai Criveti is the CTO for Cloud-Native and Red Hat Solutions at IBM. He is an STSM, TOGAF, and Red Hat Certified Architect, a member of the IBM AoT, and an advocate for Open-Source development. 


He helps clients increase their innovation, business agility, and accelerate time to market through Digital Transformation and Infrastructure Modernization. He develops solutions that help clients adopt DevSecOps, shift-left operations, SRE models – and automated operations across hybrid and multi-cloud environments that take advantage of Kubernetes, OpenShift, and modern development techniques.


Mihai leads the Ireland AoT affiliate (ITEC) and the OpenShift Solution Guidance initiative and is passionate about growing the technical community in Ireland, mentoring, coaching, and supporting academic initiatives.



Image showing instructor Tom Creamer


Tom Creamer     

OCP Technical Elite Team 
IBM





Biography



Tom is the OCP Technical Elite team and IBM Global Markets Americas CTO.  He is an IBM Distinguished Engineer, Open Group Distinguished IT Specialist, and IBM Master Inventor with over 190 patents granted.  Tom is devoted to the growth of the IBM Technical community and is the chairman of the IBM Global Markets Americas Distinguished Engineer Board.

Tom spent 12 years as the Technical Director and IBM CTO to Verizon and has 35 years with IBM working predominately in the Telecommunications industry.

Tom is a member of the IBM Telecom, Media & Entertainment CTO community, and the IBM Edge Computing and 5G Multi-access Edge Compute (MEC) architecture teams.



Image showing instructor Hardy Groeger


Hardy Groeger    

IBM Distinguished Engineer  
IBM





Biography



Hardy Groeger is an IBM Distinguished Engineer and the Director for Competitive Insights. He brings extensive technology and automotive industry experience into every engagement, based upon his roles as CTO Integrated Account Daimler, Lead Architect for Mobile Solutions, Collaboration Solutions, and Websphere / Websphere Portal. 

Hardy's technology journey started in IBM's software development labs in the US. He holds a Master of Science in Business Computing from the University of Paderborn, Germany.



Image showing instructor Dr. Thalia Hooker


Dr. Thalia Hooker    

Global OCP Elite Team
IBM 





Biography



Dr. Hooker is an innovative Thought Leader dedicated to customer success with over 20 years of experience in IT. She works across IBM and complex customer organizations with their technology strategy and recommends solutions that solve customer needs while bringing the right IBM capabilities to bear, whether software, hardware, and/or services. She helps drive OpenShift Container Platform (OCP) adoption and IBM Cloud Paks deployment on OCP in her current role. ",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/131656/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=a25d90fd-ba18-48bc-854f-0a826ca3519a&activity_id=http%3A%2F%2FRH-FND-2021-R0#/lessons/oqHOvHcc4ErPZi6f0ZnpY0oB8sb-qWEn,
Artificial Intelligence,Build with Watson,1,IBM Watson,1,About Watson,text,"About Watson
IBM Watson products unlock new levels of productivity by infusing AI and automation into core business workflows.

Carousel

1 / 2Slide 1 of 2. Showing 0 items.
Introducing watsonx
Watsonx is our upcoming next generation, enterprise-ready AI and data platform designed to multiply the impact of AI across your business.
Learn more about the watsonx platform
Geometric representation of watson x ai
watsonx.ai
Train, validate, tune, and deploy foundation and machine learning models with ease. General availability of watsonx.ai is expected in July.

Learn more about watsonx.ai
Geometric representation of watson x data
watsonx.data
Scale AI workloads, for all your data, anywhere. General availability of watsonx.data is expected in July.

Learn more about watsonx.data
Geometric representation of watson x governance
watsonx.governance
Enable responsible, transparent and explainable data and AI workflows.  General availability of watsonx.governance is expected in October.

Learn more about watsonx.governance
How to thrive in this new era of AI with trust and confidence
Read our point of view (3.7 MB)
Reimagine your work with Watson products
Watson products provide support where you are in your AI journey, allowing your organization to gain a competitive advantage in the marketplace. 

Watson Orchestrate
Automation for every employee.  Streamlines processes and repetitive or complex tasks with automation accessed through open APIs and RPA integrations.
Learn more 
Five copies of the same person handling multiple tasks.
Watson Assistant
Answers and actions, automated. Get fast, accurate answers across any application, device, or channel, from customer service to internal IT Helpdesk and HR.
Learn more 
Graphic representing multiple chat conversations being handled by Watson Assistant
Watson Code Assistant
Enable developers of all experience levels to write code with AI-generated recommendations.
Learn more 
Office workers at their desktop PCs
Watson Discovery
Dig less, discover more. Natural language AI models help your team parse complicated business documents to get the right information at the right time.
Learn more 
Graphic representing a natural language chat on a smartphone
Client and partner successes

Customer service

HR

Legal
Driving a reimagined Camping World customer experience with an AI-powered virtual assistant
The retailer needed a human-centered solution to handle the increasing numbers of customers seeking rapid assistance. The solution? An AI-powered tool developed by IBM.
Learn more 
Folding chairs next to a camping vehicle
Take the next step
Learn more about watsonx or let the consultants from IBM Garage help you shape your AI journey with an end-to-end model for digital transformation.",https://www.ibm.com/watson,
Data Science,Getting Started with Enterprise Data Science,20,Module 2 - Data Science on the Cloud,2,Quiz,text,,https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/X4f2CKFFxosbkSB7we9yZGXSks379c4-,
IBM Engineering,Quick Start Sessions,20,Getting Started with IBM Engineering Lifecycle Management and Jira,20,What is IBM Engineering Lifecycle Management and JIRA?,text,"OSLC Connect for Jira from SodiusWillert integrates Jira with the IBM ELM solution, enabling Jira to be used as a native Change Management provider whilst linking to requirements, test cases, model elements and so on in the IBM ELM tools. The connector provides support for IBM’s Global Configuration within Jira as well as the custom configuration of default data types created from Engineering Lifecycle Management tools and also provides a TRS feed so that Jira data may be included in ELM reporting and analysis capabilities (Jazz Reporting Services Report Builder and Engineering Insights)

In this Quick Start lab you will explore the creation of links between artifacts in Jira (such as stories and defects) and artifacts in ELM (such as test cases and requirements). You will also create dashboard reports and engineering insights views of the traceability.",https://learn.ibm.com/mod/page/view.php?id=202427,
IBM Automation,IBM Robotic Process Automation - Basic II,6,Data Tables,6,6 - Data tables,video,,https://learn.ibm.com/mod/video/view.php?id=152675,
Data Science,Getting Started with Enterprise Data Science,14,Module 2 - Data Science on the Cloud,2,Topic 3: Data Science lifecycle: Exploration Phase,text,"
Exploring the data journey within the exploration phase of the data science lifecycle   

Introduction

In this topic, we'll introduce you to the data science lifecycle and walk you through the three stages of the data exploration phase: exploration, preparation, model development.

Throughout this topic, we will attempt to answer the following questions:

Describe the three main steps of the exploration phase within the data science lifecycle: 
a. Data exploration
b. Data preparation 
What are the tasks involved in data exploration? 
1. Describe the three main steps of the exploration phase within the data science lifecycle. 

It's difficult for today's data science teams to respond fast enough to the demands of their business. With the explosion of data from multiple sources and formats, trying to develop models and putting them into production quickly is a huge challenge for many teams. 

This guide provides an overview of the typical data science lifecycle, common challenges organizations encounter, and how IBM® Watson Studio can address them-enabling your data science team to accelerate and optimize the value of analytics results throughout your organization.

Bringing the Data Science methodology, roles, and tools into a Data Science lifecycle

The data science lifecycle—also called the data science pipeline—includes anywhere from five to sixteen (depending on whom you ask) overlapping, continuing processes. 

Data science methodology, roles, and tools in a data science lifecycle.
The processes common to just about everyone’s definition of the lifecycle include the following:

1

Capture
This is the gathering of raw structured and unstructured data from all relevant sources via just about any method—from manual entry and web scraping to capturing data from systems and devices in real time.

12345
2

Prepare and maintain
 This involves putting the raw data into a consistent format for analytics or machine learning or deep learning models. This can include everything from cleansing, deduplicating, and reformatting the data, to using ETL (extract, transform, load) or other data integration technologies to combine the data into a data warehouse, data lake, or other unified store for analysis. 

12345
3

Preprocess or process
Here, data scientists examine biases, patterns, ranges, and distributions of values within the data to determine the data’s suitability for use with predictive analytics, machine learning, and/or deep learning algorithms (or other analytical methods).

12345
4

Analyze
This is where the discovery happens—where data scientists perform statistical analysis, predictive analytics, regression, machine learning and deep learning algorithms, and more to extract insights from the prepared data.

12345
5

Communicate
Finally, the insights are presented as reports, charts, and other data visualizations that make the insights—and their impact on the business—easier for decision-makers to understand. A data science programming language such as R or Python (see below) includes components for generating visualizations; alternatively, data scientists can use dedicated visualization tools.

12345
Bringing it all together

In the video below your instructor will guide you through these concepts:


The exploration phase of the data science lifecycle is comprised of three stages: Data Preparation, Data Exploration, and Model Development.

Exploration phase : Big Data and Analytics

Data Exploration

Once your data is in the right format to work with, you can conduct the next phase in the data analysis process. This initial exploration of the dataset is critical.

An image showing the exploration phase of big data and analytics
After you complete the up-front tasks of translating a business problem into an AI and data science solution, and understanding the data needs in support of your business problem, it’s time to prepare the data. You need to prepare the data in a format that can be used for model development, measurement, and training a machine learning model.

Most AI and data science models require data to be combined and denormalized into one large analytical record before data mining, feature selection, model development and optimization, and training can occur.

Data Preparation

Uncover hidden insights in your data with the Data Refinery tool, which provides built-in data cleaning and transformations. 

Gain access to a tabular view of your data, including visualizations and summary statistics that can help you uncover hidden insights by asking the right business questions of your data.

An image showing the exploration phase of data preparation
Data preparation involves these tasks:

Select a sample subset of data
Filter on rows that target particular customers or products that help answer the data analysis and business goals. Also, filter on attributes that are relevant to data analysis and business goals. Some data science patterns, such as Fraud Detection, AML Anti-Money Laundering, and Log Analysis might require full-volume, unsampled data.
Merge data sets or records
To join data sets, you need a common key. Aggregate records, and merge based on group by like operations.
Derive new attributes
When you merge data sets, especially when you have many relationships, it can be useful to derive new attributes. For example, if you have a customer data set and a customer purchases data set, you might condense the purchases to a new derived attribute with mean or total spending.
Format and sort the data for modeling
Sequence and temporal algorithms might need data to be presorted into a particular order. Categorical data fields might need to be converted from textual categories to numerical ones.
Remove or replace blank or missing values
Exclude rows where the missing attribute is key in making the decision. Fill in missing attributes with 0 or estimated values where the rest of the row adds value to the analysis.
If your data is sparse or you have many attributes, consider reducing the number of features. Principle Component Analysis can show you which attributes have the biggest impact on the data. Linear regression can show you that two attributes are correlated, so you need to use only one of those attributes and remove the other.
Normalize numeric fields to use the same range. Features with large values compared to features with small values can be given more weight by various algorithms. Normalization eliminates the unit of measurement by rescaling data, often to a value in the range 0 - 1.
Replace or correct data and measurement errors
Don't worry if these terms seem a bit heavy, we will dive deeper into these in the intermediate and advanced courses. 

Data Refinery

Data Refinery is an essential and early-on task that a data wrangler will undertake to cleanse and transform their data.

An image from Data refinery tool.
The Data Refinery tool, available via Watson Studio and Watson Knowledge Catalog, saves data preparation time by quickly transforming large amounts of raw data into consumable, quality information that’s ready for analytics.

The Data Refinery service reduces the amount of time it takes to prepare data. Use pre-defined operations that you can use in your data flows to transform large amounts of raw data into consumable, quality data that’s ready for analysis. 

With Data preparation tools like IBM's Data Refinery, you can:

Interactively discover, cleanse, and transform your data with over 100 built-in operations. No coding is required.
Understand the quality and distribution of your data using dozens of built-in charts, graphs, and statistics.
Automatically detect data types and business classifications.
Schedule data flow executions for repeatable outcomes.
Bringing it all together

In the video below your instructor will guide you through these concepts:


2. What are the tasks involved in data exploration?

Once your data is in the right format to work with, you can conduct the next step in the data analysis process: data exploration.

This initial exploration of the dataset is critical because it helps data scientists illuminate previously unknown patterns, relationships, or other actionable findings. 

Data Exploration

Using the included dashboarding service, produce stunning visualizations directly from your data in real-time. 

This allows you to illuminate previously unknown patterns, relationships, or other actionable findings, and easily share them with your team.

A dashboard showing graphs and pie charts.
Exploratory data analysis (EDA) is a technique used for data exploration to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods. It helps determine how best to manipulate data sources to get the answers you need, making it easier for data scientists to discover patterns, spot anomalies, test a hypothesis, or check assumptions.

EDA is primarily used to see what data can reveal beyond the formal modeling or hypothesis testing task and provides a better understanding of data set variables and the relationships between them. It can also help determine if the statistical techniques you are considering for data analysis are appropriate.

Some helpful questions to ask at this point include:

Which attributes seem promising for further analysis?
Has the exploration revealed new characteristics about the data?
How have these explorations changed any initial hypotheses?
Can a specific subset of the data be used later?
Has the data exploration altered the project goals?
The main purpose of EDA is to help look at data before making any assumptions. It can help identify obvious errors, as well as better understand patterns within the data, detect outliers or anomalous events, find interesting relations among the variables.

Data scientists can use exploratory analysis to ensure the results they produce are valid and applicable to any desired business outcomes and goals. EDA also helps stakeholders by confirming they are asking the right questions. EDA can help answer questions about standard deviations, categorical variables, and confidence intervals. Once EDA is complete and insights are drawn, its features can then be used for more sophisticated data analysis or modeling, including machine learning (Artificial Intelligence) programs.

Bringing it all together

In the video below your instructor will guide you through these concepts:


Summary

1

1
Data preparation (including data cleansing and data wrangling) is one of the most important and time-consuming aspects of data science. It is essential to first filter the data that you're working with to ""weed out"" the noise and organize data for easier processing. 

2

2
Once your data is in the right format to work with, you can conduct the next step in the
data analysis process: data exploration.

Next 

The role of the data analyst in an integrated Cloud environment 

Sources:
[1] IBM SmartPaper. The Data Science Lifecycle: From experimentation to production-level data science. 
[2] IBM.com.  IBM Watson Studio",https://keyskill-clms.comprehend.ibm.com/pluginfile.php/138177/mod_tincanlaunch/content/index.html?endpoint=https%3A%2F%2Fcars-us.comprehend.ibm.com%2Fdata%2FxAPI%2F&auth=Basic%20YTVkZjAwN2I5M2RiN2QwOTJiYjk5NGI2NThkMzc4Njc1MjJhMzNmODo4NTQ0ZDE3OGE3MTJmOGYwMjc3YTcxNWQzMTM4OGZmY2RjYmYzMTU0&actor=%7B%22objectType%22%3A%22Agent%22%2C%22name%22%3A%22Olivia%20Fossali%22%2C%22mbox%22%3A%22mailto%3Aolivia.fossali.22%40ucl.ac.uk%22%7D&registration=dc4ea551-2c11-4bc1-a756-625351b21c03&activity_id=http%3A%2F%2FDS-FND-2022-R1#/lessons/Qw-dzU7umGcdWJ27SXhS1YK_adNVAzoH,
IBM Automation,IBM Robotic Process Automation - Basic I,19,Advanced Commands,18,Use case III - Queue,video,,https://learn.ibm.com/mod/video/view.php?id=152656,
IBM Automation,IBM Robotic Process Automation - Basic II,2,Parameters,2,2 - Parameters,video,,https://learn.ibm.com/mod/video/view.php?id=152669,
